{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd8ddC7NQ8uZ"
   },
   "source": [
    "# Depression Transformer Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mb_5bl7G_n30"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S17Nfn6W_vhd"
   },
   "outputs": [],
   "source": [
    "path_to_dataset = os.path.join(os.getcwd(), \"Dataset\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset, 'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_B147qKb_0ks"
   },
   "outputs": [],
   "source": [
    "#Load and preprocess data\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "  # dictionary of line id to text\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    # get conversation in a list of line ID\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    for i in range(len(conversation) - 1):\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "  return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "mfOOK5f7Wm6c",
    "outputId": "24993723-9696-4d25-b4ff-bc05cfb8edb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: i really , really , really wanna go , but i can t . not unless my sister goes .\n",
      "Sample answer: i m workin on it . but she doesn t seem to be goin for him .\n"
     ]
    }
   ],
   "source": [
    "print('Sample question: {}'.format(questions[20]))\n",
    "print('Sample answer: {}'.format(answers[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s6XX2udMTCQt"
   },
   "outputs": [],
   "source": [
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h5h8pvRUTFt5",
    "outputId": "a54ba714-e198-4d11-d604-8ca1685f02f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [4, 312, 2, 312, 2, 152, 412, 186, 2, 43, 4, 39, 8006, 3, 36, 915, 30, 1478, 3314, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YESTPgeg_XgT"
   },
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # tokenize sentence\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "    # check tokenized sentence max length\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "pohHm8IRWlIH",
    "outputId": "10448ea8-2d4b-439b-ed9d-2a13ed7bf721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8148\n",
      "Number of samples: 194413\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S50jT4upWh5c"
   },
   "source": [
    "### Create `tf.data.Dataset`\n",
    "\n",
    "We are going to use the [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data) to contruct our input pipline in order to utilize features like caching and prefetching to speed up the training process.\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.\n",
    "\n",
    "During training this example uses teacher-forcing. Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask.\n",
    "\n",
    "Target is divided into `decoder_inputs` which padded as an input to the decoder and `cropped_targets` for calculating our loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pttC3XxgAXWQ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((questions,answers))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "mU8yNWpwPlS7",
    "outputId": "de2c0206-984a-4770-8c23-399e44acbfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: ((None, 40), (None, 40)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5QlgXsxYirg"
   },
   "source": [
    "### Masking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HzXl0h2Tvnu4"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HSVdD2zKWaXx"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDUX7Oa8Xudj"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9eeMPjGXmI1"
   },
   "source": [
    "###Multi Head Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ENfqAFna_50H"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  \n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "L9eYssGIAG4h"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  \n",
    "        k = self.wk(k)  \n",
    "        v = self.wv(v)  \n",
    "\n",
    "        q = self.split_heads(q, batch_size) \n",
    "        k = self.split_heads(k, batch_size)  \n",
    "        v = self.split_heads(v, batch_size) \n",
    "\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  \n",
    "\n",
    "        output = self.dense(concat_attention)  \n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFNuvWmAvyxT"
   },
   "source": [
    "### Positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-9Oibz2es-qW"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVazCemoW2Ye"
   },
   "source": [
    "### Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f5jcx1Hst4Qa"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'), \n",
    "      tf.keras.layers.Dense(d_model)  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5guJOLJmfcuX"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  \n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  \n",
    "\n",
    "        ffn_output = self.ffn(out1)  \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  \n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K16BIGSKfkve",
    "outputId": "1bc30cb1-5c52-4c53-dec0-3dd5f0d525c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LRfugon5Wy-Y"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "bNxCnjrvglnx",
    "outputId": "6fe35d8a-2ed9-4e25-e5d6-5342b293a066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af66azvgW9P-"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6mLvvNMWgDnf"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  \n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  \n",
    "\n",
    "        ffn_output = self.ffn(out2)  \n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  \n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8M1NrQ_NgEaM",
    "outputId": "30dfd728-8c6d-4021-e5ac-54675cb4d55c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dYRx7YzCW4bu"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "id": "tUdK8jb9hlTZ",
    "outputId": "3e9d5a6c-b223-483b-d50f-be37a11d11bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl0o97RJXAqw"
   },
   "source": [
    "### Transformer\n",
    "\n",
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TW-v7Fz6XAfC"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.tokenizer = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.tokenizer(inp, training, enc_padding_mask)  \n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  \n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "aihJLVq_iJ_T",
    "outputId": "cb623cf5-f99d-47f7-a1a6-468e7f290b71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HD7GK-nh_KT"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDDxNpA-5Q5t"
   },
   "source": [
    "### Initialize model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xE3unrOT5M5z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "num_layers = 2\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    target_vocab_size=VOCAB_SIZE, \n",
    "    pe_input=1000, \n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DkoCjMlpt4Qd"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_GCb0LaV1tI"
   },
   "source": [
    "### Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UInVM9iGAMv1"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvFM9ajSVybP"
   },
   "source": [
    "### Custom learning rate\n",
    "\n",
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WW3SeLDhAMJd"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "67BoG_UeaHHw",
    "outputId": "904f2155-eed1-4533-d6f4-83f06d26a5cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCqve3kwWCxd"
   },
   "source": [
    "### Compile Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1QqojIa5WEQq"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDMd69urLNuc"
   },
   "source": [
    "### Fit model\n",
    "\n",
    "Train our transformer by simply calling `model.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "d7iahRzlLNG2"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy(tar_real, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JcnMdqTxt4Qf",
    "outputId": "9d8c05b0-d1a7-455d-bc86-d972e95a4400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.4655 Accuracy 0.0256\n",
      "Epoch 1 Batch 50 Loss 2.4689 Accuracy 0.0256\n",
      "Epoch 1 Batch 100 Loss 2.3649 Accuracy 0.0256\n",
      "Epoch 1 Batch 150 Loss 2.2451 Accuracy 0.0266\n",
      "Epoch 1 Batch 200 Loss 2.1734 Accuracy 0.0303\n",
      "Epoch 1 Batch 250 Loss 2.1164 Accuracy 0.0336\n",
      "Epoch 1 Batch 300 Loss 2.0760 Accuracy 0.0370\n",
      "Epoch 1 Batch 350 Loss 2.0323 Accuracy 0.0404\n",
      "Epoch 1 Batch 400 Loss 1.9939 Accuracy 0.0437\n",
      "Epoch 1 Batch 450 Loss 1.9637 Accuracy 0.0467\n",
      "Epoch 1 Batch 500 Loss 1.9323 Accuracy 0.0490\n",
      "Epoch 1 Batch 550 Loss 1.9063 Accuracy 0.0512\n",
      "Epoch 1 Batch 600 Loss 1.8847 Accuracy 0.0532\n",
      "Epoch 1 Batch 650 Loss 1.8598 Accuracy 0.0549\n",
      "Epoch 1 Batch 700 Loss 1.8397 Accuracy 0.0563\n",
      "Epoch 1 Batch 750 Loss 1.8252 Accuracy 0.0577\n",
      "Epoch 1 Batch 800 Loss 1.8082 Accuracy 0.0589\n",
      "Epoch 1 Batch 850 Loss 1.7923 Accuracy 0.0601\n",
      "Epoch 1 Batch 900 Loss 1.7777 Accuracy 0.0611\n",
      "Epoch 1 Batch 950 Loss 1.7643 Accuracy 0.0621\n",
      "Epoch 1 Batch 1000 Loss 1.7525 Accuracy 0.0630\n",
      "Epoch 1 Batch 1050 Loss 1.7399 Accuracy 0.0638\n",
      "Epoch 1 Batch 1100 Loss 1.7295 Accuracy 0.0645\n",
      "Epoch 1 Batch 1150 Loss 1.7182 Accuracy 0.0653\n",
      "Epoch 1 Batch 1200 Loss 1.7077 Accuracy 0.0660\n",
      "Epoch 1 Batch 1250 Loss 1.6965 Accuracy 0.0666\n",
      "Epoch 1 Batch 1300 Loss 1.6854 Accuracy 0.0672\n",
      "Epoch 1 Batch 1350 Loss 1.6754 Accuracy 0.0677\n",
      "Epoch 1 Batch 1400 Loss 1.6669 Accuracy 0.0683\n",
      "Epoch 1 Batch 1450 Loss 1.6589 Accuracy 0.0688\n",
      "Epoch 1 Batch 1500 Loss 1.6526 Accuracy 0.0693\n",
      "Epoch 1 Batch 1550 Loss 1.6457 Accuracy 0.0697\n",
      "Epoch 1 Batch 1600 Loss 1.6392 Accuracy 0.0702\n",
      "Epoch 1 Batch 1650 Loss 1.6328 Accuracy 0.0706\n",
      "Epoch 1 Batch 1700 Loss 1.6260 Accuracy 0.0710\n",
      "Epoch 1 Batch 1750 Loss 1.6200 Accuracy 0.0714\n",
      "Epoch 1 Batch 1800 Loss 1.6142 Accuracy 0.0717\n",
      "Epoch 1 Batch 1850 Loss 1.6083 Accuracy 0.0720\n",
      "Epoch 1 Batch 1900 Loss 1.6028 Accuracy 0.0723\n",
      "Epoch 1 Batch 1950 Loss 1.5971 Accuracy 0.0726\n",
      "Epoch 1 Batch 2000 Loss 1.5917 Accuracy 0.0729\n",
      "Epoch 1 Batch 2050 Loss 1.5866 Accuracy 0.0732\n",
      "Epoch 1 Batch 2100 Loss 1.5816 Accuracy 0.0735\n",
      "Epoch 1 Batch 2150 Loss 1.5782 Accuracy 0.0738\n",
      "Epoch 1 Batch 2200 Loss 1.5746 Accuracy 0.0741\n",
      "Epoch 1 Batch 2250 Loss 1.5711 Accuracy 0.0743\n",
      "Epoch 1 Batch 2300 Loss 1.5678 Accuracy 0.0746\n",
      "Epoch 1 Batch 2350 Loss 1.5639 Accuracy 0.0749\n",
      "Epoch 1 Batch 2400 Loss 1.5610 Accuracy 0.0752\n",
      "Epoch 1 Batch 2450 Loss 1.5585 Accuracy 0.0755\n",
      "Epoch 1 Batch 2500 Loss 1.5557 Accuracy 0.0757\n",
      "Epoch 1 Batch 2550 Loss 1.5519 Accuracy 0.0759\n",
      "Epoch 1 Batch 2600 Loss 1.5495 Accuracy 0.0762\n",
      "Epoch 1 Batch 2650 Loss 1.5468 Accuracy 0.0764\n",
      "Epoch 1 Batch 2700 Loss 1.5441 Accuracy 0.0767\n",
      "Epoch 1 Batch 2750 Loss 1.5411 Accuracy 0.0769\n",
      "Epoch 1 Batch 2800 Loss 1.5393 Accuracy 0.0771\n",
      "Epoch 1 Batch 2850 Loss 1.5369 Accuracy 0.0773\n",
      "Epoch 1 Batch 2900 Loss 1.5341 Accuracy 0.0775\n",
      "Epoch 1 Batch 2950 Loss 1.5326 Accuracy 0.0777\n",
      "Epoch 1 Batch 3000 Loss 1.5298 Accuracy 0.0779\n",
      "Epoch 1 Loss 1.5282 Accuracy 0.0780\n",
      "Time taken for 1 epoch: 183.92 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3062 Accuracy 0.0777\n",
      "Epoch 2 Batch 50 Loss 1.4011 Accuracy 0.0911\n",
      "Epoch 2 Batch 100 Loss 1.3690 Accuracy 0.0904\n",
      "Epoch 2 Batch 150 Loss 1.3800 Accuracy 0.0904\n",
      "Epoch 2 Batch 200 Loss 1.3766 Accuracy 0.0904\n",
      "Epoch 2 Batch 250 Loss 1.3723 Accuracy 0.0906\n",
      "Epoch 2 Batch 300 Loss 1.3684 Accuracy 0.0908\n",
      "Epoch 2 Batch 350 Loss 1.3638 Accuracy 0.0907\n",
      "Epoch 2 Batch 400 Loss 1.3618 Accuracy 0.0906\n",
      "Epoch 2 Batch 450 Loss 1.3582 Accuracy 0.0905\n",
      "Epoch 2 Batch 500 Loss 1.3607 Accuracy 0.0907\n",
      "Epoch 2 Batch 550 Loss 1.3632 Accuracy 0.0908\n",
      "Epoch 2 Batch 600 Loss 1.3647 Accuracy 0.0908\n",
      "Epoch 2 Batch 650 Loss 1.3638 Accuracy 0.0909\n",
      "Epoch 2 Batch 700 Loss 1.3604 Accuracy 0.0907\n",
      "Epoch 2 Batch 750 Loss 1.3618 Accuracy 0.0907\n",
      "Epoch 2 Batch 800 Loss 1.3602 Accuracy 0.0908\n",
      "Epoch 2 Batch 850 Loss 1.3611 Accuracy 0.0908\n",
      "Epoch 2 Batch 900 Loss 1.3623 Accuracy 0.0909\n",
      "Epoch 2 Batch 950 Loss 1.3624 Accuracy 0.0910\n",
      "Epoch 2 Batch 1000 Loss 1.3616 Accuracy 0.0910\n",
      "Epoch 2 Batch 1050 Loss 1.3610 Accuracy 0.0910\n",
      "Epoch 2 Batch 1100 Loss 1.3614 Accuracy 0.0910\n",
      "Epoch 2 Batch 1150 Loss 1.3627 Accuracy 0.0911\n",
      "Epoch 2 Batch 1200 Loss 1.3616 Accuracy 0.0912\n",
      "Epoch 2 Batch 1250 Loss 1.3588 Accuracy 0.0912\n",
      "Epoch 2 Batch 1300 Loss 1.3556 Accuracy 0.0913\n",
      "Epoch 2 Batch 1350 Loss 1.3533 Accuracy 0.0913\n",
      "Epoch 2 Batch 1400 Loss 1.3532 Accuracy 0.0914\n",
      "Epoch 2 Batch 1450 Loss 1.3520 Accuracy 0.0914\n",
      "Epoch 2 Batch 1500 Loss 1.3504 Accuracy 0.0915\n",
      "Epoch 2 Batch 1550 Loss 1.3492 Accuracy 0.0915\n",
      "Epoch 2 Batch 1600 Loss 1.3483 Accuracy 0.0916\n",
      "Epoch 2 Batch 1650 Loss 1.3464 Accuracy 0.0915\n",
      "Epoch 2 Batch 1700 Loss 1.3445 Accuracy 0.0916\n",
      "Epoch 2 Batch 1750 Loss 1.3441 Accuracy 0.0916\n",
      "Epoch 2 Batch 1800 Loss 1.3432 Accuracy 0.0916\n",
      "Epoch 2 Batch 1850 Loss 1.3425 Accuracy 0.0916\n",
      "Epoch 2 Batch 1900 Loss 1.3417 Accuracy 0.0915\n",
      "Epoch 2 Batch 1950 Loss 1.3406 Accuracy 0.0915\n",
      "Epoch 2 Batch 2000 Loss 1.3395 Accuracy 0.0915\n",
      "Epoch 2 Batch 2050 Loss 1.3378 Accuracy 0.0915\n",
      "Epoch 2 Batch 2100 Loss 1.3364 Accuracy 0.0916\n",
      "Epoch 2 Batch 2150 Loss 1.3366 Accuracy 0.0916\n",
      "Epoch 2 Batch 2200 Loss 1.3367 Accuracy 0.0917\n",
      "Epoch 2 Batch 2250 Loss 1.3361 Accuracy 0.0917\n",
      "Epoch 2 Batch 2300 Loss 1.3363 Accuracy 0.0918\n",
      "Epoch 2 Batch 2350 Loss 1.3362 Accuracy 0.0919\n",
      "Epoch 2 Batch 2400 Loss 1.3357 Accuracy 0.0920\n",
      "Epoch 2 Batch 2450 Loss 1.3359 Accuracy 0.0920\n",
      "Epoch 2 Batch 2500 Loss 1.3356 Accuracy 0.0921\n",
      "Epoch 2 Batch 2550 Loss 1.3352 Accuracy 0.0922\n",
      "Epoch 2 Batch 2600 Loss 1.3345 Accuracy 0.0922\n",
      "Epoch 2 Batch 2650 Loss 1.3343 Accuracy 0.0923\n",
      "Epoch 2 Batch 2700 Loss 1.3335 Accuracy 0.0923\n",
      "Epoch 2 Batch 2750 Loss 1.3334 Accuracy 0.0924\n",
      "Epoch 2 Batch 2800 Loss 1.3335 Accuracy 0.0924\n",
      "Epoch 2 Batch 2850 Loss 1.3331 Accuracy 0.0925\n",
      "Epoch 2 Batch 2900 Loss 1.3326 Accuracy 0.0925\n",
      "Epoch 2 Batch 2950 Loss 1.3325 Accuracy 0.0925\n",
      "Epoch 2 Batch 3000 Loss 1.3323 Accuracy 0.0926\n",
      "Epoch 2 Loss 1.3319 Accuracy 0.0927\n",
      "Time taken for 1 epoch: 183.95 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2155 Accuracy 0.0970\n",
      "Epoch 3 Batch 50 Loss 1.3201 Accuracy 0.0958\n",
      "Epoch 3 Batch 100 Loss 1.3147 Accuracy 0.0963\n",
      "Epoch 3 Batch 150 Loss 1.3009 Accuracy 0.0964\n",
      "Epoch 3 Batch 200 Loss 1.2926 Accuracy 0.0963\n",
      "Epoch 3 Batch 250 Loss 1.2841 Accuracy 0.0965\n",
      "Epoch 3 Batch 300 Loss 1.2840 Accuracy 0.0965\n",
      "Epoch 3 Batch 350 Loss 1.2823 Accuracy 0.0965\n",
      "Epoch 3 Batch 400 Loss 1.2804 Accuracy 0.0967\n",
      "Epoch 3 Batch 450 Loss 1.2790 Accuracy 0.0965\n",
      "Epoch 3 Batch 500 Loss 1.2804 Accuracy 0.0966\n",
      "Epoch 3 Batch 550 Loss 1.2792 Accuracy 0.0966\n",
      "Epoch 3 Batch 600 Loss 1.2785 Accuracy 0.0967\n",
      "Epoch 3 Batch 650 Loss 1.2776 Accuracy 0.0968\n",
      "Epoch 3 Batch 700 Loss 1.2807 Accuracy 0.0968\n",
      "Epoch 3 Batch 750 Loss 1.2802 Accuracy 0.0968\n",
      "Epoch 3 Batch 800 Loss 1.2812 Accuracy 0.0968\n",
      "Epoch 3 Batch 850 Loss 1.2831 Accuracy 0.0970\n",
      "Epoch 3 Batch 900 Loss 1.2842 Accuracy 0.0971\n",
      "Epoch 3 Batch 950 Loss 1.2839 Accuracy 0.0971\n",
      "Epoch 3 Batch 1000 Loss 1.2835 Accuracy 0.0971\n",
      "Epoch 3 Batch 1050 Loss 1.2835 Accuracy 0.0972\n",
      "Epoch 3 Batch 1100 Loss 1.2827 Accuracy 0.0971\n",
      "Epoch 3 Batch 1150 Loss 1.2818 Accuracy 0.0971\n",
      "Epoch 3 Batch 1200 Loss 1.2813 Accuracy 0.0971\n",
      "Epoch 3 Batch 1250 Loss 1.2792 Accuracy 0.0971\n",
      "Epoch 3 Batch 1300 Loss 1.2776 Accuracy 0.0972\n",
      "Epoch 3 Batch 1350 Loss 1.2760 Accuracy 0.0972\n",
      "Epoch 3 Batch 1400 Loss 1.2752 Accuracy 0.0972\n",
      "Epoch 3 Batch 1450 Loss 1.2741 Accuracy 0.0972\n",
      "Epoch 3 Batch 1500 Loss 1.2732 Accuracy 0.0971\n",
      "Epoch 3 Batch 1550 Loss 1.2718 Accuracy 0.0971\n",
      "Epoch 3 Batch 1600 Loss 1.2710 Accuracy 0.0971\n",
      "Epoch 3 Batch 1650 Loss 1.2703 Accuracy 0.0971\n",
      "Epoch 3 Batch 1700 Loss 1.2696 Accuracy 0.0971\n",
      "Epoch 3 Batch 1750 Loss 1.2699 Accuracy 0.0971\n",
      "Epoch 3 Batch 1800 Loss 1.2703 Accuracy 0.0971\n",
      "Epoch 3 Batch 1850 Loss 1.2702 Accuracy 0.0971\n",
      "Epoch 3 Batch 1900 Loss 1.2695 Accuracy 0.0971\n",
      "Epoch 3 Batch 1950 Loss 1.2695 Accuracy 0.0970\n",
      "Epoch 3 Batch 2000 Loss 1.2683 Accuracy 0.0970\n",
      "Epoch 3 Batch 2050 Loss 1.2670 Accuracy 0.0969\n",
      "Epoch 3 Batch 2100 Loss 1.2659 Accuracy 0.0969\n",
      "Epoch 3 Batch 2150 Loss 1.2662 Accuracy 0.0969\n",
      "Epoch 3 Batch 2200 Loss 1.2660 Accuracy 0.0969\n",
      "Epoch 3 Batch 2250 Loss 1.2666 Accuracy 0.0970\n",
      "Epoch 3 Batch 2300 Loss 1.2665 Accuracy 0.0970\n",
      "Epoch 3 Batch 2350 Loss 1.2667 Accuracy 0.0971\n",
      "Epoch 3 Batch 2400 Loss 1.2669 Accuracy 0.0971\n",
      "Epoch 3 Batch 2450 Loss 1.2672 Accuracy 0.0971\n",
      "Epoch 3 Batch 2500 Loss 1.2669 Accuracy 0.0971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 2550 Loss 1.2667 Accuracy 0.0972\n",
      "Epoch 3 Batch 2600 Loss 1.2667 Accuracy 0.0972\n",
      "Epoch 3 Batch 2650 Loss 1.2670 Accuracy 0.0972\n",
      "Epoch 3 Batch 2700 Loss 1.2671 Accuracy 0.0972\n",
      "Epoch 3 Batch 2750 Loss 1.2674 Accuracy 0.0973\n",
      "Epoch 3 Batch 2800 Loss 1.2681 Accuracy 0.0973\n",
      "Epoch 3 Batch 2850 Loss 1.2677 Accuracy 0.0974\n",
      "Epoch 3 Batch 2900 Loss 1.2674 Accuracy 0.0974\n",
      "Epoch 3 Batch 2950 Loss 1.2674 Accuracy 0.0974\n",
      "Epoch 3 Batch 3000 Loss 1.2677 Accuracy 0.0974\n",
      "Epoch 3 Loss 1.2679 Accuracy 0.0975\n",
      "Time taken for 1 epoch: 183.95 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2114 Accuracy 0.1046\n",
      "Epoch 4 Batch 50 Loss 1.2469 Accuracy 0.0984\n",
      "Epoch 4 Batch 100 Loss 1.2337 Accuracy 0.0988\n",
      "Epoch 4 Batch 150 Loss 1.2380 Accuracy 0.0985\n",
      "Epoch 4 Batch 200 Loss 1.2341 Accuracy 0.0987\n",
      "Epoch 4 Batch 250 Loss 1.2288 Accuracy 0.0991\n",
      "Epoch 4 Batch 300 Loss 1.2244 Accuracy 0.0993\n",
      "Epoch 4 Batch 350 Loss 1.2270 Accuracy 0.0995\n",
      "Epoch 4 Batch 400 Loss 1.2277 Accuracy 0.0997\n",
      "Epoch 4 Batch 450 Loss 1.2288 Accuracy 0.0998\n",
      "Epoch 4 Batch 500 Loss 1.2317 Accuracy 0.0999\n",
      "Epoch 4 Batch 550 Loss 1.2326 Accuracy 0.0998\n",
      "Epoch 4 Batch 600 Loss 1.2356 Accuracy 0.0999\n",
      "Epoch 4 Batch 650 Loss 1.2365 Accuracy 0.0999\n",
      "Epoch 4 Batch 700 Loss 1.2378 Accuracy 0.1000\n",
      "Epoch 4 Batch 750 Loss 1.2388 Accuracy 0.0999\n",
      "Epoch 4 Batch 800 Loss 1.2390 Accuracy 0.0999\n",
      "Epoch 4 Batch 850 Loss 1.2403 Accuracy 0.0999\n",
      "Epoch 4 Batch 900 Loss 1.2433 Accuracy 0.1000\n",
      "Epoch 4 Batch 950 Loss 1.2429 Accuracy 0.1000\n",
      "Epoch 4 Batch 1000 Loss 1.2435 Accuracy 0.1000\n",
      "Epoch 4 Batch 1050 Loss 1.2432 Accuracy 0.1001\n",
      "Epoch 4 Batch 1100 Loss 1.2438 Accuracy 0.1001\n",
      "Epoch 4 Batch 1150 Loss 1.2423 Accuracy 0.1001\n",
      "Epoch 4 Batch 1200 Loss 1.2410 Accuracy 0.1002\n",
      "Epoch 4 Batch 1250 Loss 1.2401 Accuracy 0.1001\n",
      "Epoch 4 Batch 1300 Loss 1.2386 Accuracy 0.1001\n",
      "Epoch 4 Batch 1350 Loss 1.2376 Accuracy 0.1002\n",
      "Epoch 4 Batch 1400 Loss 1.2368 Accuracy 0.1002\n",
      "Epoch 4 Batch 1450 Loss 1.2356 Accuracy 0.1002\n",
      "Epoch 4 Batch 1500 Loss 1.2354 Accuracy 0.1002\n",
      "Epoch 4 Batch 1550 Loss 1.2356 Accuracy 0.1001\n",
      "Epoch 4 Batch 1600 Loss 1.2346 Accuracy 0.1001\n",
      "Epoch 4 Batch 1650 Loss 1.2337 Accuracy 0.1001\n",
      "Epoch 4 Batch 1700 Loss 1.2328 Accuracy 0.1000\n",
      "Epoch 4 Batch 1750 Loss 1.2325 Accuracy 0.1000\n",
      "Epoch 4 Batch 1800 Loss 1.2325 Accuracy 0.1000\n",
      "Epoch 4 Batch 1850 Loss 1.2326 Accuracy 0.0999\n",
      "Epoch 4 Batch 1900 Loss 1.2329 Accuracy 0.0999\n",
      "Epoch 4 Batch 1950 Loss 1.2319 Accuracy 0.0999\n",
      "Epoch 4 Batch 2000 Loss 1.2310 Accuracy 0.0998\n",
      "Epoch 4 Batch 2050 Loss 1.2296 Accuracy 0.0997\n",
      "Epoch 4 Batch 2100 Loss 1.2295 Accuracy 0.0997\n",
      "Epoch 4 Batch 2150 Loss 1.2297 Accuracy 0.0997\n",
      "Epoch 4 Batch 2200 Loss 1.2297 Accuracy 0.0997\n",
      "Epoch 4 Batch 2250 Loss 1.2295 Accuracy 0.0998\n",
      "Epoch 4 Batch 2300 Loss 1.2295 Accuracy 0.0998\n",
      "Epoch 4 Batch 2350 Loss 1.2302 Accuracy 0.0999\n",
      "Epoch 4 Batch 2400 Loss 1.2309 Accuracy 0.1000\n",
      "Epoch 4 Batch 2450 Loss 1.2314 Accuracy 0.1000\n",
      "Epoch 4 Batch 2500 Loss 1.2313 Accuracy 0.1000\n",
      "Epoch 4 Batch 2550 Loss 1.2315 Accuracy 0.1000\n",
      "Epoch 4 Batch 2600 Loss 1.2314 Accuracy 0.1000\n",
      "Epoch 4 Batch 2650 Loss 1.2318 Accuracy 0.1000\n",
      "Epoch 4 Batch 2700 Loss 1.2315 Accuracy 0.1000\n",
      "Epoch 4 Batch 2750 Loss 1.2313 Accuracy 0.1000\n",
      "Epoch 4 Batch 2800 Loss 1.2315 Accuracy 0.1001\n",
      "Epoch 4 Batch 2850 Loss 1.2320 Accuracy 0.1001\n",
      "Epoch 4 Batch 2900 Loss 1.2320 Accuracy 0.1001\n",
      "Epoch 4 Batch 2950 Loss 1.2320 Accuracy 0.1001\n",
      "Epoch 4 Batch 3000 Loss 1.2321 Accuracy 0.1002\n",
      "Epoch 4 Loss 1.2324 Accuracy 0.1002\n",
      "Time taken for 1 epoch: 183.19 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.1981 Accuracy 0.1086\n",
      "Epoch 5 Batch 50 Loss 1.2657 Accuracy 0.1027\n",
      "Epoch 5 Batch 100 Loss 1.2426 Accuracy 0.1024\n",
      "Epoch 5 Batch 150 Loss 1.2305 Accuracy 0.1024\n",
      "Epoch 5 Batch 200 Loss 1.2194 Accuracy 0.1022\n",
      "Epoch 5 Batch 250 Loss 1.2163 Accuracy 0.1023\n",
      "Epoch 5 Batch 300 Loss 1.2117 Accuracy 0.1022\n",
      "Epoch 5 Batch 350 Loss 1.2099 Accuracy 0.1021\n",
      "Epoch 5 Batch 400 Loss 1.2125 Accuracy 0.1022\n",
      "Epoch 5 Batch 450 Loss 1.2129 Accuracy 0.1023\n",
      "Epoch 5 Batch 500 Loss 1.2130 Accuracy 0.1024\n",
      "Epoch 5 Batch 550 Loss 1.2115 Accuracy 0.1021\n",
      "Epoch 5 Batch 600 Loss 1.2120 Accuracy 0.1023\n",
      "Epoch 5 Batch 650 Loss 1.2116 Accuracy 0.1024\n",
      "Epoch 5 Batch 700 Loss 1.2114 Accuracy 0.1023\n",
      "Epoch 5 Batch 750 Loss 1.2125 Accuracy 0.1024\n",
      "Epoch 5 Batch 800 Loss 1.2147 Accuracy 0.1023\n",
      "Epoch 5 Batch 850 Loss 1.2149 Accuracy 0.1024\n",
      "Epoch 5 Batch 900 Loss 1.2160 Accuracy 0.1024\n",
      "Epoch 5 Batch 950 Loss 1.2177 Accuracy 0.1026\n",
      "Epoch 5 Batch 1000 Loss 1.2175 Accuracy 0.1025\n",
      "Epoch 5 Batch 1050 Loss 1.2156 Accuracy 0.1025\n",
      "Epoch 5 Batch 1100 Loss 1.2155 Accuracy 0.1025\n",
      "Epoch 5 Batch 1150 Loss 1.2154 Accuracy 0.1025\n",
      "Epoch 5 Batch 1200 Loss 1.2155 Accuracy 0.1025\n",
      "Epoch 5 Batch 1250 Loss 1.2133 Accuracy 0.1025\n",
      "Epoch 5 Batch 1300 Loss 1.2117 Accuracy 0.1024\n",
      "Epoch 5 Batch 1350 Loss 1.2097 Accuracy 0.1024\n",
      "Epoch 5 Batch 1400 Loss 1.2082 Accuracy 0.1024\n",
      "Epoch 5 Batch 1450 Loss 1.2072 Accuracy 0.1024\n",
      "Epoch 5 Batch 1500 Loss 1.2073 Accuracy 0.1023\n",
      "Epoch 5 Batch 1550 Loss 1.2075 Accuracy 0.1024\n",
      "Epoch 5 Batch 1600 Loss 1.2071 Accuracy 0.1023\n",
      "Epoch 5 Batch 1650 Loss 1.2066 Accuracy 0.1023\n",
      "Epoch 5 Batch 1700 Loss 1.2065 Accuracy 0.1023\n",
      "Epoch 5 Batch 1750 Loss 1.2057 Accuracy 0.1022\n",
      "Epoch 5 Batch 1800 Loss 1.2055 Accuracy 0.1021\n",
      "Epoch 5 Batch 1850 Loss 1.2060 Accuracy 0.1021\n",
      "Epoch 5 Batch 1900 Loss 1.2063 Accuracy 0.1021\n",
      "Epoch 5 Batch 1950 Loss 1.2065 Accuracy 0.1021\n",
      "Epoch 5 Batch 2000 Loss 1.2059 Accuracy 0.1021\n",
      "Epoch 5 Batch 2050 Loss 1.2047 Accuracy 0.1020\n",
      "Epoch 5 Batch 2100 Loss 1.2044 Accuracy 0.1020\n",
      "Epoch 5 Batch 2150 Loss 1.2048 Accuracy 0.1020\n",
      "Epoch 5 Batch 2200 Loss 1.2050 Accuracy 0.1020\n",
      "Epoch 5 Batch 2250 Loss 1.2057 Accuracy 0.1021\n",
      "Epoch 5 Batch 2300 Loss 1.2056 Accuracy 0.1021\n",
      "Epoch 5 Batch 2350 Loss 1.2064 Accuracy 0.1021\n",
      "Epoch 5 Batch 2400 Loss 1.2061 Accuracy 0.1021\n",
      "Epoch 5 Batch 2450 Loss 1.2063 Accuracy 0.1021\n",
      "Epoch 5 Batch 2500 Loss 1.2065 Accuracy 0.1021\n",
      "Epoch 5 Batch 2550 Loss 1.2067 Accuracy 0.1022\n",
      "Epoch 5 Batch 2600 Loss 1.2075 Accuracy 0.1022\n",
      "Epoch 5 Batch 2650 Loss 1.2079 Accuracy 0.1022\n",
      "Epoch 5 Batch 2700 Loss 1.2081 Accuracy 0.1022\n",
      "Epoch 5 Batch 2750 Loss 1.2086 Accuracy 0.1023\n",
      "Epoch 5 Batch 2800 Loss 1.2086 Accuracy 0.1023\n",
      "Epoch 5 Batch 2850 Loss 1.2086 Accuracy 0.1023\n",
      "Epoch 5 Batch 2900 Loss 1.2083 Accuracy 0.1022\n",
      "Epoch 5 Batch 2950 Loss 1.2082 Accuracy 0.1023\n",
      "Epoch 5 Batch 3000 Loss 1.2086 Accuracy 0.1023\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Epoch 5 Loss 1.2084 Accuracy 0.1023\n",
      "Time taken for 1 epoch: 184.01 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2678 Accuracy 0.1018\n",
      "Epoch 6 Batch 50 Loss 1.2127 Accuracy 0.1021\n",
      "Epoch 6 Batch 100 Loss 1.2034 Accuracy 0.1032\n",
      "Epoch 6 Batch 150 Loss 1.2063 Accuracy 0.1037\n",
      "Epoch 6 Batch 200 Loss 1.1978 Accuracy 0.1033\n",
      "Epoch 6 Batch 250 Loss 1.1948 Accuracy 0.1034\n",
      "Epoch 6 Batch 300 Loss 1.1939 Accuracy 0.1037\n",
      "Epoch 6 Batch 350 Loss 1.1885 Accuracy 0.1035\n",
      "Epoch 6 Batch 400 Loss 1.1868 Accuracy 0.1035\n",
      "Epoch 6 Batch 450 Loss 1.1852 Accuracy 0.1035\n",
      "Epoch 6 Batch 500 Loss 1.1875 Accuracy 0.1038\n",
      "Epoch 6 Batch 550 Loss 1.1863 Accuracy 0.1037\n",
      "Epoch 6 Batch 600 Loss 1.1884 Accuracy 0.1037\n",
      "Epoch 6 Batch 650 Loss 1.1870 Accuracy 0.1036\n",
      "Epoch 6 Batch 700 Loss 1.1900 Accuracy 0.1037\n",
      "Epoch 6 Batch 750 Loss 1.1928 Accuracy 0.1037\n",
      "Epoch 6 Batch 800 Loss 1.1930 Accuracy 0.1037\n",
      "Epoch 6 Batch 850 Loss 1.1939 Accuracy 0.1038\n",
      "Epoch 6 Batch 900 Loss 1.1922 Accuracy 0.1038\n",
      "Epoch 6 Batch 950 Loss 1.1930 Accuracy 0.1038\n",
      "Epoch 6 Batch 1000 Loss 1.1936 Accuracy 0.1038\n",
      "Epoch 6 Batch 1050 Loss 1.1942 Accuracy 0.1038\n",
      "Epoch 6 Batch 1100 Loss 1.1937 Accuracy 0.1039\n",
      "Epoch 6 Batch 1150 Loss 1.1932 Accuracy 0.1040\n",
      "Epoch 6 Batch 1200 Loss 1.1928 Accuracy 0.1040\n",
      "Epoch 6 Batch 1250 Loss 1.1924 Accuracy 0.1040\n",
      "Epoch 6 Batch 1300 Loss 1.1913 Accuracy 0.1040\n",
      "Epoch 6 Batch 1350 Loss 1.1916 Accuracy 0.1040\n",
      "Epoch 6 Batch 1400 Loss 1.1909 Accuracy 0.1040\n",
      "Epoch 6 Batch 1450 Loss 1.1906 Accuracy 0.1041\n",
      "Epoch 6 Batch 1500 Loss 1.1901 Accuracy 0.1041\n",
      "Epoch 6 Batch 1550 Loss 1.1904 Accuracy 0.1040\n",
      "Epoch 6 Batch 1600 Loss 1.1899 Accuracy 0.1040\n",
      "Epoch 6 Batch 1650 Loss 1.1893 Accuracy 0.1040\n",
      "Epoch 6 Batch 1700 Loss 1.1889 Accuracy 0.1039\n",
      "Epoch 6 Batch 1750 Loss 1.1891 Accuracy 0.1039\n",
      "Epoch 6 Batch 1800 Loss 1.1892 Accuracy 0.1039\n",
      "Epoch 6 Batch 1850 Loss 1.1885 Accuracy 0.1038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 1900 Loss 1.1879 Accuracy 0.1037\n",
      "Epoch 6 Batch 1950 Loss 1.1875 Accuracy 0.1037\n",
      "Epoch 6 Batch 2000 Loss 1.1871 Accuracy 0.1036\n",
      "Epoch 6 Batch 2050 Loss 1.1867 Accuracy 0.1036\n",
      "Epoch 6 Batch 2100 Loss 1.1859 Accuracy 0.1036\n",
      "Epoch 6 Batch 2150 Loss 1.1855 Accuracy 0.1035\n",
      "Epoch 6 Batch 2200 Loss 1.1863 Accuracy 0.1036\n",
      "Epoch 6 Batch 2250 Loss 1.1868 Accuracy 0.1036\n",
      "Epoch 6 Batch 2300 Loss 1.1867 Accuracy 0.1036\n",
      "Epoch 6 Batch 2350 Loss 1.1869 Accuracy 0.1036\n",
      "Epoch 6 Batch 2400 Loss 1.1867 Accuracy 0.1036\n",
      "Epoch 6 Batch 2450 Loss 1.1872 Accuracy 0.1036\n",
      "Epoch 6 Batch 2500 Loss 1.1878 Accuracy 0.1036\n",
      "Epoch 6 Batch 2550 Loss 1.1880 Accuracy 0.1036\n",
      "Epoch 6 Batch 2600 Loss 1.1881 Accuracy 0.1037\n",
      "Epoch 6 Batch 2650 Loss 1.1883 Accuracy 0.1037\n",
      "Epoch 6 Batch 2700 Loss 1.1883 Accuracy 0.1037\n",
      "Epoch 6 Batch 2750 Loss 1.1887 Accuracy 0.1037\n",
      "Epoch 6 Batch 2800 Loss 1.1895 Accuracy 0.1038\n",
      "Epoch 6 Batch 2850 Loss 1.1900 Accuracy 0.1038\n",
      "Epoch 6 Batch 2900 Loss 1.1904 Accuracy 0.1038\n",
      "Epoch 6 Batch 2950 Loss 1.1902 Accuracy 0.1038\n",
      "Epoch 6 Batch 3000 Loss 1.1906 Accuracy 0.1038\n",
      "Epoch 6 Loss 1.1906 Accuracy 0.1038\n",
      "Time taken for 1 epoch: 183.39 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.1752 Accuracy 0.1126\n",
      "Epoch 7 Batch 50 Loss 1.1884 Accuracy 0.1045\n",
      "Epoch 7 Batch 100 Loss 1.1701 Accuracy 0.1037\n",
      "Epoch 7 Batch 150 Loss 1.1682 Accuracy 0.1043\n",
      "Epoch 7 Batch 200 Loss 1.1703 Accuracy 0.1047\n",
      "Epoch 7 Batch 250 Loss 1.1655 Accuracy 0.1051\n",
      "Epoch 7 Batch 300 Loss 1.1675 Accuracy 0.1053\n",
      "Epoch 7 Batch 350 Loss 1.1683 Accuracy 0.1052\n",
      "Epoch 7 Batch 400 Loss 1.1635 Accuracy 0.1051\n",
      "Epoch 7 Batch 450 Loss 1.1627 Accuracy 0.1052\n",
      "Epoch 7 Batch 500 Loss 1.1665 Accuracy 0.1052\n",
      "Epoch 7 Batch 550 Loss 1.1677 Accuracy 0.1053\n",
      "Epoch 7 Batch 600 Loss 1.1664 Accuracy 0.1053\n",
      "Epoch 7 Batch 650 Loss 1.1664 Accuracy 0.1051\n",
      "Epoch 7 Batch 700 Loss 1.1684 Accuracy 0.1050\n",
      "Epoch 7 Batch 750 Loss 1.1697 Accuracy 0.1049\n",
      "Epoch 7 Batch 800 Loss 1.1730 Accuracy 0.1050\n",
      "Epoch 7 Batch 850 Loss 1.1757 Accuracy 0.1051\n",
      "Epoch 7 Batch 900 Loss 1.1773 Accuracy 0.1053\n",
      "Epoch 7 Batch 950 Loss 1.1783 Accuracy 0.1054\n",
      "Epoch 7 Batch 1000 Loss 1.1788 Accuracy 0.1054\n",
      "Epoch 7 Batch 1050 Loss 1.1788 Accuracy 0.1054\n",
      "Epoch 7 Batch 1100 Loss 1.1795 Accuracy 0.1054\n",
      "Epoch 7 Batch 1150 Loss 1.1798 Accuracy 0.1054\n",
      "Epoch 7 Batch 1200 Loss 1.1788 Accuracy 0.1055\n",
      "Epoch 7 Batch 1250 Loss 1.1769 Accuracy 0.1054\n",
      "Epoch 7 Batch 1300 Loss 1.1771 Accuracy 0.1054\n",
      "Epoch 7 Batch 1350 Loss 1.1765 Accuracy 0.1054\n",
      "Epoch 7 Batch 1400 Loss 1.1760 Accuracy 0.1053\n",
      "Epoch 7 Batch 1450 Loss 1.1754 Accuracy 0.1053\n",
      "Epoch 7 Batch 1500 Loss 1.1753 Accuracy 0.1053\n",
      "Epoch 7 Batch 1550 Loss 1.1746 Accuracy 0.1053\n",
      "Epoch 7 Batch 1600 Loss 1.1744 Accuracy 0.1053\n",
      "Epoch 7 Batch 1650 Loss 1.1736 Accuracy 0.1053\n",
      "Epoch 7 Batch 1700 Loss 1.1722 Accuracy 0.1052\n",
      "Epoch 7 Batch 1750 Loss 1.1720 Accuracy 0.1051\n",
      "Epoch 7 Batch 1800 Loss 1.1718 Accuracy 0.1051\n",
      "Epoch 7 Batch 1850 Loss 1.1730 Accuracy 0.1051\n",
      "Epoch 7 Batch 1900 Loss 1.1730 Accuracy 0.1051\n",
      "Epoch 7 Batch 1950 Loss 1.1729 Accuracy 0.1051\n",
      "Epoch 7 Batch 2000 Loss 1.1720 Accuracy 0.1049\n",
      "Epoch 7 Batch 2050 Loss 1.1713 Accuracy 0.1049\n",
      "Epoch 7 Batch 2100 Loss 1.1714 Accuracy 0.1049\n",
      "Epoch 7 Batch 2150 Loss 1.1716 Accuracy 0.1049\n",
      "Epoch 7 Batch 2200 Loss 1.1716 Accuracy 0.1048\n",
      "Epoch 7 Batch 2250 Loss 1.1717 Accuracy 0.1048\n",
      "Epoch 7 Batch 2300 Loss 1.1717 Accuracy 0.1048\n",
      "Epoch 7 Batch 2350 Loss 1.1715 Accuracy 0.1048\n",
      "Epoch 7 Batch 2400 Loss 1.1724 Accuracy 0.1049\n",
      "Epoch 7 Batch 2450 Loss 1.1726 Accuracy 0.1049\n",
      "Epoch 7 Batch 2500 Loss 1.1733 Accuracy 0.1049\n",
      "Epoch 7 Batch 2550 Loss 1.1737 Accuracy 0.1049\n",
      "Epoch 7 Batch 2600 Loss 1.1738 Accuracy 0.1049\n",
      "Epoch 7 Batch 2650 Loss 1.1742 Accuracy 0.1050\n",
      "Epoch 7 Batch 2700 Loss 1.1745 Accuracy 0.1050\n",
      "Epoch 7 Batch 2750 Loss 1.1746 Accuracy 0.1050\n",
      "Epoch 7 Batch 2800 Loss 1.1753 Accuracy 0.1050\n",
      "Epoch 7 Batch 2850 Loss 1.1753 Accuracy 0.1050\n",
      "Epoch 7 Batch 2900 Loss 1.1756 Accuracy 0.1050\n",
      "Epoch 7 Batch 2950 Loss 1.1758 Accuracy 0.1050\n",
      "Epoch 7 Batch 3000 Loss 1.1765 Accuracy 0.1051\n",
      "Epoch 7 Loss 1.1767 Accuracy 0.1051\n",
      "Time taken for 1 epoch: 182.96 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.1111 Accuracy 0.0950\n",
      "Epoch 8 Batch 50 Loss 1.1583 Accuracy 0.1047\n",
      "Epoch 8 Batch 100 Loss 1.1759 Accuracy 0.1059\n",
      "Epoch 8 Batch 150 Loss 1.1690 Accuracy 0.1059\n",
      "Epoch 8 Batch 200 Loss 1.1716 Accuracy 0.1063\n",
      "Epoch 8 Batch 250 Loss 1.1606 Accuracy 0.1057\n",
      "Epoch 8 Batch 300 Loss 1.1553 Accuracy 0.1056\n",
      "Epoch 8 Batch 350 Loss 1.1579 Accuracy 0.1060\n",
      "Epoch 8 Batch 400 Loss 1.1591 Accuracy 0.1063\n",
      "Epoch 8 Batch 450 Loss 1.1575 Accuracy 0.1062\n",
      "Epoch 8 Batch 500 Loss 1.1584 Accuracy 0.1063\n",
      "Epoch 8 Batch 550 Loss 1.1603 Accuracy 0.1063\n",
      "Epoch 8 Batch 600 Loss 1.1602 Accuracy 0.1062\n",
      "Epoch 8 Batch 650 Loss 1.1607 Accuracy 0.1062\n",
      "Epoch 8 Batch 700 Loss 1.1608 Accuracy 0.1062\n",
      "Epoch 8 Batch 750 Loss 1.1624 Accuracy 0.1062\n",
      "Epoch 8 Batch 800 Loss 1.1626 Accuracy 0.1062\n",
      "Epoch 8 Batch 850 Loss 1.1629 Accuracy 0.1062\n",
      "Epoch 8 Batch 900 Loss 1.1630 Accuracy 0.1061\n",
      "Epoch 8 Batch 950 Loss 1.1632 Accuracy 0.1062\n",
      "Epoch 8 Batch 1000 Loss 1.1640 Accuracy 0.1063\n",
      "Epoch 8 Batch 1050 Loss 1.1649 Accuracy 0.1063\n",
      "Epoch 8 Batch 1100 Loss 1.1654 Accuracy 0.1064\n",
      "Epoch 8 Batch 1150 Loss 1.1652 Accuracy 0.1065\n",
      "Epoch 8 Batch 1200 Loss 1.1650 Accuracy 0.1065\n",
      "Epoch 8 Batch 1250 Loss 1.1642 Accuracy 0.1064\n",
      "Epoch 8 Batch 1300 Loss 1.1629 Accuracy 0.1064\n",
      "Epoch 8 Batch 1350 Loss 1.1625 Accuracy 0.1064\n",
      "Epoch 8 Batch 1400 Loss 1.1627 Accuracy 0.1064\n",
      "Epoch 8 Batch 1450 Loss 1.1625 Accuracy 0.1065\n",
      "Epoch 8 Batch 1500 Loss 1.1627 Accuracy 0.1065\n",
      "Epoch 8 Batch 1550 Loss 1.1623 Accuracy 0.1064\n",
      "Epoch 8 Batch 1600 Loss 1.1620 Accuracy 0.1065\n",
      "Epoch 8 Batch 1650 Loss 1.1608 Accuracy 0.1064\n",
      "Epoch 8 Batch 1700 Loss 1.1601 Accuracy 0.1063\n",
      "Epoch 8 Batch 1750 Loss 1.1596 Accuracy 0.1062\n",
      "Epoch 8 Batch 1800 Loss 1.1595 Accuracy 0.1062\n",
      "Epoch 8 Batch 1850 Loss 1.1595 Accuracy 0.1061\n",
      "Epoch 8 Batch 1900 Loss 1.1598 Accuracy 0.1061\n",
      "Epoch 8 Batch 1950 Loss 1.1597 Accuracy 0.1060\n",
      "Epoch 8 Batch 2000 Loss 1.1597 Accuracy 0.1060\n",
      "Epoch 8 Batch 2050 Loss 1.1591 Accuracy 0.1058\n",
      "Epoch 8 Batch 2100 Loss 1.1595 Accuracy 0.1058\n",
      "Epoch 8 Batch 2150 Loss 1.1596 Accuracy 0.1059\n",
      "Epoch 8 Batch 2200 Loss 1.1598 Accuracy 0.1059\n",
      "Epoch 8 Batch 2250 Loss 1.1603 Accuracy 0.1059\n",
      "Epoch 8 Batch 2300 Loss 1.1603 Accuracy 0.1060\n",
      "Epoch 8 Batch 2350 Loss 1.1605 Accuracy 0.1059\n",
      "Epoch 8 Batch 2400 Loss 1.1613 Accuracy 0.1060\n",
      "Epoch 8 Batch 2450 Loss 1.1618 Accuracy 0.1060\n",
      "Epoch 8 Batch 2500 Loss 1.1622 Accuracy 0.1060\n",
      "Epoch 8 Batch 2550 Loss 1.1625 Accuracy 0.1060\n",
      "Epoch 8 Batch 2600 Loss 1.1627 Accuracy 0.1060\n",
      "Epoch 8 Batch 2650 Loss 1.1633 Accuracy 0.1060\n",
      "Epoch 8 Batch 2700 Loss 1.1630 Accuracy 0.1060\n",
      "Epoch 8 Batch 2750 Loss 1.1638 Accuracy 0.1061\n",
      "Epoch 8 Batch 2800 Loss 1.1644 Accuracy 0.1061\n",
      "Epoch 8 Batch 2850 Loss 1.1645 Accuracy 0.1061\n",
      "Epoch 8 Batch 2900 Loss 1.1647 Accuracy 0.1061\n",
      "Epoch 8 Batch 2950 Loss 1.1649 Accuracy 0.1061\n",
      "Epoch 8 Batch 3000 Loss 1.1650 Accuracy 0.1061\n",
      "Epoch 8 Loss 1.1652 Accuracy 0.1061\n",
      "Time taken for 1 epoch: 181.64 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0941 Accuracy 0.1050\n",
      "Epoch 9 Batch 50 Loss 1.1281 Accuracy 0.1039\n",
      "Epoch 9 Batch 100 Loss 1.1577 Accuracy 0.1069\n",
      "Epoch 9 Batch 150 Loss 1.1578 Accuracy 0.1074\n",
      "Epoch 9 Batch 200 Loss 1.1505 Accuracy 0.1071\n",
      "Epoch 9 Batch 250 Loss 1.1473 Accuracy 0.1069\n",
      "Epoch 9 Batch 300 Loss 1.1448 Accuracy 0.1070\n",
      "Epoch 9 Batch 350 Loss 1.1480 Accuracy 0.1074\n",
      "Epoch 9 Batch 400 Loss 1.1480 Accuracy 0.1074\n",
      "Epoch 9 Batch 450 Loss 1.1482 Accuracy 0.1073\n",
      "Epoch 9 Batch 500 Loss 1.1498 Accuracy 0.1073\n",
      "Epoch 9 Batch 550 Loss 1.1506 Accuracy 0.1074\n",
      "Epoch 9 Batch 600 Loss 1.1518 Accuracy 0.1075\n",
      "Epoch 9 Batch 650 Loss 1.1523 Accuracy 0.1075\n",
      "Epoch 9 Batch 700 Loss 1.1540 Accuracy 0.1076\n",
      "Epoch 9 Batch 750 Loss 1.1550 Accuracy 0.1076\n",
      "Epoch 9 Batch 800 Loss 1.1566 Accuracy 0.1076\n",
      "Epoch 9 Batch 850 Loss 1.1577 Accuracy 0.1077\n",
      "Epoch 9 Batch 900 Loss 1.1564 Accuracy 0.1076\n",
      "Epoch 9 Batch 950 Loss 1.1557 Accuracy 0.1076\n",
      "Epoch 9 Batch 1000 Loss 1.1565 Accuracy 0.1077\n",
      "Epoch 9 Batch 1050 Loss 1.1571 Accuracy 0.1078\n",
      "Epoch 9 Batch 1100 Loss 1.1564 Accuracy 0.1076\n",
      "Epoch 9 Batch 1150 Loss 1.1571 Accuracy 0.1078\n",
      "Epoch 9 Batch 1200 Loss 1.1568 Accuracy 0.1078\n",
      "Epoch 9 Batch 1250 Loss 1.1551 Accuracy 0.1076\n",
      "Epoch 9 Batch 1300 Loss 1.1531 Accuracy 0.1075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 1350 Loss 1.1521 Accuracy 0.1074\n",
      "Epoch 9 Batch 1400 Loss 1.1518 Accuracy 0.1075\n",
      "Epoch 9 Batch 1450 Loss 1.1516 Accuracy 0.1075\n",
      "Epoch 9 Batch 1500 Loss 1.1513 Accuracy 0.1075\n",
      "Epoch 9 Batch 1550 Loss 1.1516 Accuracy 0.1074\n",
      "Epoch 9 Batch 1600 Loss 1.1510 Accuracy 0.1074\n",
      "Epoch 9 Batch 1650 Loss 1.1500 Accuracy 0.1074\n",
      "Epoch 9 Batch 1700 Loss 1.1509 Accuracy 0.1074\n",
      "Epoch 9 Batch 1750 Loss 1.1506 Accuracy 0.1074\n",
      "Epoch 9 Batch 1800 Loss 1.1504 Accuracy 0.1074\n",
      "Epoch 9 Batch 1850 Loss 1.1506 Accuracy 0.1073\n",
      "Epoch 9 Batch 1900 Loss 1.1500 Accuracy 0.1072\n",
      "Epoch 9 Batch 1950 Loss 1.1496 Accuracy 0.1071\n",
      "Epoch 9 Batch 2000 Loss 1.1495 Accuracy 0.1071\n",
      "Epoch 9 Batch 2050 Loss 1.1488 Accuracy 0.1070\n",
      "Epoch 9 Batch 2100 Loss 1.1485 Accuracy 0.1070\n",
      "Epoch 9 Batch 2150 Loss 1.1490 Accuracy 0.1070\n",
      "Epoch 9 Batch 2200 Loss 1.1501 Accuracy 0.1070\n",
      "Epoch 9 Batch 2250 Loss 1.1510 Accuracy 0.1070\n",
      "Epoch 9 Batch 2300 Loss 1.1511 Accuracy 0.1071\n",
      "Epoch 9 Batch 2350 Loss 1.1519 Accuracy 0.1071\n",
      "Epoch 9 Batch 2400 Loss 1.1530 Accuracy 0.1072\n",
      "Epoch 9 Batch 2450 Loss 1.1530 Accuracy 0.1072\n",
      "Epoch 9 Batch 2500 Loss 1.1536 Accuracy 0.1072\n",
      "Epoch 9 Batch 2550 Loss 1.1538 Accuracy 0.1072\n",
      "Epoch 9 Batch 2600 Loss 1.1540 Accuracy 0.1072\n",
      "Epoch 9 Batch 2650 Loss 1.1539 Accuracy 0.1072\n",
      "Epoch 9 Batch 2700 Loss 1.1539 Accuracy 0.1072\n",
      "Epoch 9 Batch 2750 Loss 1.1537 Accuracy 0.1072\n",
      "Epoch 9 Batch 2800 Loss 1.1541 Accuracy 0.1071\n",
      "Epoch 9 Batch 2850 Loss 1.1548 Accuracy 0.1072\n",
      "Epoch 9 Batch 2900 Loss 1.1552 Accuracy 0.1072\n",
      "Epoch 9 Batch 2950 Loss 1.1553 Accuracy 0.1072\n",
      "Epoch 9 Batch 3000 Loss 1.1552 Accuracy 0.1072\n",
      "Epoch 9 Loss 1.1555 Accuracy 0.1072\n",
      "Time taken for 1 epoch: 179.34 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.0597 Accuracy 0.1058\n",
      "Epoch 10 Batch 50 Loss 1.1646 Accuracy 0.1085\n",
      "Epoch 10 Batch 100 Loss 1.1697 Accuracy 0.1087\n",
      "Epoch 10 Batch 150 Loss 1.1609 Accuracy 0.1085\n",
      "Epoch 10 Batch 200 Loss 1.1526 Accuracy 0.1085\n",
      "Epoch 10 Batch 250 Loss 1.1427 Accuracy 0.1082\n",
      "Epoch 10 Batch 300 Loss 1.1444 Accuracy 0.1085\n",
      "Epoch 10 Batch 350 Loss 1.1417 Accuracy 0.1084\n",
      "Epoch 10 Batch 400 Loss 1.1398 Accuracy 0.1083\n",
      "Epoch 10 Batch 450 Loss 1.1403 Accuracy 0.1084\n",
      "Epoch 10 Batch 500 Loss 1.1402 Accuracy 0.1084\n",
      "Epoch 10 Batch 550 Loss 1.1419 Accuracy 0.1085\n",
      "Epoch 10 Batch 600 Loss 1.1401 Accuracy 0.1083\n",
      "Epoch 10 Batch 650 Loss 1.1393 Accuracy 0.1081\n",
      "Epoch 10 Batch 700 Loss 1.1411 Accuracy 0.1081\n",
      "Epoch 10 Batch 750 Loss 1.1416 Accuracy 0.1081\n",
      "Epoch 10 Batch 800 Loss 1.1445 Accuracy 0.1083\n",
      "Epoch 10 Batch 850 Loss 1.1456 Accuracy 0.1084\n",
      "Epoch 10 Batch 900 Loss 1.1470 Accuracy 0.1084\n",
      "Epoch 10 Batch 950 Loss 1.1478 Accuracy 0.1085\n",
      "Epoch 10 Batch 1000 Loss 1.1468 Accuracy 0.1084\n",
      "Epoch 10 Batch 1050 Loss 1.1458 Accuracy 0.1083\n",
      "Epoch 10 Batch 1100 Loss 1.1450 Accuracy 0.1084\n",
      "Epoch 10 Batch 1150 Loss 1.1458 Accuracy 0.1084\n",
      "Epoch 10 Batch 1200 Loss 1.1455 Accuracy 0.1084\n",
      "Epoch 10 Batch 1250 Loss 1.1445 Accuracy 0.1084\n",
      "Epoch 10 Batch 1300 Loss 1.1432 Accuracy 0.1084\n",
      "Epoch 10 Batch 1350 Loss 1.1417 Accuracy 0.1084\n",
      "Epoch 10 Batch 1400 Loss 1.1415 Accuracy 0.1083\n",
      "Epoch 10 Batch 1450 Loss 1.1415 Accuracy 0.1083\n",
      "Epoch 10 Batch 1500 Loss 1.1419 Accuracy 0.1083\n",
      "Epoch 10 Batch 1550 Loss 1.1413 Accuracy 0.1083\n",
      "Epoch 10 Batch 1600 Loss 1.1409 Accuracy 0.1083\n",
      "Epoch 10 Batch 1650 Loss 1.1403 Accuracy 0.1081\n",
      "Epoch 10 Batch 1700 Loss 1.1400 Accuracy 0.1081\n",
      "Epoch 10 Batch 1750 Loss 1.1405 Accuracy 0.1081\n",
      "Epoch 10 Batch 1800 Loss 1.1401 Accuracy 0.1081\n",
      "Epoch 10 Batch 1850 Loss 1.1404 Accuracy 0.1080\n",
      "Epoch 10 Batch 1900 Loss 1.1404 Accuracy 0.1079\n",
      "Epoch 10 Batch 1950 Loss 1.1404 Accuracy 0.1079\n",
      "Epoch 10 Batch 2000 Loss 1.1405 Accuracy 0.1078\n",
      "Epoch 10 Batch 2050 Loss 1.1406 Accuracy 0.1078\n",
      "Epoch 10 Batch 2100 Loss 1.1404 Accuracy 0.1077\n",
      "Epoch 10 Batch 2150 Loss 1.1406 Accuracy 0.1077\n",
      "Epoch 10 Batch 2200 Loss 1.1411 Accuracy 0.1077\n",
      "Epoch 10 Batch 2250 Loss 1.1420 Accuracy 0.1078\n",
      "Epoch 10 Batch 2300 Loss 1.1423 Accuracy 0.1078\n",
      "Epoch 10 Batch 2350 Loss 1.1431 Accuracy 0.1078\n",
      "Epoch 10 Batch 2400 Loss 1.1434 Accuracy 0.1078\n",
      "Epoch 10 Batch 2450 Loss 1.1441 Accuracy 0.1078\n",
      "Epoch 10 Batch 2500 Loss 1.1441 Accuracy 0.1078\n",
      "Epoch 10 Batch 2550 Loss 1.1443 Accuracy 0.1078\n",
      "Epoch 10 Batch 2600 Loss 1.1442 Accuracy 0.1078\n",
      "Epoch 10 Batch 2650 Loss 1.1445 Accuracy 0.1078\n",
      "Epoch 10 Batch 2700 Loss 1.1450 Accuracy 0.1079\n",
      "Epoch 10 Batch 2750 Loss 1.1454 Accuracy 0.1079\n",
      "Epoch 10 Batch 2800 Loss 1.1453 Accuracy 0.1079\n",
      "Epoch 10 Batch 2850 Loss 1.1454 Accuracy 0.1079\n",
      "Epoch 10 Batch 2900 Loss 1.1457 Accuracy 0.1079\n",
      "Epoch 10 Batch 2950 Loss 1.1463 Accuracy 0.1080\n",
      "Epoch 10 Batch 3000 Loss 1.1466 Accuracy 0.1080\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Epoch 10 Loss 1.1471 Accuracy 0.1080\n",
      "Time taken for 1 epoch: 179.67 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.2585 Accuracy 0.1134\n",
      "Epoch 11 Batch 50 Loss 1.1399 Accuracy 0.1076\n",
      "Epoch 11 Batch 100 Loss 1.1489 Accuracy 0.1088\n",
      "Epoch 11 Batch 150 Loss 1.1489 Accuracy 0.1091\n",
      "Epoch 11 Batch 200 Loss 1.1419 Accuracy 0.1092\n",
      "Epoch 11 Batch 250 Loss 1.1361 Accuracy 0.1089\n",
      "Epoch 11 Batch 300 Loss 1.1338 Accuracy 0.1092\n",
      "Epoch 11 Batch 350 Loss 1.1316 Accuracy 0.1092\n",
      "Epoch 11 Batch 400 Loss 1.1305 Accuracy 0.1093\n",
      "Epoch 11 Batch 450 Loss 1.1292 Accuracy 0.1093\n",
      "Epoch 11 Batch 500 Loss 1.1314 Accuracy 0.1094\n",
      "Epoch 11 Batch 550 Loss 1.1341 Accuracy 0.1095\n",
      "Epoch 11 Batch 600 Loss 1.1336 Accuracy 0.1093\n",
      "Epoch 11 Batch 650 Loss 1.1339 Accuracy 0.1092\n",
      "Epoch 11 Batch 700 Loss 1.1340 Accuracy 0.1092\n",
      "Epoch 11 Batch 750 Loss 1.1343 Accuracy 0.1092\n",
      "Epoch 11 Batch 800 Loss 1.1349 Accuracy 0.1091\n",
      "Epoch 11 Batch 850 Loss 1.1352 Accuracy 0.1090\n",
      "Epoch 11 Batch 900 Loss 1.1355 Accuracy 0.1091\n",
      "Epoch 11 Batch 950 Loss 1.1364 Accuracy 0.1090\n",
      "Epoch 11 Batch 1000 Loss 1.1365 Accuracy 0.1090\n",
      "Epoch 11 Batch 1050 Loss 1.1368 Accuracy 0.1090\n",
      "Epoch 11 Batch 1100 Loss 1.1381 Accuracy 0.1091\n",
      "Epoch 11 Batch 1150 Loss 1.1383 Accuracy 0.1092\n",
      "Epoch 11 Batch 1200 Loss 1.1379 Accuracy 0.1091\n",
      "Epoch 11 Batch 1250 Loss 1.1383 Accuracy 0.1092\n",
      "Epoch 11 Batch 1300 Loss 1.1381 Accuracy 0.1092\n",
      "Epoch 11 Batch 1350 Loss 1.1372 Accuracy 0.1092\n",
      "Epoch 11 Batch 1400 Loss 1.1363 Accuracy 0.1091\n",
      "Epoch 11 Batch 1450 Loss 1.1352 Accuracy 0.1091\n",
      "Epoch 11 Batch 1500 Loss 1.1356 Accuracy 0.1091\n",
      "Epoch 11 Batch 1550 Loss 1.1358 Accuracy 0.1091\n",
      "Epoch 11 Batch 1600 Loss 1.1353 Accuracy 0.1090\n",
      "Epoch 11 Batch 1650 Loss 1.1351 Accuracy 0.1090\n",
      "Epoch 11 Batch 1700 Loss 1.1343 Accuracy 0.1089\n",
      "Epoch 11 Batch 1750 Loss 1.1350 Accuracy 0.1089\n",
      "Epoch 11 Batch 1800 Loss 1.1355 Accuracy 0.1089\n",
      "Epoch 11 Batch 1850 Loss 1.1360 Accuracy 0.1089\n",
      "Epoch 11 Batch 1900 Loss 1.1363 Accuracy 0.1088\n",
      "Epoch 11 Batch 1950 Loss 1.1356 Accuracy 0.1087\n",
      "Epoch 11 Batch 2000 Loss 1.1346 Accuracy 0.1087\n",
      "Epoch 11 Batch 2050 Loss 1.1347 Accuracy 0.1086\n",
      "Epoch 11 Batch 2100 Loss 1.1337 Accuracy 0.1086\n",
      "Epoch 11 Batch 2150 Loss 1.1348 Accuracy 0.1086\n",
      "Epoch 11 Batch 2200 Loss 1.1357 Accuracy 0.1087\n",
      "Epoch 11 Batch 2250 Loss 1.1358 Accuracy 0.1087\n",
      "Epoch 11 Batch 2300 Loss 1.1358 Accuracy 0.1086\n",
      "Epoch 11 Batch 2350 Loss 1.1367 Accuracy 0.1086\n",
      "Epoch 11 Batch 2400 Loss 1.1366 Accuracy 0.1086\n",
      "Epoch 11 Batch 2450 Loss 1.1372 Accuracy 0.1087\n",
      "Epoch 11 Batch 2500 Loss 1.1378 Accuracy 0.1087\n",
      "Epoch 11 Batch 2550 Loss 1.1380 Accuracy 0.1087\n",
      "Epoch 11 Batch 2600 Loss 1.1374 Accuracy 0.1086\n",
      "Epoch 11 Batch 2650 Loss 1.1379 Accuracy 0.1087\n",
      "Epoch 11 Batch 2700 Loss 1.1379 Accuracy 0.1087\n",
      "Epoch 11 Batch 2750 Loss 1.1385 Accuracy 0.1087\n",
      "Epoch 11 Batch 2800 Loss 1.1389 Accuracy 0.1087\n",
      "Epoch 11 Batch 2850 Loss 1.1390 Accuracy 0.1087\n",
      "Epoch 11 Batch 2900 Loss 1.1392 Accuracy 0.1087\n",
      "Epoch 11 Batch 2950 Loss 1.1398 Accuracy 0.1088\n",
      "Epoch 11 Batch 3000 Loss 1.1401 Accuracy 0.1088\n",
      "Epoch 11 Loss 1.1398 Accuracy 0.1088\n",
      "Time taken for 1 epoch: 180.34 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.2127 Accuracy 0.1138\n",
      "Epoch 12 Batch 50 Loss 1.1364 Accuracy 0.1099\n",
      "Epoch 12 Batch 100 Loss 1.1463 Accuracy 0.1107\n",
      "Epoch 12 Batch 150 Loss 1.1361 Accuracy 0.1103\n",
      "Epoch 12 Batch 200 Loss 1.1356 Accuracy 0.1103\n",
      "Epoch 12 Batch 250 Loss 1.1395 Accuracy 0.1105\n",
      "Epoch 12 Batch 300 Loss 1.1327 Accuracy 0.1102\n",
      "Epoch 12 Batch 350 Loss 1.1307 Accuracy 0.1098\n",
      "Epoch 12 Batch 400 Loss 1.1321 Accuracy 0.1099\n",
      "Epoch 12 Batch 450 Loss 1.1294 Accuracy 0.1098\n",
      "Epoch 12 Batch 500 Loss 1.1274 Accuracy 0.1096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 550 Loss 1.1282 Accuracy 0.1096\n",
      "Epoch 12 Batch 600 Loss 1.1302 Accuracy 0.1096\n",
      "Epoch 12 Batch 650 Loss 1.1308 Accuracy 0.1098\n",
      "Epoch 12 Batch 700 Loss 1.1287 Accuracy 0.1097\n",
      "Epoch 12 Batch 750 Loss 1.1285 Accuracy 0.1096\n",
      "Epoch 12 Batch 800 Loss 1.1300 Accuracy 0.1096\n",
      "Epoch 12 Batch 850 Loss 1.1316 Accuracy 0.1097\n",
      "Epoch 12 Batch 900 Loss 1.1325 Accuracy 0.1098\n",
      "Epoch 12 Batch 950 Loss 1.1306 Accuracy 0.1097\n",
      "Epoch 12 Batch 1000 Loss 1.1327 Accuracy 0.1098\n",
      "Epoch 12 Batch 1050 Loss 1.1330 Accuracy 0.1098\n",
      "Epoch 12 Batch 1100 Loss 1.1332 Accuracy 0.1098\n",
      "Epoch 12 Batch 1150 Loss 1.1326 Accuracy 0.1098\n",
      "Epoch 12 Batch 1200 Loss 1.1336 Accuracy 0.1099\n",
      "Epoch 12 Batch 1250 Loss 1.1323 Accuracy 0.1098\n",
      "Epoch 12 Batch 1300 Loss 1.1305 Accuracy 0.1097\n",
      "Epoch 12 Batch 1350 Loss 1.1294 Accuracy 0.1097\n",
      "Epoch 12 Batch 1400 Loss 1.1288 Accuracy 0.1097\n",
      "Epoch 12 Batch 1450 Loss 1.1289 Accuracy 0.1097\n",
      "Epoch 12 Batch 1500 Loss 1.1285 Accuracy 0.1096\n",
      "Epoch 12 Batch 1550 Loss 1.1275 Accuracy 0.1096\n",
      "Epoch 12 Batch 1600 Loss 1.1271 Accuracy 0.1096\n",
      "Epoch 12 Batch 1650 Loss 1.1268 Accuracy 0.1095\n",
      "Epoch 12 Batch 1700 Loss 1.1273 Accuracy 0.1095\n",
      "Epoch 12 Batch 1750 Loss 1.1279 Accuracy 0.1095\n",
      "Epoch 12 Batch 1800 Loss 1.1285 Accuracy 0.1095\n",
      "Epoch 12 Batch 1850 Loss 1.1288 Accuracy 0.1095\n",
      "Epoch 12 Batch 1900 Loss 1.1287 Accuracy 0.1094\n",
      "Epoch 12 Batch 1950 Loss 1.1280 Accuracy 0.1093\n",
      "Epoch 12 Batch 2000 Loss 1.1277 Accuracy 0.1092\n",
      "Epoch 12 Batch 2050 Loss 1.1277 Accuracy 0.1092\n",
      "Epoch 12 Batch 2100 Loss 1.1274 Accuracy 0.1092\n",
      "Epoch 12 Batch 2150 Loss 1.1286 Accuracy 0.1092\n",
      "Epoch 12 Batch 2200 Loss 1.1286 Accuracy 0.1092\n",
      "Epoch 12 Batch 2250 Loss 1.1288 Accuracy 0.1092\n",
      "Epoch 12 Batch 2300 Loss 1.1292 Accuracy 0.1092\n",
      "Epoch 12 Batch 2350 Loss 1.1294 Accuracy 0.1092\n",
      "Epoch 12 Batch 2400 Loss 1.1303 Accuracy 0.1092\n",
      "Epoch 12 Batch 2450 Loss 1.1305 Accuracy 0.1093\n",
      "Epoch 12 Batch 2500 Loss 1.1308 Accuracy 0.1093\n",
      "Epoch 12 Batch 2550 Loss 1.1312 Accuracy 0.1093\n",
      "Epoch 12 Batch 2600 Loss 1.1317 Accuracy 0.1093\n",
      "Epoch 12 Batch 2650 Loss 1.1314 Accuracy 0.1093\n",
      "Epoch 12 Batch 2700 Loss 1.1319 Accuracy 0.1093\n",
      "Epoch 12 Batch 2750 Loss 1.1321 Accuracy 0.1093\n",
      "Epoch 12 Batch 2800 Loss 1.1323 Accuracy 0.1093\n",
      "Epoch 12 Batch 2850 Loss 1.1327 Accuracy 0.1094\n",
      "Epoch 12 Batch 2900 Loss 1.1326 Accuracy 0.1094\n",
      "Epoch 12 Batch 2950 Loss 1.1326 Accuracy 0.1094\n",
      "Epoch 12 Batch 3000 Loss 1.1331 Accuracy 0.1094\n",
      "Epoch 12 Loss 1.1335 Accuracy 0.1094\n",
      "Time taken for 1 epoch: 183.12 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.2729 Accuracy 0.1330\n",
      "Epoch 13 Batch 50 Loss 1.1374 Accuracy 0.1107\n",
      "Epoch 13 Batch 100 Loss 1.1282 Accuracy 0.1104\n",
      "Epoch 13 Batch 150 Loss 1.1232 Accuracy 0.1102\n",
      "Epoch 13 Batch 200 Loss 1.1231 Accuracy 0.1105\n",
      "Epoch 13 Batch 250 Loss 1.1262 Accuracy 0.1104\n",
      "Epoch 13 Batch 300 Loss 1.1253 Accuracy 0.1104\n",
      "Epoch 13 Batch 350 Loss 1.1248 Accuracy 0.1103\n",
      "Epoch 13 Batch 400 Loss 1.1219 Accuracy 0.1104\n",
      "Epoch 13 Batch 450 Loss 1.1212 Accuracy 0.1104\n",
      "Epoch 13 Batch 500 Loss 1.1211 Accuracy 0.1103\n",
      "Epoch 13 Batch 550 Loss 1.1232 Accuracy 0.1104\n",
      "Epoch 13 Batch 600 Loss 1.1247 Accuracy 0.1105\n",
      "Epoch 13 Batch 650 Loss 1.1235 Accuracy 0.1105\n",
      "Epoch 13 Batch 700 Loss 1.1236 Accuracy 0.1104\n",
      "Epoch 13 Batch 750 Loss 1.1233 Accuracy 0.1103\n",
      "Epoch 13 Batch 800 Loss 1.1237 Accuracy 0.1104\n",
      "Epoch 13 Batch 850 Loss 1.1258 Accuracy 0.1105\n",
      "Epoch 13 Batch 900 Loss 1.1275 Accuracy 0.1106\n",
      "Epoch 13 Batch 950 Loss 1.1271 Accuracy 0.1105\n",
      "Epoch 13 Batch 1000 Loss 1.1274 Accuracy 0.1105\n",
      "Epoch 13 Batch 1050 Loss 1.1276 Accuracy 0.1105\n",
      "Epoch 13 Batch 1100 Loss 1.1281 Accuracy 0.1106\n",
      "Epoch 13 Batch 1150 Loss 1.1266 Accuracy 0.1105\n",
      "Epoch 13 Batch 1200 Loss 1.1268 Accuracy 0.1106\n",
      "Epoch 13 Batch 1250 Loss 1.1263 Accuracy 0.1107\n",
      "Epoch 13 Batch 1300 Loss 1.1249 Accuracy 0.1107\n",
      "Epoch 13 Batch 1350 Loss 1.1243 Accuracy 0.1106\n",
      "Epoch 13 Batch 1400 Loss 1.1245 Accuracy 0.1106\n",
      "Epoch 13 Batch 1450 Loss 1.1241 Accuracy 0.1106\n",
      "Epoch 13 Batch 1500 Loss 1.1240 Accuracy 0.1106\n",
      "Epoch 13 Batch 1550 Loss 1.1240 Accuracy 0.1106\n",
      "Epoch 13 Batch 1600 Loss 1.1233 Accuracy 0.1105\n",
      "Epoch 13 Batch 1650 Loss 1.1225 Accuracy 0.1104\n",
      "Epoch 13 Batch 1700 Loss 1.1229 Accuracy 0.1105\n",
      "Epoch 13 Batch 1750 Loss 1.1226 Accuracy 0.1104\n",
      "Epoch 13 Batch 1800 Loss 1.1224 Accuracy 0.1103\n",
      "Epoch 13 Batch 1850 Loss 1.1230 Accuracy 0.1103\n",
      "Epoch 13 Batch 1900 Loss 1.1230 Accuracy 0.1102\n",
      "Epoch 13 Batch 1950 Loss 1.1228 Accuracy 0.1101\n",
      "Epoch 13 Batch 2000 Loss 1.1231 Accuracy 0.1101\n",
      "Epoch 13 Batch 2050 Loss 1.1222 Accuracy 0.1100\n",
      "Epoch 13 Batch 2100 Loss 1.1227 Accuracy 0.1100\n",
      "Epoch 13 Batch 2150 Loss 1.1230 Accuracy 0.1100\n",
      "Epoch 13 Batch 2200 Loss 1.1231 Accuracy 0.1100\n",
      "Epoch 13 Batch 2250 Loss 1.1234 Accuracy 0.1101\n",
      "Epoch 13 Batch 2300 Loss 1.1232 Accuracy 0.1101\n",
      "Epoch 13 Batch 2350 Loss 1.1233 Accuracy 0.1100\n",
      "Epoch 13 Batch 2400 Loss 1.1240 Accuracy 0.1100\n",
      "Epoch 13 Batch 2450 Loss 1.1247 Accuracy 0.1101\n",
      "Epoch 13 Batch 2500 Loss 1.1247 Accuracy 0.1100\n",
      "Epoch 13 Batch 2550 Loss 1.1251 Accuracy 0.1101\n",
      "Epoch 13 Batch 2600 Loss 1.1256 Accuracy 0.1101\n",
      "Epoch 13 Batch 2650 Loss 1.1266 Accuracy 0.1101\n",
      "Epoch 13 Batch 2700 Loss 1.1265 Accuracy 0.1101\n",
      "Epoch 13 Batch 2750 Loss 1.1264 Accuracy 0.1101\n",
      "Epoch 13 Batch 2800 Loss 1.1264 Accuracy 0.1101\n",
      "Epoch 13 Batch 2850 Loss 1.1264 Accuracy 0.1101\n",
      "Epoch 13 Batch 2900 Loss 1.1269 Accuracy 0.1102\n",
      "Epoch 13 Batch 2950 Loss 1.1273 Accuracy 0.1102\n",
      "Epoch 13 Batch 3000 Loss 1.1275 Accuracy 0.1102\n",
      "Epoch 13 Loss 1.1277 Accuracy 0.1102\n",
      "Time taken for 1 epoch: 182.82 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.1913 Accuracy 0.1114\n",
      "Epoch 14 Batch 50 Loss 1.1474 Accuracy 0.1112\n",
      "Epoch 14 Batch 100 Loss 1.1328 Accuracy 0.1108\n",
      "Epoch 14 Batch 150 Loss 1.1228 Accuracy 0.1106\n",
      "Epoch 14 Batch 200 Loss 1.1158 Accuracy 0.1104\n",
      "Epoch 14 Batch 250 Loss 1.1118 Accuracy 0.1105\n",
      "Epoch 14 Batch 300 Loss 1.1146 Accuracy 0.1107\n",
      "Epoch 14 Batch 350 Loss 1.1135 Accuracy 0.1109\n",
      "Epoch 14 Batch 400 Loss 1.1137 Accuracy 0.1110\n",
      "Epoch 14 Batch 450 Loss 1.1144 Accuracy 0.1108\n",
      "Epoch 14 Batch 500 Loss 1.1159 Accuracy 0.1109\n",
      "Epoch 14 Batch 550 Loss 1.1170 Accuracy 0.1111\n",
      "Epoch 14 Batch 600 Loss 1.1175 Accuracy 0.1111\n",
      "Epoch 14 Batch 650 Loss 1.1173 Accuracy 0.1112\n",
      "Epoch 14 Batch 700 Loss 1.1169 Accuracy 0.1110\n",
      "Epoch 14 Batch 750 Loss 1.1186 Accuracy 0.1110\n",
      "Epoch 14 Batch 800 Loss 1.1197 Accuracy 0.1110\n",
      "Epoch 14 Batch 850 Loss 1.1194 Accuracy 0.1110\n",
      "Epoch 14 Batch 900 Loss 1.1186 Accuracy 0.1111\n",
      "Epoch 14 Batch 950 Loss 1.1189 Accuracy 0.1111\n",
      "Epoch 14 Batch 1000 Loss 1.1214 Accuracy 0.1112\n",
      "Epoch 14 Batch 1050 Loss 1.1224 Accuracy 0.1113\n",
      "Epoch 14 Batch 1100 Loss 1.1227 Accuracy 0.1112\n",
      "Epoch 14 Batch 1150 Loss 1.1235 Accuracy 0.1113\n",
      "Epoch 14 Batch 1200 Loss 1.1225 Accuracy 0.1112\n",
      "Epoch 14 Batch 1250 Loss 1.1215 Accuracy 0.1111\n",
      "Epoch 14 Batch 1300 Loss 1.1201 Accuracy 0.1111\n",
      "Epoch 14 Batch 1350 Loss 1.1190 Accuracy 0.1110\n",
      "Epoch 14 Batch 1400 Loss 1.1179 Accuracy 0.1109\n",
      "Epoch 14 Batch 1450 Loss 1.1185 Accuracy 0.1110\n",
      "Epoch 14 Batch 1500 Loss 1.1189 Accuracy 0.1110\n",
      "Epoch 14 Batch 1550 Loss 1.1192 Accuracy 0.1111\n",
      "Epoch 14 Batch 1600 Loss 1.1194 Accuracy 0.1111\n",
      "Epoch 14 Batch 1650 Loss 1.1184 Accuracy 0.1110\n",
      "Epoch 14 Batch 1700 Loss 1.1174 Accuracy 0.1109\n",
      "Epoch 14 Batch 1750 Loss 1.1173 Accuracy 0.1108\n",
      "Epoch 14 Batch 1800 Loss 1.1171 Accuracy 0.1108\n",
      "Epoch 14 Batch 1850 Loss 1.1172 Accuracy 0.1107\n",
      "Epoch 14 Batch 1900 Loss 1.1171 Accuracy 0.1106\n",
      "Epoch 14 Batch 1950 Loss 1.1172 Accuracy 0.1106\n",
      "Epoch 14 Batch 2000 Loss 1.1172 Accuracy 0.1105\n",
      "Epoch 14 Batch 2050 Loss 1.1170 Accuracy 0.1105\n",
      "Epoch 14 Batch 2100 Loss 1.1170 Accuracy 0.1104\n",
      "Epoch 14 Batch 2150 Loss 1.1171 Accuracy 0.1104\n",
      "Epoch 14 Batch 2200 Loss 1.1175 Accuracy 0.1104\n",
      "Epoch 14 Batch 2250 Loss 1.1177 Accuracy 0.1104\n",
      "Epoch 14 Batch 2300 Loss 1.1178 Accuracy 0.1104\n",
      "Epoch 14 Batch 2350 Loss 1.1186 Accuracy 0.1104\n",
      "Epoch 14 Batch 2400 Loss 1.1190 Accuracy 0.1105\n",
      "Epoch 14 Batch 2450 Loss 1.1198 Accuracy 0.1105\n",
      "Epoch 14 Batch 2500 Loss 1.1201 Accuracy 0.1105\n",
      "Epoch 14 Batch 2550 Loss 1.1207 Accuracy 0.1105\n",
      "Epoch 14 Batch 2600 Loss 1.1207 Accuracy 0.1105\n",
      "Epoch 14 Batch 2650 Loss 1.1207 Accuracy 0.1105\n",
      "Epoch 14 Batch 2700 Loss 1.1206 Accuracy 0.1106\n",
      "Epoch 14 Batch 2750 Loss 1.1210 Accuracy 0.1106\n",
      "Epoch 14 Batch 2800 Loss 1.1212 Accuracy 0.1106\n",
      "Epoch 14 Batch 2850 Loss 1.1216 Accuracy 0.1107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 2900 Loss 1.1222 Accuracy 0.1107\n",
      "Epoch 14 Batch 2950 Loss 1.1229 Accuracy 0.1107\n",
      "Epoch 14 Batch 3000 Loss 1.1231 Accuracy 0.1107\n",
      "Epoch 14 Loss 1.1231 Accuracy 0.1107\n",
      "Time taken for 1 epoch: 179.61 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.9642 Accuracy 0.1066\n",
      "Epoch 15 Batch 50 Loss 1.1455 Accuracy 0.1120\n",
      "Epoch 15 Batch 100 Loss 1.1247 Accuracy 0.1115\n",
      "Epoch 15 Batch 150 Loss 1.1220 Accuracy 0.1119\n",
      "Epoch 15 Batch 200 Loss 1.1236 Accuracy 0.1121\n",
      "Epoch 15 Batch 250 Loss 1.1191 Accuracy 0.1118\n",
      "Epoch 15 Batch 300 Loss 1.1131 Accuracy 0.1117\n",
      "Epoch 15 Batch 350 Loss 1.1110 Accuracy 0.1115\n",
      "Epoch 15 Batch 400 Loss 1.1091 Accuracy 0.1116\n",
      "Epoch 15 Batch 450 Loss 1.1070 Accuracy 0.1115\n",
      "Epoch 15 Batch 500 Loss 1.1082 Accuracy 0.1115\n",
      "Epoch 15 Batch 550 Loss 1.1099 Accuracy 0.1116\n",
      "Epoch 15 Batch 600 Loss 1.1119 Accuracy 0.1116\n",
      "Epoch 15 Batch 650 Loss 1.1109 Accuracy 0.1115\n",
      "Epoch 15 Batch 700 Loss 1.1119 Accuracy 0.1115\n",
      "Epoch 15 Batch 750 Loss 1.1122 Accuracy 0.1114\n",
      "Epoch 15 Batch 800 Loss 1.1146 Accuracy 0.1116\n",
      "Epoch 15 Batch 850 Loss 1.1147 Accuracy 0.1115\n",
      "Epoch 15 Batch 900 Loss 1.1177 Accuracy 0.1117\n",
      "Epoch 15 Batch 950 Loss 1.1178 Accuracy 0.1118\n",
      "Epoch 15 Batch 1000 Loss 1.1177 Accuracy 0.1118\n",
      "Epoch 15 Batch 1050 Loss 1.1180 Accuracy 0.1118\n",
      "Epoch 15 Batch 1100 Loss 1.1185 Accuracy 0.1117\n",
      "Epoch 15 Batch 1150 Loss 1.1175 Accuracy 0.1116\n",
      "Epoch 15 Batch 1200 Loss 1.1179 Accuracy 0.1117\n",
      "Epoch 15 Batch 1250 Loss 1.1162 Accuracy 0.1117\n",
      "Epoch 15 Batch 1300 Loss 1.1159 Accuracy 0.1117\n",
      "Epoch 15 Batch 1350 Loss 1.1151 Accuracy 0.1116\n",
      "Epoch 15 Batch 1400 Loss 1.1152 Accuracy 0.1117\n",
      "Epoch 15 Batch 1450 Loss 1.1153 Accuracy 0.1117\n",
      "Epoch 15 Batch 1500 Loss 1.1148 Accuracy 0.1116\n",
      "Epoch 15 Batch 1550 Loss 1.1152 Accuracy 0.1117\n",
      "Epoch 15 Batch 1600 Loss 1.1150 Accuracy 0.1116\n",
      "Epoch 15 Batch 1650 Loss 1.1137 Accuracy 0.1115\n",
      "Epoch 15 Batch 1700 Loss 1.1135 Accuracy 0.1115\n",
      "Epoch 15 Batch 1750 Loss 1.1137 Accuracy 0.1115\n",
      "Epoch 15 Batch 1800 Loss 1.1135 Accuracy 0.1114\n",
      "Epoch 15 Batch 1850 Loss 1.1132 Accuracy 0.1113\n",
      "Epoch 15 Batch 1900 Loss 1.1130 Accuracy 0.1112\n",
      "Epoch 15 Batch 1950 Loss 1.1126 Accuracy 0.1112\n",
      "Epoch 15 Batch 2000 Loss 1.1128 Accuracy 0.1111\n",
      "Epoch 15 Batch 2050 Loss 1.1118 Accuracy 0.1110\n",
      "Epoch 15 Batch 2100 Loss 1.1115 Accuracy 0.1110\n",
      "Epoch 15 Batch 2150 Loss 1.1125 Accuracy 0.1110\n",
      "Epoch 15 Batch 2200 Loss 1.1134 Accuracy 0.1110\n",
      "Epoch 15 Batch 2250 Loss 1.1136 Accuracy 0.1111\n",
      "Epoch 15 Batch 2300 Loss 1.1136 Accuracy 0.1111\n",
      "Epoch 15 Batch 2350 Loss 1.1139 Accuracy 0.1111\n",
      "Epoch 15 Batch 2400 Loss 1.1144 Accuracy 0.1111\n",
      "Epoch 15 Batch 2450 Loss 1.1145 Accuracy 0.1111\n",
      "Epoch 15 Batch 2500 Loss 1.1146 Accuracy 0.1111\n",
      "Epoch 15 Batch 2550 Loss 1.1151 Accuracy 0.1111\n",
      "Epoch 15 Batch 2600 Loss 1.1159 Accuracy 0.1111\n",
      "Epoch 15 Batch 2650 Loss 1.1162 Accuracy 0.1112\n",
      "Epoch 15 Batch 2700 Loss 1.1162 Accuracy 0.1112\n",
      "Epoch 15 Batch 2750 Loss 1.1164 Accuracy 0.1112\n",
      "Epoch 15 Batch 2800 Loss 1.1167 Accuracy 0.1112\n",
      "Epoch 15 Batch 2850 Loss 1.1170 Accuracy 0.1112\n",
      "Epoch 15 Batch 2900 Loss 1.1172 Accuracy 0.1112\n",
      "Epoch 15 Batch 2950 Loss 1.1175 Accuracy 0.1112\n",
      "Epoch 15 Batch 3000 Loss 1.1180 Accuracy 0.1112\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Epoch 15 Loss 1.1179 Accuracy 0.1112\n",
      "Time taken for 1 epoch: 180.59 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.0972 Accuracy 0.1034\n",
      "Epoch 16 Batch 50 Loss 1.1255 Accuracy 0.1117\n",
      "Epoch 16 Batch 100 Loss 1.1297 Accuracy 0.1126\n",
      "Epoch 16 Batch 150 Loss 1.1119 Accuracy 0.1126\n",
      "Epoch 16 Batch 200 Loss 1.1076 Accuracy 0.1123\n",
      "Epoch 16 Batch 250 Loss 1.1048 Accuracy 0.1123\n",
      "Epoch 16 Batch 300 Loss 1.1055 Accuracy 0.1122\n",
      "Epoch 16 Batch 350 Loss 1.1053 Accuracy 0.1120\n",
      "Epoch 16 Batch 400 Loss 1.1056 Accuracy 0.1122\n",
      "Epoch 16 Batch 450 Loss 1.1060 Accuracy 0.1123\n",
      "Epoch 16 Batch 500 Loss 1.1073 Accuracy 0.1123\n",
      "Epoch 16 Batch 550 Loss 1.1064 Accuracy 0.1120\n",
      "Epoch 16 Batch 600 Loss 1.1069 Accuracy 0.1121\n",
      "Epoch 16 Batch 650 Loss 1.1080 Accuracy 0.1122\n",
      "Epoch 16 Batch 700 Loss 1.1087 Accuracy 0.1121\n",
      "Epoch 16 Batch 750 Loss 1.1093 Accuracy 0.1121\n",
      "Epoch 16 Batch 800 Loss 1.1103 Accuracy 0.1122\n",
      "Epoch 16 Batch 850 Loss 1.1122 Accuracy 0.1123\n",
      "Epoch 16 Batch 900 Loss 1.1124 Accuracy 0.1124\n",
      "Epoch 16 Batch 950 Loss 1.1137 Accuracy 0.1125\n",
      "Epoch 16 Batch 1000 Loss 1.1136 Accuracy 0.1124\n",
      "Epoch 16 Batch 1050 Loss 1.1142 Accuracy 0.1125\n",
      "Epoch 16 Batch 1100 Loss 1.1159 Accuracy 0.1125\n",
      "Epoch 16 Batch 1150 Loss 1.1164 Accuracy 0.1125\n",
      "Epoch 16 Batch 1200 Loss 1.1160 Accuracy 0.1125\n",
      "Epoch 16 Batch 1250 Loss 1.1136 Accuracy 0.1124\n",
      "Epoch 16 Batch 1300 Loss 1.1120 Accuracy 0.1123\n",
      "Epoch 16 Batch 1350 Loss 1.1116 Accuracy 0.1123\n",
      "Epoch 16 Batch 1400 Loss 1.1119 Accuracy 0.1123\n",
      "Epoch 16 Batch 1450 Loss 1.1113 Accuracy 0.1123\n",
      "Epoch 16 Batch 1500 Loss 1.1108 Accuracy 0.1122\n",
      "Epoch 16 Batch 1550 Loss 1.1109 Accuracy 0.1121\n",
      "Epoch 16 Batch 1600 Loss 1.1109 Accuracy 0.1121\n",
      "Epoch 16 Batch 1650 Loss 1.1097 Accuracy 0.1120\n",
      "Epoch 16 Batch 1700 Loss 1.1092 Accuracy 0.1120\n",
      "Epoch 16 Batch 1750 Loss 1.1092 Accuracy 0.1119\n",
      "Epoch 16 Batch 1800 Loss 1.1094 Accuracy 0.1119\n",
      "Epoch 16 Batch 1850 Loss 1.1096 Accuracy 0.1118\n",
      "Epoch 16 Batch 1900 Loss 1.1092 Accuracy 0.1117\n",
      "Epoch 16 Batch 1950 Loss 1.1088 Accuracy 0.1117\n",
      "Epoch 16 Batch 2000 Loss 1.1078 Accuracy 0.1116\n",
      "Epoch 16 Batch 2050 Loss 1.1073 Accuracy 0.1115\n",
      "Epoch 16 Batch 2100 Loss 1.1071 Accuracy 0.1114\n",
      "Epoch 16 Batch 2150 Loss 1.1076 Accuracy 0.1114\n",
      "Epoch 16 Batch 2200 Loss 1.1083 Accuracy 0.1115\n",
      "Epoch 16 Batch 2250 Loss 1.1096 Accuracy 0.1115\n",
      "Epoch 16 Batch 2300 Loss 1.1095 Accuracy 0.1115\n",
      "Epoch 16 Batch 2350 Loss 1.1099 Accuracy 0.1115\n",
      "Epoch 16 Batch 2400 Loss 1.1101 Accuracy 0.1115\n",
      "Epoch 16 Batch 2450 Loss 1.1109 Accuracy 0.1115\n",
      "Epoch 16 Batch 2500 Loss 1.1113 Accuracy 0.1116\n",
      "Epoch 16 Batch 2550 Loss 1.1115 Accuracy 0.1116\n",
      "Epoch 16 Batch 2600 Loss 1.1114 Accuracy 0.1115\n",
      "Epoch 16 Batch 2650 Loss 1.1117 Accuracy 0.1115\n",
      "Epoch 16 Batch 2700 Loss 1.1117 Accuracy 0.1116\n",
      "Epoch 16 Batch 2750 Loss 1.1122 Accuracy 0.1116\n",
      "Epoch 16 Batch 2800 Loss 1.1127 Accuracy 0.1116\n",
      "Epoch 16 Batch 2850 Loss 1.1132 Accuracy 0.1116\n",
      "Epoch 16 Batch 2900 Loss 1.1127 Accuracy 0.1116\n",
      "Epoch 16 Batch 2950 Loss 1.1132 Accuracy 0.1116\n",
      "Epoch 16 Batch 3000 Loss 1.1136 Accuracy 0.1117\n",
      "Epoch 16 Loss 1.1140 Accuracy 0.1117\n",
      "Time taken for 1 epoch: 179.24 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.0801 Accuracy 0.0982\n",
      "Epoch 17 Batch 50 Loss 1.1270 Accuracy 0.1118\n",
      "Epoch 17 Batch 100 Loss 1.1182 Accuracy 0.1120\n",
      "Epoch 17 Batch 150 Loss 1.1142 Accuracy 0.1124\n",
      "Epoch 17 Batch 200 Loss 1.1089 Accuracy 0.1123\n",
      "Epoch 17 Batch 250 Loss 1.1034 Accuracy 0.1122\n",
      "Epoch 17 Batch 300 Loss 1.1042 Accuracy 0.1125\n",
      "Epoch 17 Batch 350 Loss 1.1012 Accuracy 0.1124\n",
      "Epoch 17 Batch 400 Loss 1.1014 Accuracy 0.1126\n",
      "Epoch 17 Batch 450 Loss 1.1024 Accuracy 0.1127\n",
      "Epoch 17 Batch 500 Loss 1.1026 Accuracy 0.1126\n",
      "Epoch 17 Batch 550 Loss 1.1040 Accuracy 0.1126\n",
      "Epoch 17 Batch 600 Loss 1.1039 Accuracy 0.1125\n",
      "Epoch 17 Batch 650 Loss 1.1028 Accuracy 0.1125\n",
      "Epoch 17 Batch 700 Loss 1.1038 Accuracy 0.1125\n",
      "Epoch 17 Batch 750 Loss 1.1058 Accuracy 0.1124\n",
      "Epoch 17 Batch 800 Loss 1.1066 Accuracy 0.1125\n",
      "Epoch 17 Batch 850 Loss 1.1094 Accuracy 0.1127\n",
      "Epoch 17 Batch 900 Loss 1.1085 Accuracy 0.1127\n",
      "Epoch 17 Batch 950 Loss 1.1077 Accuracy 0.1127\n",
      "Epoch 17 Batch 1000 Loss 1.1082 Accuracy 0.1127\n",
      "Epoch 17 Batch 1050 Loss 1.1084 Accuracy 0.1127\n",
      "Epoch 17 Batch 1100 Loss 1.1093 Accuracy 0.1127\n",
      "Epoch 17 Batch 1150 Loss 1.1105 Accuracy 0.1127\n",
      "Epoch 17 Batch 1200 Loss 1.1107 Accuracy 0.1128\n",
      "Epoch 17 Batch 1250 Loss 1.1094 Accuracy 0.1127\n",
      "Epoch 17 Batch 1300 Loss 1.1084 Accuracy 0.1126\n",
      "Epoch 17 Batch 1350 Loss 1.1073 Accuracy 0.1126\n",
      "Epoch 17 Batch 1400 Loss 1.1070 Accuracy 0.1126\n",
      "Epoch 17 Batch 1450 Loss 1.1067 Accuracy 0.1126\n",
      "Epoch 17 Batch 1500 Loss 1.1062 Accuracy 0.1125\n",
      "Epoch 17 Batch 1550 Loss 1.1055 Accuracy 0.1125\n",
      "Epoch 17 Batch 1600 Loss 1.1056 Accuracy 0.1125\n",
      "Epoch 17 Batch 1650 Loss 1.1050 Accuracy 0.1125\n",
      "Epoch 17 Batch 1700 Loss 1.1047 Accuracy 0.1124\n",
      "Epoch 17 Batch 1750 Loss 1.1042 Accuracy 0.1123\n",
      "Epoch 17 Batch 1800 Loss 1.1048 Accuracy 0.1123\n",
      "Epoch 17 Batch 1850 Loss 1.1052 Accuracy 0.1123\n",
      "Epoch 17 Batch 1900 Loss 1.1056 Accuracy 0.1123\n",
      "Epoch 17 Batch 1950 Loss 1.1052 Accuracy 0.1122\n",
      "Epoch 17 Batch 2000 Loss 1.1041 Accuracy 0.1121\n",
      "Epoch 17 Batch 2050 Loss 1.1043 Accuracy 0.1120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 2100 Loss 1.1046 Accuracy 0.1120\n",
      "Epoch 17 Batch 2150 Loss 1.1049 Accuracy 0.1120\n",
      "Epoch 17 Batch 2200 Loss 1.1055 Accuracy 0.1120\n",
      "Epoch 17 Batch 2250 Loss 1.1061 Accuracy 0.1120\n",
      "Epoch 17 Batch 2300 Loss 1.1058 Accuracy 0.1120\n",
      "Epoch 17 Batch 2350 Loss 1.1066 Accuracy 0.1120\n",
      "Epoch 17 Batch 2400 Loss 1.1071 Accuracy 0.1120\n",
      "Epoch 17 Batch 2450 Loss 1.1077 Accuracy 0.1121\n",
      "Epoch 17 Batch 2500 Loss 1.1078 Accuracy 0.1121\n",
      "Epoch 17 Batch 2550 Loss 1.1078 Accuracy 0.1121\n",
      "Epoch 17 Batch 2600 Loss 1.1079 Accuracy 0.1120\n",
      "Epoch 17 Batch 2650 Loss 1.1079 Accuracy 0.1120\n",
      "Epoch 17 Batch 2700 Loss 1.1081 Accuracy 0.1121\n",
      "Epoch 17 Batch 2750 Loss 1.1084 Accuracy 0.1121\n",
      "Epoch 17 Batch 2800 Loss 1.1088 Accuracy 0.1121\n",
      "Epoch 17 Batch 2850 Loss 1.1088 Accuracy 0.1121\n",
      "Epoch 17 Batch 2900 Loss 1.1090 Accuracy 0.1121\n",
      "Epoch 17 Batch 2950 Loss 1.1093 Accuracy 0.1121\n",
      "Epoch 17 Batch 3000 Loss 1.1097 Accuracy 0.1122\n",
      "Epoch 17 Loss 1.1098 Accuracy 0.1122\n",
      "Time taken for 1 epoch: 179.52 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.0942 Accuracy 0.1078\n",
      "Epoch 18 Batch 50 Loss 1.1353 Accuracy 0.1126\n",
      "Epoch 18 Batch 100 Loss 1.1058 Accuracy 0.1116\n",
      "Epoch 18 Batch 150 Loss 1.1011 Accuracy 0.1122\n",
      "Epoch 18 Batch 200 Loss 1.0982 Accuracy 0.1125\n",
      "Epoch 18 Batch 250 Loss 1.1009 Accuracy 0.1129\n",
      "Epoch 18 Batch 300 Loss 1.0993 Accuracy 0.1130\n",
      "Epoch 18 Batch 350 Loss 1.0957 Accuracy 0.1129\n",
      "Epoch 18 Batch 400 Loss 1.0976 Accuracy 0.1133\n",
      "Epoch 18 Batch 450 Loss 1.0979 Accuracy 0.1132\n",
      "Epoch 18 Batch 500 Loss 1.0986 Accuracy 0.1132\n",
      "Epoch 18 Batch 550 Loss 1.1012 Accuracy 0.1133\n",
      "Epoch 18 Batch 600 Loss 1.1019 Accuracy 0.1132\n",
      "Epoch 18 Batch 650 Loss 1.0999 Accuracy 0.1130\n",
      "Epoch 18 Batch 700 Loss 1.0999 Accuracy 0.1129\n",
      "Epoch 18 Batch 750 Loss 1.1005 Accuracy 0.1129\n",
      "Epoch 18 Batch 800 Loss 1.1025 Accuracy 0.1129\n",
      "Epoch 18 Batch 850 Loss 1.1033 Accuracy 0.1129\n",
      "Epoch 18 Batch 900 Loss 1.1038 Accuracy 0.1130\n",
      "Epoch 18 Batch 950 Loss 1.1034 Accuracy 0.1130\n",
      "Epoch 18 Batch 1000 Loss 1.1036 Accuracy 0.1129\n",
      "Epoch 18 Batch 1050 Loss 1.1042 Accuracy 0.1130\n",
      "Epoch 18 Batch 1100 Loss 1.1039 Accuracy 0.1129\n",
      "Epoch 18 Batch 1150 Loss 1.1046 Accuracy 0.1129\n",
      "Epoch 18 Batch 1200 Loss 1.1046 Accuracy 0.1130\n",
      "Epoch 18 Batch 1250 Loss 1.1032 Accuracy 0.1129\n",
      "Epoch 18 Batch 1300 Loss 1.1022 Accuracy 0.1129\n",
      "Epoch 18 Batch 1350 Loss 1.1018 Accuracy 0.1129\n",
      "Epoch 18 Batch 1400 Loss 1.1017 Accuracy 0.1129\n",
      "Epoch 18 Batch 1450 Loss 1.1018 Accuracy 0.1130\n",
      "Epoch 18 Batch 1500 Loss 1.1013 Accuracy 0.1129\n",
      "Epoch 18 Batch 1550 Loss 1.1011 Accuracy 0.1129\n",
      "Epoch 18 Batch 1600 Loss 1.1011 Accuracy 0.1129\n",
      "Epoch 18 Batch 1650 Loss 1.1003 Accuracy 0.1128\n",
      "Epoch 18 Batch 1700 Loss 1.1000 Accuracy 0.1127\n",
      "Epoch 18 Batch 1750 Loss 1.1000 Accuracy 0.1127\n",
      "Epoch 18 Batch 1800 Loss 1.0998 Accuracy 0.1126\n",
      "Epoch 18 Batch 1850 Loss 1.1005 Accuracy 0.1126\n",
      "Epoch 18 Batch 1900 Loss 1.1010 Accuracy 0.1125\n",
      "Epoch 18 Batch 1950 Loss 1.1014 Accuracy 0.1125\n",
      "Epoch 18 Batch 2000 Loss 1.1007 Accuracy 0.1124\n",
      "Epoch 18 Batch 2050 Loss 1.0998 Accuracy 0.1123\n",
      "Epoch 18 Batch 2100 Loss 1.0991 Accuracy 0.1122\n",
      "Epoch 18 Batch 2150 Loss 1.0997 Accuracy 0.1122\n",
      "Epoch 18 Batch 2200 Loss 1.0999 Accuracy 0.1122\n",
      "Epoch 18 Batch 2250 Loss 1.0999 Accuracy 0.1122\n",
      "Epoch 18 Batch 2300 Loss 1.1004 Accuracy 0.1122\n",
      "Epoch 18 Batch 2350 Loss 1.1011 Accuracy 0.1123\n",
      "Epoch 18 Batch 2400 Loss 1.1018 Accuracy 0.1123\n",
      "Epoch 18 Batch 2450 Loss 1.1024 Accuracy 0.1123\n",
      "Epoch 18 Batch 2500 Loss 1.1030 Accuracy 0.1124\n",
      "Epoch 18 Batch 2550 Loss 1.1036 Accuracy 0.1124\n",
      "Epoch 18 Batch 2600 Loss 1.1038 Accuracy 0.1124\n",
      "Epoch 18 Batch 2650 Loss 1.1040 Accuracy 0.1124\n",
      "Epoch 18 Batch 2700 Loss 1.1043 Accuracy 0.1125\n",
      "Epoch 18 Batch 2750 Loss 1.1046 Accuracy 0.1125\n",
      "Epoch 18 Batch 2800 Loss 1.1049 Accuracy 0.1125\n",
      "Epoch 18 Batch 2850 Loss 1.1058 Accuracy 0.1126\n",
      "Epoch 18 Batch 2900 Loss 1.1061 Accuracy 0.1126\n",
      "Epoch 18 Batch 2950 Loss 1.1059 Accuracy 0.1126\n",
      "Epoch 18 Batch 3000 Loss 1.1061 Accuracy 0.1126\n",
      "Epoch 18 Loss 1.1061 Accuracy 0.1126\n",
      "Time taken for 1 epoch: 181.19 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.9616 Accuracy 0.1058\n",
      "Epoch 19 Batch 50 Loss 1.1080 Accuracy 0.1130\n",
      "Epoch 19 Batch 100 Loss 1.1013 Accuracy 0.1137\n",
      "Epoch 19 Batch 150 Loss 1.0972 Accuracy 0.1139\n",
      "Epoch 19 Batch 200 Loss 1.0981 Accuracy 0.1139\n",
      "Epoch 19 Batch 250 Loss 1.0982 Accuracy 0.1140\n",
      "Epoch 19 Batch 300 Loss 1.0955 Accuracy 0.1138\n",
      "Epoch 19 Batch 350 Loss 1.0980 Accuracy 0.1138\n",
      "Epoch 19 Batch 400 Loss 1.0941 Accuracy 0.1137\n",
      "Epoch 19 Batch 450 Loss 1.0938 Accuracy 0.1136\n",
      "Epoch 19 Batch 500 Loss 1.0932 Accuracy 0.1135\n",
      "Epoch 19 Batch 550 Loss 1.0953 Accuracy 0.1136\n",
      "Epoch 19 Batch 600 Loss 1.0947 Accuracy 0.1136\n",
      "Epoch 19 Batch 650 Loss 1.0948 Accuracy 0.1136\n",
      "Epoch 19 Batch 700 Loss 1.0961 Accuracy 0.1136\n",
      "Epoch 19 Batch 750 Loss 1.0966 Accuracy 0.1135\n",
      "Epoch 19 Batch 800 Loss 1.0961 Accuracy 0.1134\n",
      "Epoch 19 Batch 850 Loss 1.0975 Accuracy 0.1134\n",
      "Epoch 19 Batch 900 Loss 1.0993 Accuracy 0.1136\n",
      "Epoch 19 Batch 950 Loss 1.1001 Accuracy 0.1136\n",
      "Epoch 19 Batch 1000 Loss 1.1010 Accuracy 0.1136\n",
      "Epoch 19 Batch 1050 Loss 1.1001 Accuracy 0.1136\n",
      "Epoch 19 Batch 1100 Loss 1.1010 Accuracy 0.1136\n",
      "Epoch 19 Batch 1150 Loss 1.1006 Accuracy 0.1135\n",
      "Epoch 19 Batch 1200 Loss 1.1013 Accuracy 0.1136\n",
      "Epoch 19 Batch 1250 Loss 1.1001 Accuracy 0.1136\n",
      "Epoch 19 Batch 1300 Loss 1.0993 Accuracy 0.1135\n",
      "Epoch 19 Batch 1350 Loss 1.0993 Accuracy 0.1135\n",
      "Epoch 19 Batch 1400 Loss 1.0989 Accuracy 0.1135\n",
      "Epoch 19 Batch 1450 Loss 1.0988 Accuracy 0.1135\n",
      "Epoch 19 Batch 1500 Loss 1.0977 Accuracy 0.1134\n",
      "Epoch 19 Batch 1550 Loss 1.0977 Accuracy 0.1133\n",
      "Epoch 19 Batch 1600 Loss 1.0969 Accuracy 0.1133\n",
      "Epoch 19 Batch 1650 Loss 1.0962 Accuracy 0.1132\n",
      "Epoch 19 Batch 1700 Loss 1.0959 Accuracy 0.1132\n",
      "Epoch 19 Batch 1750 Loss 1.0963 Accuracy 0.1132\n",
      "Epoch 19 Batch 1800 Loss 1.0965 Accuracy 0.1132\n",
      "Epoch 19 Batch 1850 Loss 1.0967 Accuracy 0.1131\n",
      "Epoch 19 Batch 1900 Loss 1.0965 Accuracy 0.1130\n",
      "Epoch 19 Batch 1950 Loss 1.0967 Accuracy 0.1129\n",
      "Epoch 19 Batch 2000 Loss 1.0966 Accuracy 0.1129\n",
      "Epoch 19 Batch 2050 Loss 1.0964 Accuracy 0.1128\n",
      "Epoch 19 Batch 2100 Loss 1.0961 Accuracy 0.1128\n",
      "Epoch 19 Batch 2150 Loss 1.0969 Accuracy 0.1128\n",
      "Epoch 19 Batch 2200 Loss 1.0972 Accuracy 0.1128\n",
      "Epoch 19 Batch 2250 Loss 1.0972 Accuracy 0.1127\n",
      "Epoch 19 Batch 2300 Loss 1.0974 Accuracy 0.1127\n",
      "Epoch 19 Batch 2350 Loss 1.0982 Accuracy 0.1128\n",
      "Epoch 19 Batch 2400 Loss 1.0987 Accuracy 0.1127\n",
      "Epoch 19 Batch 2450 Loss 1.0989 Accuracy 0.1127\n",
      "Epoch 19 Batch 2500 Loss 1.0990 Accuracy 0.1127\n",
      "Epoch 19 Batch 2550 Loss 1.0997 Accuracy 0.1128\n",
      "Epoch 19 Batch 2600 Loss 1.1002 Accuracy 0.1128\n",
      "Epoch 19 Batch 2650 Loss 1.0998 Accuracy 0.1128\n",
      "Epoch 19 Batch 2700 Loss 1.1003 Accuracy 0.1128\n",
      "Epoch 19 Batch 2750 Loss 1.1012 Accuracy 0.1129\n",
      "Epoch 19 Batch 2800 Loss 1.1018 Accuracy 0.1129\n",
      "Epoch 19 Batch 2850 Loss 1.1019 Accuracy 0.1129\n",
      "Epoch 19 Batch 2900 Loss 1.1022 Accuracy 0.1129\n",
      "Epoch 19 Batch 2950 Loss 1.1026 Accuracy 0.1129\n",
      "Epoch 19 Batch 3000 Loss 1.1034 Accuracy 0.1130\n",
      "Epoch 19 Loss 1.1029 Accuracy 0.1130\n",
      "Time taken for 1 epoch: 181.08 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.1097 Accuracy 0.1042\n",
      "Epoch 20 Batch 50 Loss 1.1167 Accuracy 0.1128\n",
      "Epoch 20 Batch 100 Loss 1.1003 Accuracy 0.1136\n",
      "Epoch 20 Batch 150 Loss 1.1014 Accuracy 0.1138\n",
      "Epoch 20 Batch 200 Loss 1.0968 Accuracy 0.1137\n",
      "Epoch 20 Batch 250 Loss 1.0920 Accuracy 0.1135\n",
      "Epoch 20 Batch 300 Loss 1.0888 Accuracy 0.1135\n",
      "Epoch 20 Batch 350 Loss 1.0864 Accuracy 0.1134\n",
      "Epoch 20 Batch 400 Loss 1.0838 Accuracy 0.1131\n",
      "Epoch 20 Batch 450 Loss 1.0866 Accuracy 0.1133\n",
      "Epoch 20 Batch 500 Loss 1.0900 Accuracy 0.1136\n",
      "Epoch 20 Batch 550 Loss 1.0892 Accuracy 0.1136\n",
      "Epoch 20 Batch 600 Loss 1.0878 Accuracy 0.1136\n",
      "Epoch 20 Batch 650 Loss 1.0871 Accuracy 0.1135\n",
      "Epoch 20 Batch 700 Loss 1.0892 Accuracy 0.1136\n",
      "Epoch 20 Batch 750 Loss 1.0901 Accuracy 0.1135\n",
      "Epoch 20 Batch 800 Loss 1.0899 Accuracy 0.1135\n",
      "Epoch 20 Batch 850 Loss 1.0922 Accuracy 0.1136\n",
      "Epoch 20 Batch 900 Loss 1.0932 Accuracy 0.1137\n",
      "Epoch 20 Batch 950 Loss 1.0957 Accuracy 0.1138\n",
      "Epoch 20 Batch 1000 Loss 1.0965 Accuracy 0.1139\n",
      "Epoch 20 Batch 1050 Loss 1.0970 Accuracy 0.1139\n",
      "Epoch 20 Batch 1100 Loss 1.0980 Accuracy 0.1139\n",
      "Epoch 20 Batch 1150 Loss 1.0979 Accuracy 0.1139\n",
      "Epoch 20 Batch 1200 Loss 1.0981 Accuracy 0.1140\n",
      "Epoch 20 Batch 1250 Loss 1.0975 Accuracy 0.1139\n",
      "Epoch 20 Batch 1300 Loss 1.0959 Accuracy 0.1138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 1350 Loss 1.0952 Accuracy 0.1138\n",
      "Epoch 20 Batch 1400 Loss 1.0957 Accuracy 0.1138\n",
      "Epoch 20 Batch 1450 Loss 1.0955 Accuracy 0.1138\n",
      "Epoch 20 Batch 1500 Loss 1.0956 Accuracy 0.1138\n",
      "Epoch 20 Batch 1550 Loss 1.0959 Accuracy 0.1138\n",
      "Epoch 20 Batch 1600 Loss 1.0954 Accuracy 0.1137\n",
      "Epoch 20 Batch 1650 Loss 1.0949 Accuracy 0.1137\n",
      "Epoch 20 Batch 1700 Loss 1.0946 Accuracy 0.1136\n",
      "Epoch 20 Batch 1750 Loss 1.0947 Accuracy 0.1137\n",
      "Epoch 20 Batch 1800 Loss 1.0951 Accuracy 0.1136\n",
      "Epoch 20 Batch 1850 Loss 1.0961 Accuracy 0.1135\n",
      "Epoch 20 Batch 1900 Loss 1.0962 Accuracy 0.1135\n",
      "Epoch 20 Batch 1950 Loss 1.0959 Accuracy 0.1134\n",
      "Epoch 20 Batch 2000 Loss 1.0947 Accuracy 0.1132\n",
      "Epoch 20 Batch 2050 Loss 1.0938 Accuracy 0.1132\n",
      "Epoch 20 Batch 2100 Loss 1.0936 Accuracy 0.1131\n",
      "Epoch 20 Batch 2150 Loss 1.0938 Accuracy 0.1131\n",
      "Epoch 20 Batch 2200 Loss 1.0944 Accuracy 0.1131\n",
      "Epoch 20 Batch 2250 Loss 1.0949 Accuracy 0.1131\n",
      "Epoch 20 Batch 2300 Loss 1.0947 Accuracy 0.1132\n",
      "Epoch 20 Batch 2350 Loss 1.0952 Accuracy 0.1132\n",
      "Epoch 20 Batch 2400 Loss 1.0959 Accuracy 0.1132\n",
      "Epoch 20 Batch 2450 Loss 1.0963 Accuracy 0.1132\n",
      "Epoch 20 Batch 2500 Loss 1.0963 Accuracy 0.1132\n",
      "Epoch 20 Batch 2550 Loss 1.0967 Accuracy 0.1132\n",
      "Epoch 20 Batch 2600 Loss 1.0974 Accuracy 0.1132\n",
      "Epoch 20 Batch 2650 Loss 1.0979 Accuracy 0.1132\n",
      "Epoch 20 Batch 2700 Loss 1.0980 Accuracy 0.1132\n",
      "Epoch 20 Batch 2750 Loss 1.0981 Accuracy 0.1132\n",
      "Epoch 20 Batch 2800 Loss 1.0986 Accuracy 0.1132\n",
      "Epoch 20 Batch 2850 Loss 1.0985 Accuracy 0.1133\n",
      "Epoch 20 Batch 2900 Loss 1.0985 Accuracy 0.1133\n",
      "Epoch 20 Batch 2950 Loss 1.0986 Accuracy 0.1133\n",
      "Epoch 20 Batch 3000 Loss 1.0994 Accuracy 0.1133\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Epoch 20 Loss 1.0994 Accuracy 0.1133\n",
      "Time taken for 1 epoch: 178.89 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.1961 Accuracy 0.1054\n",
      "Epoch 21 Batch 50 Loss 1.0877 Accuracy 0.1120\n",
      "Epoch 21 Batch 100 Loss 1.0938 Accuracy 0.1139\n",
      "Epoch 21 Batch 150 Loss 1.1009 Accuracy 0.1144\n",
      "Epoch 21 Batch 200 Loss 1.0908 Accuracy 0.1139\n",
      "Epoch 21 Batch 250 Loss 1.0880 Accuracy 0.1141\n",
      "Epoch 21 Batch 300 Loss 1.0865 Accuracy 0.1139\n",
      "Epoch 21 Batch 350 Loss 1.0831 Accuracy 0.1138\n",
      "Epoch 21 Batch 400 Loss 1.0839 Accuracy 0.1139\n",
      "Epoch 21 Batch 450 Loss 1.0842 Accuracy 0.1139\n",
      "Epoch 21 Batch 500 Loss 1.0856 Accuracy 0.1138\n",
      "Epoch 21 Batch 550 Loss 1.0859 Accuracy 0.1139\n",
      "Epoch 21 Batch 600 Loss 1.0858 Accuracy 0.1139\n",
      "Epoch 21 Batch 650 Loss 1.0867 Accuracy 0.1139\n",
      "Epoch 21 Batch 700 Loss 1.0874 Accuracy 0.1138\n",
      "Epoch 21 Batch 750 Loss 1.0892 Accuracy 0.1139\n",
      "Epoch 21 Batch 800 Loss 1.0904 Accuracy 0.1140\n",
      "Epoch 21 Batch 850 Loss 1.0921 Accuracy 0.1140\n",
      "Epoch 21 Batch 900 Loss 1.0931 Accuracy 0.1142\n",
      "Epoch 21 Batch 950 Loss 1.0936 Accuracy 0.1142\n",
      "Epoch 21 Batch 1000 Loss 1.0930 Accuracy 0.1142\n",
      "Epoch 21 Batch 1050 Loss 1.0939 Accuracy 0.1142\n",
      "Epoch 21 Batch 1100 Loss 1.0963 Accuracy 0.1143\n",
      "Epoch 21 Batch 1150 Loss 1.0960 Accuracy 0.1143\n",
      "Epoch 21 Batch 1200 Loss 1.0959 Accuracy 0.1142\n",
      "Epoch 21 Batch 1250 Loss 1.0947 Accuracy 0.1142\n",
      "Epoch 21 Batch 1300 Loss 1.0936 Accuracy 0.1141\n",
      "Epoch 21 Batch 1350 Loss 1.0928 Accuracy 0.1141\n",
      "Epoch 21 Batch 1400 Loss 1.0923 Accuracy 0.1141\n",
      "Epoch 21 Batch 1450 Loss 1.0929 Accuracy 0.1141\n",
      "Epoch 21 Batch 1500 Loss 1.0923 Accuracy 0.1141\n",
      "Epoch 21 Batch 1550 Loss 1.0930 Accuracy 0.1141\n",
      "Epoch 21 Batch 1600 Loss 1.0929 Accuracy 0.1141\n",
      "Epoch 21 Batch 1650 Loss 1.0917 Accuracy 0.1140\n",
      "Epoch 21 Batch 1700 Loss 1.0912 Accuracy 0.1140\n",
      "Epoch 21 Batch 1750 Loss 1.0918 Accuracy 0.1140\n",
      "Epoch 21 Batch 1800 Loss 1.0913 Accuracy 0.1139\n",
      "Epoch 21 Batch 1850 Loss 1.0914 Accuracy 0.1139\n",
      "Epoch 21 Batch 1900 Loss 1.0917 Accuracy 0.1138\n",
      "Epoch 21 Batch 1950 Loss 1.0916 Accuracy 0.1137\n",
      "Epoch 21 Batch 2000 Loss 1.0906 Accuracy 0.1136\n",
      "Epoch 21 Batch 2050 Loss 1.0908 Accuracy 0.1136\n",
      "Epoch 21 Batch 2100 Loss 1.0909 Accuracy 0.1136\n",
      "Epoch 21 Batch 2150 Loss 1.0913 Accuracy 0.1136\n",
      "Epoch 21 Batch 2200 Loss 1.0918 Accuracy 0.1136\n",
      "Epoch 21 Batch 2250 Loss 1.0918 Accuracy 0.1136\n",
      "Epoch 21 Batch 2300 Loss 1.0920 Accuracy 0.1136\n",
      "Epoch 21 Batch 2350 Loss 1.0923 Accuracy 0.1135\n",
      "Epoch 21 Batch 2400 Loss 1.0927 Accuracy 0.1135\n",
      "Epoch 21 Batch 2450 Loss 1.0926 Accuracy 0.1135\n",
      "Epoch 21 Batch 2500 Loss 1.0931 Accuracy 0.1135\n",
      "Epoch 21 Batch 2550 Loss 1.0934 Accuracy 0.1135\n",
      "Epoch 21 Batch 2600 Loss 1.0936 Accuracy 0.1135\n",
      "Epoch 21 Batch 2650 Loss 1.0937 Accuracy 0.1135\n",
      "Epoch 21 Batch 2700 Loss 1.0940 Accuracy 0.1136\n",
      "Epoch 21 Batch 2750 Loss 1.0941 Accuracy 0.1136\n",
      "Epoch 21 Batch 2800 Loss 1.0949 Accuracy 0.1136\n",
      "Epoch 21 Batch 2850 Loss 1.0951 Accuracy 0.1136\n",
      "Epoch 21 Batch 2900 Loss 1.0957 Accuracy 0.1137\n",
      "Epoch 21 Batch 2950 Loss 1.0959 Accuracy 0.1137\n",
      "Epoch 21 Batch 3000 Loss 1.0963 Accuracy 0.1137\n",
      "Epoch 21 Loss 1.0965 Accuracy 0.1137\n",
      "Time taken for 1 epoch: 180.81 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.0047 Accuracy 0.0978\n",
      "Epoch 22 Batch 50 Loss 1.1060 Accuracy 0.1149\n",
      "Epoch 22 Batch 100 Loss 1.1005 Accuracy 0.1152\n",
      "Epoch 22 Batch 150 Loss 1.0875 Accuracy 0.1148\n",
      "Epoch 22 Batch 200 Loss 1.0855 Accuracy 0.1147\n",
      "Epoch 22 Batch 250 Loss 1.0835 Accuracy 0.1147\n",
      "Epoch 22 Batch 300 Loss 1.0821 Accuracy 0.1145\n",
      "Epoch 22 Batch 350 Loss 1.0823 Accuracy 0.1147\n",
      "Epoch 22 Batch 400 Loss 1.0818 Accuracy 0.1146\n",
      "Epoch 22 Batch 450 Loss 1.0820 Accuracy 0.1146\n",
      "Epoch 22 Batch 500 Loss 1.0832 Accuracy 0.1146\n",
      "Epoch 22 Batch 550 Loss 1.0862 Accuracy 0.1148\n",
      "Epoch 22 Batch 600 Loss 1.0870 Accuracy 0.1147\n",
      "Epoch 22 Batch 650 Loss 1.0885 Accuracy 0.1148\n",
      "Epoch 22 Batch 700 Loss 1.0884 Accuracy 0.1148\n",
      "Epoch 22 Batch 750 Loss 1.0897 Accuracy 0.1148\n",
      "Epoch 22 Batch 800 Loss 1.0906 Accuracy 0.1147\n",
      "Epoch 22 Batch 850 Loss 1.0926 Accuracy 0.1149\n",
      "Epoch 22 Batch 900 Loss 1.0930 Accuracy 0.1149\n",
      "Epoch 22 Batch 950 Loss 1.0942 Accuracy 0.1149\n",
      "Epoch 22 Batch 1000 Loss 1.0946 Accuracy 0.1148\n",
      "Epoch 22 Batch 1050 Loss 1.0947 Accuracy 0.1149\n",
      "Epoch 22 Batch 1100 Loss 1.0950 Accuracy 0.1149\n",
      "Epoch 22 Batch 1150 Loss 1.0947 Accuracy 0.1149\n",
      "Epoch 22 Batch 1200 Loss 1.0937 Accuracy 0.1148\n",
      "Epoch 22 Batch 1250 Loss 1.0919 Accuracy 0.1148\n",
      "Epoch 22 Batch 1300 Loss 1.0913 Accuracy 0.1147\n",
      "Epoch 22 Batch 1350 Loss 1.0907 Accuracy 0.1147\n",
      "Epoch 22 Batch 1400 Loss 1.0904 Accuracy 0.1147\n",
      "Epoch 22 Batch 1450 Loss 1.0897 Accuracy 0.1146\n",
      "Epoch 22 Batch 1500 Loss 1.0895 Accuracy 0.1146\n",
      "Epoch 22 Batch 1550 Loss 1.0896 Accuracy 0.1146\n",
      "Epoch 22 Batch 1600 Loss 1.0893 Accuracy 0.1145\n",
      "Epoch 22 Batch 1650 Loss 1.0883 Accuracy 0.1144\n",
      "Epoch 22 Batch 1700 Loss 1.0887 Accuracy 0.1144\n",
      "Epoch 22 Batch 1750 Loss 1.0889 Accuracy 0.1144\n",
      "Epoch 22 Batch 1800 Loss 1.0888 Accuracy 0.1143\n",
      "Epoch 22 Batch 1850 Loss 1.0890 Accuracy 0.1142\n",
      "Epoch 22 Batch 1900 Loss 1.0886 Accuracy 0.1142\n",
      "Epoch 22 Batch 1950 Loss 1.0880 Accuracy 0.1141\n",
      "Epoch 22 Batch 2000 Loss 1.0875 Accuracy 0.1140\n",
      "Epoch 22 Batch 2050 Loss 1.0877 Accuracy 0.1139\n",
      "Epoch 22 Batch 2100 Loss 1.0873 Accuracy 0.1139\n",
      "Epoch 22 Batch 2150 Loss 1.0881 Accuracy 0.1139\n",
      "Epoch 22 Batch 2200 Loss 1.0884 Accuracy 0.1139\n",
      "Epoch 22 Batch 2250 Loss 1.0888 Accuracy 0.1140\n",
      "Epoch 22 Batch 2300 Loss 1.0894 Accuracy 0.1140\n",
      "Epoch 22 Batch 2350 Loss 1.0897 Accuracy 0.1140\n",
      "Epoch 22 Batch 2400 Loss 1.0901 Accuracy 0.1140\n",
      "Epoch 22 Batch 2450 Loss 1.0898 Accuracy 0.1139\n",
      "Epoch 22 Batch 2500 Loss 1.0901 Accuracy 0.1140\n",
      "Epoch 22 Batch 2550 Loss 1.0905 Accuracy 0.1139\n",
      "Epoch 22 Batch 2600 Loss 1.0912 Accuracy 0.1139\n",
      "Epoch 22 Batch 2650 Loss 1.0910 Accuracy 0.1140\n",
      "Epoch 22 Batch 2700 Loss 1.0915 Accuracy 0.1140\n",
      "Epoch 22 Batch 2750 Loss 1.0921 Accuracy 0.1140\n",
      "Epoch 22 Batch 2800 Loss 1.0922 Accuracy 0.1140\n",
      "Epoch 22 Batch 2850 Loss 1.0921 Accuracy 0.1140\n",
      "Epoch 22 Batch 2900 Loss 1.0931 Accuracy 0.1141\n",
      "Epoch 22 Batch 2950 Loss 1.0934 Accuracy 0.1141\n",
      "Epoch 22 Batch 3000 Loss 1.0937 Accuracy 0.1141\n",
      "Epoch 22 Loss 1.0937 Accuracy 0.1141\n",
      "Time taken for 1 epoch: 180.56 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.2322 Accuracy 0.1314\n",
      "Epoch 23 Batch 50 Loss 1.0923 Accuracy 0.1146\n",
      "Epoch 23 Batch 100 Loss 1.0914 Accuracy 0.1152\n",
      "Epoch 23 Batch 150 Loss 1.0815 Accuracy 0.1150\n",
      "Epoch 23 Batch 200 Loss 1.0825 Accuracy 0.1151\n",
      "Epoch 23 Batch 250 Loss 1.0832 Accuracy 0.1152\n",
      "Epoch 23 Batch 300 Loss 1.0853 Accuracy 0.1152\n",
      "Epoch 23 Batch 350 Loss 1.0853 Accuracy 0.1150\n",
      "Epoch 23 Batch 400 Loss 1.0840 Accuracy 0.1150\n",
      "Epoch 23 Batch 450 Loss 1.0858 Accuracy 0.1150\n",
      "Epoch 23 Batch 500 Loss 1.0850 Accuracy 0.1148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 550 Loss 1.0859 Accuracy 0.1148\n",
      "Epoch 23 Batch 600 Loss 1.0867 Accuracy 0.1149\n",
      "Epoch 23 Batch 650 Loss 1.0863 Accuracy 0.1147\n",
      "Epoch 23 Batch 700 Loss 1.0854 Accuracy 0.1147\n",
      "Epoch 23 Batch 750 Loss 1.0860 Accuracy 0.1147\n",
      "Epoch 23 Batch 800 Loss 1.0878 Accuracy 0.1147\n",
      "Epoch 23 Batch 850 Loss 1.0878 Accuracy 0.1148\n",
      "Epoch 23 Batch 900 Loss 1.0884 Accuracy 0.1148\n",
      "Epoch 23 Batch 950 Loss 1.0883 Accuracy 0.1148\n",
      "Epoch 23 Batch 1000 Loss 1.0884 Accuracy 0.1148\n",
      "Epoch 23 Batch 1050 Loss 1.0889 Accuracy 0.1148\n",
      "Epoch 23 Batch 1100 Loss 1.0900 Accuracy 0.1148\n",
      "Epoch 23 Batch 1150 Loss 1.0905 Accuracy 0.1149\n",
      "Epoch 23 Batch 1200 Loss 1.0891 Accuracy 0.1148\n",
      "Epoch 23 Batch 1250 Loss 1.0874 Accuracy 0.1148\n",
      "Epoch 23 Batch 1300 Loss 1.0862 Accuracy 0.1147\n",
      "Epoch 23 Batch 1350 Loss 1.0850 Accuracy 0.1147\n",
      "Epoch 23 Batch 1400 Loss 1.0853 Accuracy 0.1147\n",
      "Epoch 23 Batch 1450 Loss 1.0855 Accuracy 0.1147\n",
      "Epoch 23 Batch 1500 Loss 1.0851 Accuracy 0.1146\n",
      "Epoch 23 Batch 1550 Loss 1.0848 Accuracy 0.1146\n",
      "Epoch 23 Batch 1600 Loss 1.0850 Accuracy 0.1146\n",
      "Epoch 23 Batch 1650 Loss 1.0840 Accuracy 0.1145\n",
      "Epoch 23 Batch 1700 Loss 1.0838 Accuracy 0.1145\n",
      "Epoch 23 Batch 1750 Loss 1.0841 Accuracy 0.1145\n",
      "Epoch 23 Batch 1800 Loss 1.0847 Accuracy 0.1145\n",
      "Epoch 23 Batch 1850 Loss 1.0852 Accuracy 0.1145\n",
      "Epoch 23 Batch 1900 Loss 1.0858 Accuracy 0.1145\n",
      "Epoch 23 Batch 1950 Loss 1.0854 Accuracy 0.1143\n",
      "Epoch 23 Batch 2000 Loss 1.0849 Accuracy 0.1143\n",
      "Epoch 23 Batch 2050 Loss 1.0850 Accuracy 0.1143\n",
      "Epoch 23 Batch 2100 Loss 1.0852 Accuracy 0.1142\n",
      "Epoch 23 Batch 2150 Loss 1.0859 Accuracy 0.1142\n",
      "Epoch 23 Batch 2200 Loss 1.0860 Accuracy 0.1142\n",
      "Epoch 23 Batch 2250 Loss 1.0865 Accuracy 0.1142\n",
      "Epoch 23 Batch 2300 Loss 1.0867 Accuracy 0.1142\n",
      "Epoch 23 Batch 2350 Loss 1.0871 Accuracy 0.1143\n",
      "Epoch 23 Batch 2400 Loss 1.0874 Accuracy 0.1142\n",
      "Epoch 23 Batch 2450 Loss 1.0877 Accuracy 0.1142\n",
      "Epoch 23 Batch 2500 Loss 1.0877 Accuracy 0.1142\n",
      "Epoch 23 Batch 2550 Loss 1.0880 Accuracy 0.1142\n",
      "Epoch 23 Batch 2600 Loss 1.0885 Accuracy 0.1142\n",
      "Epoch 23 Batch 2650 Loss 1.0888 Accuracy 0.1142\n",
      "Epoch 23 Batch 2700 Loss 1.0893 Accuracy 0.1142\n",
      "Epoch 23 Batch 2750 Loss 1.0894 Accuracy 0.1142\n",
      "Epoch 23 Batch 2800 Loss 1.0900 Accuracy 0.1143\n",
      "Epoch 23 Batch 2850 Loss 1.0903 Accuracy 0.1143\n",
      "Epoch 23 Batch 2900 Loss 1.0905 Accuracy 0.1143\n",
      "Epoch 23 Batch 2950 Loss 1.0905 Accuracy 0.1143\n",
      "Epoch 23 Batch 3000 Loss 1.0911 Accuracy 0.1143\n",
      "Epoch 23 Loss 1.0912 Accuracy 0.1144\n",
      "Time taken for 1 epoch: 178.58 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.0736 Accuracy 0.1014\n",
      "Epoch 24 Batch 50 Loss 1.0842 Accuracy 0.1129\n",
      "Epoch 24 Batch 100 Loss 1.0841 Accuracy 0.1147\n",
      "Epoch 24 Batch 150 Loss 1.0802 Accuracy 0.1146\n",
      "Epoch 24 Batch 200 Loss 1.0816 Accuracy 0.1148\n",
      "Epoch 24 Batch 250 Loss 1.0796 Accuracy 0.1148\n",
      "Epoch 24 Batch 300 Loss 1.0792 Accuracy 0.1148\n",
      "Epoch 24 Batch 350 Loss 1.0817 Accuracy 0.1150\n",
      "Epoch 24 Batch 400 Loss 1.0805 Accuracy 0.1150\n",
      "Epoch 24 Batch 450 Loss 1.0804 Accuracy 0.1150\n",
      "Epoch 24 Batch 500 Loss 1.0804 Accuracy 0.1148\n",
      "Epoch 24 Batch 550 Loss 1.0814 Accuracy 0.1149\n",
      "Epoch 24 Batch 600 Loss 1.0816 Accuracy 0.1149\n",
      "Epoch 24 Batch 650 Loss 1.0815 Accuracy 0.1150\n",
      "Epoch 24 Batch 700 Loss 1.0838 Accuracy 0.1151\n",
      "Epoch 24 Batch 750 Loss 1.0842 Accuracy 0.1150\n",
      "Epoch 24 Batch 800 Loss 1.0840 Accuracy 0.1150\n",
      "Epoch 24 Batch 850 Loss 1.0854 Accuracy 0.1151\n",
      "Epoch 24 Batch 900 Loss 1.0850 Accuracy 0.1151\n",
      "Epoch 24 Batch 950 Loss 1.0845 Accuracy 0.1151\n",
      "Epoch 24 Batch 1000 Loss 1.0849 Accuracy 0.1151\n",
      "Epoch 24 Batch 1050 Loss 1.0848 Accuracy 0.1151\n",
      "Epoch 24 Batch 1100 Loss 1.0853 Accuracy 0.1151\n",
      "Epoch 24 Batch 1150 Loss 1.0866 Accuracy 0.1153\n",
      "Epoch 24 Batch 1200 Loss 1.0866 Accuracy 0.1153\n",
      "Epoch 24 Batch 1250 Loss 1.0862 Accuracy 0.1153\n",
      "Epoch 24 Batch 1300 Loss 1.0849 Accuracy 0.1152\n",
      "Epoch 24 Batch 1350 Loss 1.0836 Accuracy 0.1152\n",
      "Epoch 24 Batch 1400 Loss 1.0832 Accuracy 0.1152\n",
      "Epoch 24 Batch 1450 Loss 1.0825 Accuracy 0.1151\n",
      "Epoch 24 Batch 1500 Loss 1.0828 Accuracy 0.1151\n",
      "Epoch 24 Batch 1550 Loss 1.0827 Accuracy 0.1151\n",
      "Epoch 24 Batch 1600 Loss 1.0828 Accuracy 0.1150\n",
      "Epoch 24 Batch 1650 Loss 1.0823 Accuracy 0.1150\n",
      "Epoch 24 Batch 1700 Loss 1.0821 Accuracy 0.1149\n",
      "Epoch 24 Batch 1750 Loss 1.0814 Accuracy 0.1148\n",
      "Epoch 24 Batch 1800 Loss 1.0814 Accuracy 0.1147\n",
      "Epoch 24 Batch 1850 Loss 1.0824 Accuracy 0.1147\n",
      "Epoch 24 Batch 1900 Loss 1.0826 Accuracy 0.1147\n",
      "Epoch 24 Batch 1950 Loss 1.0825 Accuracy 0.1146\n",
      "Epoch 24 Batch 2000 Loss 1.0820 Accuracy 0.1145\n",
      "Epoch 24 Batch 2050 Loss 1.0820 Accuracy 0.1144\n",
      "Epoch 24 Batch 2100 Loss 1.0819 Accuracy 0.1144\n",
      "Epoch 24 Batch 2150 Loss 1.0819 Accuracy 0.1144\n",
      "Epoch 24 Batch 2200 Loss 1.0820 Accuracy 0.1143\n",
      "Epoch 24 Batch 2250 Loss 1.0835 Accuracy 0.1144\n",
      "Epoch 24 Batch 2300 Loss 1.0839 Accuracy 0.1145\n",
      "Epoch 24 Batch 2350 Loss 1.0845 Accuracy 0.1144\n",
      "Epoch 24 Batch 2400 Loss 1.0849 Accuracy 0.1144\n",
      "Epoch 24 Batch 2450 Loss 1.0853 Accuracy 0.1145\n",
      "Epoch 24 Batch 2500 Loss 1.0857 Accuracy 0.1145\n",
      "Epoch 24 Batch 2550 Loss 1.0862 Accuracy 0.1145\n",
      "Epoch 24 Batch 2600 Loss 1.0866 Accuracy 0.1146\n",
      "Epoch 24 Batch 2650 Loss 1.0871 Accuracy 0.1146\n",
      "Epoch 24 Batch 2700 Loss 1.0874 Accuracy 0.1146\n",
      "Epoch 24 Batch 2750 Loss 1.0873 Accuracy 0.1146\n",
      "Epoch 24 Batch 2800 Loss 1.0877 Accuracy 0.1147\n",
      "Epoch 24 Batch 2850 Loss 1.0877 Accuracy 0.1146\n",
      "Epoch 24 Batch 2900 Loss 1.0876 Accuracy 0.1147\n",
      "Epoch 24 Batch 2950 Loss 1.0877 Accuracy 0.1147\n",
      "Epoch 24 Batch 3000 Loss 1.0876 Accuracy 0.1147\n",
      "Epoch 24 Loss 1.0883 Accuracy 0.1147\n",
      "Time taken for 1 epoch: 183.02 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.2120 Accuracy 0.1202\n",
      "Epoch 25 Batch 50 Loss 1.0858 Accuracy 0.1147\n",
      "Epoch 25 Batch 100 Loss 1.0819 Accuracy 0.1154\n",
      "Epoch 25 Batch 150 Loss 1.0781 Accuracy 0.1155\n",
      "Epoch 25 Batch 200 Loss 1.0768 Accuracy 0.1156\n",
      "Epoch 25 Batch 250 Loss 1.0739 Accuracy 0.1156\n",
      "Epoch 25 Batch 300 Loss 1.0733 Accuracy 0.1154\n",
      "Epoch 25 Batch 350 Loss 1.0725 Accuracy 0.1154\n",
      "Epoch 25 Batch 400 Loss 1.0741 Accuracy 0.1154\n",
      "Epoch 25 Batch 450 Loss 1.0764 Accuracy 0.1156\n",
      "Epoch 25 Batch 500 Loss 1.0749 Accuracy 0.1155\n",
      "Epoch 25 Batch 550 Loss 1.0764 Accuracy 0.1154\n",
      "Epoch 25 Batch 600 Loss 1.0777 Accuracy 0.1154\n",
      "Epoch 25 Batch 650 Loss 1.0802 Accuracy 0.1155\n",
      "Epoch 25 Batch 700 Loss 1.0803 Accuracy 0.1155\n",
      "Epoch 25 Batch 750 Loss 1.0818 Accuracy 0.1156\n",
      "Epoch 25 Batch 800 Loss 1.0834 Accuracy 0.1157\n",
      "Epoch 25 Batch 850 Loss 1.0850 Accuracy 0.1157\n",
      "Epoch 25 Batch 900 Loss 1.0841 Accuracy 0.1157\n",
      "Epoch 25 Batch 950 Loss 1.0840 Accuracy 0.1158\n",
      "Epoch 25 Batch 1000 Loss 1.0838 Accuracy 0.1158\n",
      "Epoch 25 Batch 1050 Loss 1.0844 Accuracy 0.1158\n",
      "Epoch 25 Batch 1100 Loss 1.0845 Accuracy 0.1157\n",
      "Epoch 25 Batch 1150 Loss 1.0852 Accuracy 0.1158\n",
      "Epoch 25 Batch 1200 Loss 1.0832 Accuracy 0.1156\n",
      "Epoch 25 Batch 1250 Loss 1.0811 Accuracy 0.1155\n",
      "Epoch 25 Batch 1300 Loss 1.0802 Accuracy 0.1154\n",
      "Epoch 25 Batch 1350 Loss 1.0799 Accuracy 0.1154\n",
      "Epoch 25 Batch 1400 Loss 1.0798 Accuracy 0.1154\n",
      "Epoch 25 Batch 1450 Loss 1.0792 Accuracy 0.1154\n",
      "Epoch 25 Batch 1500 Loss 1.0794 Accuracy 0.1154\n",
      "Epoch 25 Batch 1550 Loss 1.0793 Accuracy 0.1153\n",
      "Epoch 25 Batch 1600 Loss 1.0799 Accuracy 0.1154\n",
      "Epoch 25 Batch 1650 Loss 1.0800 Accuracy 0.1153\n",
      "Epoch 25 Batch 1700 Loss 1.0790 Accuracy 0.1152\n",
      "Epoch 25 Batch 1750 Loss 1.0793 Accuracy 0.1152\n",
      "Epoch 25 Batch 1800 Loss 1.0796 Accuracy 0.1152\n",
      "Epoch 25 Batch 1850 Loss 1.0798 Accuracy 0.1151\n",
      "Epoch 25 Batch 1900 Loss 1.0799 Accuracy 0.1151\n",
      "Epoch 25 Batch 1950 Loss 1.0800 Accuracy 0.1151\n",
      "Epoch 25 Batch 2000 Loss 1.0801 Accuracy 0.1150\n",
      "Epoch 25 Batch 2050 Loss 1.0798 Accuracy 0.1149\n",
      "Epoch 25 Batch 2100 Loss 1.0797 Accuracy 0.1148\n",
      "Epoch 25 Batch 2150 Loss 1.0810 Accuracy 0.1149\n",
      "Epoch 25 Batch 2200 Loss 1.0807 Accuracy 0.1148\n",
      "Epoch 25 Batch 2250 Loss 1.0817 Accuracy 0.1149\n",
      "Epoch 25 Batch 2300 Loss 1.0818 Accuracy 0.1148\n",
      "Epoch 25 Batch 2350 Loss 1.0825 Accuracy 0.1149\n",
      "Epoch 25 Batch 2400 Loss 1.0828 Accuracy 0.1149\n",
      "Epoch 25 Batch 2450 Loss 1.0836 Accuracy 0.1149\n",
      "Epoch 25 Batch 2500 Loss 1.0838 Accuracy 0.1149\n",
      "Epoch 25 Batch 2550 Loss 1.0836 Accuracy 0.1150\n",
      "Epoch 25 Batch 2600 Loss 1.0843 Accuracy 0.1150\n",
      "Epoch 25 Batch 2650 Loss 1.0845 Accuracy 0.1150\n",
      "Epoch 25 Batch 2700 Loss 1.0846 Accuracy 0.1150\n",
      "Epoch 25 Batch 2750 Loss 1.0849 Accuracy 0.1150\n",
      "Epoch 25 Batch 2800 Loss 1.0853 Accuracy 0.1150\n",
      "Epoch 25 Batch 2850 Loss 1.0854 Accuracy 0.1150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 2900 Loss 1.0858 Accuracy 0.1150\n",
      "Epoch 25 Batch 2950 Loss 1.0856 Accuracy 0.1150\n",
      "Epoch 25 Batch 3000 Loss 1.0858 Accuracy 0.1151\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
      "Epoch 25 Loss 1.0860 Accuracy 0.1151\n",
      "Time taken for 1 epoch: 183.69 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.9993 Accuracy 0.1134\n",
      "Epoch 26 Batch 50 Loss 1.1066 Accuracy 0.1167\n",
      "Epoch 26 Batch 100 Loss 1.1028 Accuracy 0.1165\n",
      "Epoch 26 Batch 150 Loss 1.0955 Accuracy 0.1156\n",
      "Epoch 26 Batch 200 Loss 1.0930 Accuracy 0.1158\n",
      "Epoch 26 Batch 250 Loss 1.0867 Accuracy 0.1160\n",
      "Epoch 26 Batch 300 Loss 1.0779 Accuracy 0.1157\n",
      "Epoch 26 Batch 350 Loss 1.0780 Accuracy 0.1156\n",
      "Epoch 26 Batch 400 Loss 1.0702 Accuracy 0.1152\n",
      "Epoch 26 Batch 450 Loss 1.0730 Accuracy 0.1154\n",
      "Epoch 26 Batch 500 Loss 1.0742 Accuracy 0.1154\n",
      "Epoch 26 Batch 550 Loss 1.0762 Accuracy 0.1156\n",
      "Epoch 26 Batch 600 Loss 1.0778 Accuracy 0.1156\n",
      "Epoch 26 Batch 650 Loss 1.0790 Accuracy 0.1158\n",
      "Epoch 26 Batch 700 Loss 1.0812 Accuracy 0.1160\n",
      "Epoch 26 Batch 750 Loss 1.0815 Accuracy 0.1158\n",
      "Epoch 26 Batch 800 Loss 1.0823 Accuracy 0.1160\n",
      "Epoch 26 Batch 850 Loss 1.0823 Accuracy 0.1158\n",
      "Epoch 26 Batch 900 Loss 1.0824 Accuracy 0.1159\n",
      "Epoch 26 Batch 950 Loss 1.0818 Accuracy 0.1158\n",
      "Epoch 26 Batch 1000 Loss 1.0824 Accuracy 0.1158\n",
      "Epoch 26 Batch 1050 Loss 1.0828 Accuracy 0.1157\n",
      "Epoch 26 Batch 1100 Loss 1.0830 Accuracy 0.1157\n",
      "Epoch 26 Batch 1150 Loss 1.0828 Accuracy 0.1158\n",
      "Epoch 26 Batch 1200 Loss 1.0824 Accuracy 0.1158\n",
      "Epoch 26 Batch 1250 Loss 1.0810 Accuracy 0.1157\n",
      "Epoch 26 Batch 1300 Loss 1.0800 Accuracy 0.1157\n",
      "Epoch 26 Batch 1350 Loss 1.0801 Accuracy 0.1157\n",
      "Epoch 26 Batch 1400 Loss 1.0788 Accuracy 0.1157\n",
      "Epoch 26 Batch 1450 Loss 1.0789 Accuracy 0.1157\n",
      "Epoch 26 Batch 1500 Loss 1.0787 Accuracy 0.1157\n",
      "Epoch 26 Batch 1550 Loss 1.0783 Accuracy 0.1156\n",
      "Epoch 26 Batch 1600 Loss 1.0776 Accuracy 0.1156\n",
      "Epoch 26 Batch 1650 Loss 1.0774 Accuracy 0.1156\n",
      "Epoch 26 Batch 1700 Loss 1.0776 Accuracy 0.1155\n",
      "Epoch 26 Batch 1750 Loss 1.0780 Accuracy 0.1155\n",
      "Epoch 26 Batch 1800 Loss 1.0778 Accuracy 0.1154\n",
      "Epoch 26 Batch 1850 Loss 1.0783 Accuracy 0.1154\n",
      "Epoch 26 Batch 1900 Loss 1.0781 Accuracy 0.1153\n",
      "Epoch 26 Batch 1950 Loss 1.0779 Accuracy 0.1153\n",
      "Epoch 26 Batch 2000 Loss 1.0780 Accuracy 0.1152\n",
      "Epoch 26 Batch 2050 Loss 1.0775 Accuracy 0.1151\n",
      "Epoch 26 Batch 2100 Loss 1.0772 Accuracy 0.1151\n",
      "Epoch 26 Batch 2150 Loss 1.0772 Accuracy 0.1150\n",
      "Epoch 26 Batch 2200 Loss 1.0780 Accuracy 0.1151\n",
      "Epoch 26 Batch 2250 Loss 1.0784 Accuracy 0.1150\n",
      "Epoch 26 Batch 2300 Loss 1.0784 Accuracy 0.1150\n",
      "Epoch 26 Batch 2350 Loss 1.0790 Accuracy 0.1151\n",
      "Epoch 26 Batch 2400 Loss 1.0795 Accuracy 0.1151\n",
      "Epoch 26 Batch 2450 Loss 1.0803 Accuracy 0.1151\n",
      "Epoch 26 Batch 2500 Loss 1.0810 Accuracy 0.1151\n",
      "Epoch 26 Batch 2550 Loss 1.0809 Accuracy 0.1151\n",
      "Epoch 26 Batch 2600 Loss 1.0813 Accuracy 0.1151\n",
      "Epoch 26 Batch 2650 Loss 1.0817 Accuracy 0.1151\n",
      "Epoch 26 Batch 2700 Loss 1.0819 Accuracy 0.1151\n",
      "Epoch 26 Batch 2750 Loss 1.0823 Accuracy 0.1151\n",
      "Epoch 26 Batch 2800 Loss 1.0828 Accuracy 0.1152\n",
      "Epoch 26 Batch 2850 Loss 1.0830 Accuracy 0.1152\n",
      "Epoch 26 Batch 2900 Loss 1.0832 Accuracy 0.1152\n",
      "Epoch 26 Batch 2950 Loss 1.0833 Accuracy 0.1152\n",
      "Epoch 26 Batch 3000 Loss 1.0836 Accuracy 0.1152\n",
      "Epoch 26 Loss 1.0838 Accuracy 0.1153\n",
      "Time taken for 1 epoch: 183.41 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.1274 Accuracy 0.1246\n",
      "Epoch 27 Batch 50 Loss 1.0973 Accuracy 0.1173\n",
      "Epoch 27 Batch 100 Loss 1.0911 Accuracy 0.1169\n",
      "Epoch 27 Batch 150 Loss 1.0839 Accuracy 0.1167\n",
      "Epoch 27 Batch 200 Loss 1.0822 Accuracy 0.1169\n",
      "Epoch 27 Batch 250 Loss 1.0774 Accuracy 0.1165\n",
      "Epoch 27 Batch 300 Loss 1.0777 Accuracy 0.1166\n",
      "Epoch 27 Batch 350 Loss 1.0773 Accuracy 0.1164\n",
      "Epoch 27 Batch 400 Loss 1.0787 Accuracy 0.1166\n",
      "Epoch 27 Batch 450 Loss 1.0777 Accuracy 0.1163\n",
      "Epoch 27 Batch 500 Loss 1.0761 Accuracy 0.1162\n",
      "Epoch 27 Batch 550 Loss 1.0765 Accuracy 0.1163\n",
      "Epoch 27 Batch 600 Loss 1.0782 Accuracy 0.1165\n",
      "Epoch 27 Batch 650 Loss 1.0791 Accuracy 0.1165\n",
      "Epoch 27 Batch 700 Loss 1.0806 Accuracy 0.1164\n",
      "Epoch 27 Batch 750 Loss 1.0816 Accuracy 0.1164\n",
      "Epoch 27 Batch 800 Loss 1.0804 Accuracy 0.1162\n",
      "Epoch 27 Batch 850 Loss 1.0797 Accuracy 0.1162\n",
      "Epoch 27 Batch 900 Loss 1.0800 Accuracy 0.1162\n",
      "Epoch 27 Batch 950 Loss 1.0808 Accuracy 0.1163\n",
      "Epoch 27 Batch 1000 Loss 1.0812 Accuracy 0.1163\n",
      "Epoch 27 Batch 1050 Loss 1.0821 Accuracy 0.1163\n",
      "Epoch 27 Batch 1100 Loss 1.0824 Accuracy 0.1164\n",
      "Epoch 27 Batch 1150 Loss 1.0817 Accuracy 0.1164\n",
      "Epoch 27 Batch 1200 Loss 1.0812 Accuracy 0.1164\n",
      "Epoch 27 Batch 1250 Loss 1.0794 Accuracy 0.1163\n",
      "Epoch 27 Batch 1300 Loss 1.0781 Accuracy 0.1162\n",
      "Epoch 27 Batch 1350 Loss 1.0770 Accuracy 0.1162\n",
      "Epoch 27 Batch 1400 Loss 1.0766 Accuracy 0.1161\n",
      "Epoch 27 Batch 1450 Loss 1.0758 Accuracy 0.1160\n",
      "Epoch 27 Batch 1500 Loss 1.0762 Accuracy 0.1160\n",
      "Epoch 27 Batch 1550 Loss 1.0761 Accuracy 0.1160\n",
      "Epoch 27 Batch 1600 Loss 1.0757 Accuracy 0.1160\n",
      "Epoch 27 Batch 1650 Loss 1.0745 Accuracy 0.1159\n",
      "Epoch 27 Batch 1700 Loss 1.0748 Accuracy 0.1159\n",
      "Epoch 27 Batch 1750 Loss 1.0753 Accuracy 0.1159\n",
      "Epoch 27 Batch 1800 Loss 1.0755 Accuracy 0.1158\n",
      "Epoch 27 Batch 1850 Loss 1.0763 Accuracy 0.1157\n",
      "Epoch 27 Batch 1900 Loss 1.0766 Accuracy 0.1157\n",
      "Epoch 27 Batch 1950 Loss 1.0766 Accuracy 0.1156\n",
      "Epoch 27 Batch 2000 Loss 1.0762 Accuracy 0.1155\n",
      "Epoch 27 Batch 2050 Loss 1.0754 Accuracy 0.1154\n",
      "Epoch 27 Batch 2100 Loss 1.0754 Accuracy 0.1154\n",
      "Epoch 27 Batch 2150 Loss 1.0758 Accuracy 0.1154\n",
      "Epoch 27 Batch 2200 Loss 1.0764 Accuracy 0.1154\n",
      "Epoch 27 Batch 2250 Loss 1.0770 Accuracy 0.1154\n",
      "Epoch 27 Batch 2300 Loss 1.0767 Accuracy 0.1154\n",
      "Epoch 27 Batch 2350 Loss 1.0771 Accuracy 0.1154\n",
      "Epoch 27 Batch 2400 Loss 1.0777 Accuracy 0.1154\n",
      "Epoch 27 Batch 2450 Loss 1.0780 Accuracy 0.1155\n",
      "Epoch 27 Batch 2500 Loss 1.0781 Accuracy 0.1154\n",
      "Epoch 27 Batch 2550 Loss 1.0783 Accuracy 0.1154\n",
      "Epoch 27 Batch 2600 Loss 1.0789 Accuracy 0.1154\n",
      "Epoch 27 Batch 2650 Loss 1.0794 Accuracy 0.1155\n",
      "Epoch 27 Batch 2700 Loss 1.0796 Accuracy 0.1155\n",
      "Epoch 27 Batch 2750 Loss 1.0798 Accuracy 0.1155\n",
      "Epoch 27 Batch 2800 Loss 1.0800 Accuracy 0.1155\n",
      "Epoch 27 Batch 2850 Loss 1.0805 Accuracy 0.1156\n",
      "Epoch 27 Batch 2900 Loss 1.0806 Accuracy 0.1155\n",
      "Epoch 27 Batch 2950 Loss 1.0808 Accuracy 0.1155\n",
      "Epoch 27 Batch 3000 Loss 1.0810 Accuracy 0.1156\n",
      "Epoch 27 Loss 1.0813 Accuracy 0.1156\n",
      "Time taken for 1 epoch: 183.79 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.8648 Accuracy 0.0966\n",
      "Epoch 28 Batch 50 Loss 1.1158 Accuracy 0.1174\n",
      "Epoch 28 Batch 100 Loss 1.0903 Accuracy 0.1171\n",
      "Epoch 28 Batch 150 Loss 1.0826 Accuracy 0.1166\n",
      "Epoch 28 Batch 200 Loss 1.0766 Accuracy 0.1162\n",
      "Epoch 28 Batch 250 Loss 1.0761 Accuracy 0.1161\n",
      "Epoch 28 Batch 300 Loss 1.0719 Accuracy 0.1162\n",
      "Epoch 28 Batch 350 Loss 1.0700 Accuracy 0.1161\n",
      "Epoch 28 Batch 400 Loss 1.0701 Accuracy 0.1163\n",
      "Epoch 28 Batch 450 Loss 1.0713 Accuracy 0.1163\n",
      "Epoch 28 Batch 500 Loss 1.0700 Accuracy 0.1162\n",
      "Epoch 28 Batch 550 Loss 1.0728 Accuracy 0.1164\n",
      "Epoch 28 Batch 600 Loss 1.0744 Accuracy 0.1164\n",
      "Epoch 28 Batch 650 Loss 1.0736 Accuracy 0.1163\n",
      "Epoch 28 Batch 700 Loss 1.0747 Accuracy 0.1164\n",
      "Epoch 28 Batch 750 Loss 1.0762 Accuracy 0.1164\n",
      "Epoch 28 Batch 800 Loss 1.0771 Accuracy 0.1165\n",
      "Epoch 28 Batch 850 Loss 1.0781 Accuracy 0.1166\n",
      "Epoch 28 Batch 900 Loss 1.0789 Accuracy 0.1167\n",
      "Epoch 28 Batch 950 Loss 1.0782 Accuracy 0.1166\n",
      "Epoch 28 Batch 1000 Loss 1.0790 Accuracy 0.1166\n",
      "Epoch 28 Batch 1050 Loss 1.0787 Accuracy 0.1166\n",
      "Epoch 28 Batch 1100 Loss 1.0792 Accuracy 0.1166\n",
      "Epoch 28 Batch 1150 Loss 1.0799 Accuracy 0.1166\n",
      "Epoch 28 Batch 1200 Loss 1.0782 Accuracy 0.1166\n",
      "Epoch 28 Batch 1250 Loss 1.0760 Accuracy 0.1164\n",
      "Epoch 28 Batch 1300 Loss 1.0748 Accuracy 0.1163\n",
      "Epoch 28 Batch 1350 Loss 1.0746 Accuracy 0.1163\n",
      "Epoch 28 Batch 1400 Loss 1.0749 Accuracy 0.1164\n",
      "Epoch 28 Batch 1450 Loss 1.0747 Accuracy 0.1164\n",
      "Epoch 28 Batch 1500 Loss 1.0743 Accuracy 0.1164\n",
      "Epoch 28 Batch 1550 Loss 1.0749 Accuracy 0.1164\n",
      "Epoch 28 Batch 1600 Loss 1.0749 Accuracy 0.1164\n",
      "Epoch 28 Batch 1650 Loss 1.0739 Accuracy 0.1163\n",
      "Epoch 28 Batch 1700 Loss 1.0740 Accuracy 0.1162\n",
      "Epoch 28 Batch 1750 Loss 1.0748 Accuracy 0.1162\n",
      "Epoch 28 Batch 1800 Loss 1.0744 Accuracy 0.1161\n",
      "Epoch 28 Batch 1850 Loss 1.0743 Accuracy 0.1161\n",
      "Epoch 28 Batch 1900 Loss 1.0739 Accuracy 0.1160\n",
      "Epoch 28 Batch 1950 Loss 1.0738 Accuracy 0.1159\n",
      "Epoch 28 Batch 2000 Loss 1.0733 Accuracy 0.1158\n",
      "Epoch 28 Batch 2050 Loss 1.0732 Accuracy 0.1158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 2100 Loss 1.0728 Accuracy 0.1157\n",
      "Epoch 28 Batch 2150 Loss 1.0734 Accuracy 0.1157\n",
      "Epoch 28 Batch 2200 Loss 1.0735 Accuracy 0.1157\n",
      "Epoch 28 Batch 2250 Loss 1.0744 Accuracy 0.1157\n",
      "Epoch 28 Batch 2300 Loss 1.0745 Accuracy 0.1157\n",
      "Epoch 28 Batch 2350 Loss 1.0750 Accuracy 0.1157\n",
      "Epoch 28 Batch 2400 Loss 1.0750 Accuracy 0.1157\n",
      "Epoch 28 Batch 2450 Loss 1.0749 Accuracy 0.1157\n",
      "Epoch 28 Batch 2500 Loss 1.0757 Accuracy 0.1157\n",
      "Epoch 28 Batch 2550 Loss 1.0761 Accuracy 0.1158\n",
      "Epoch 28 Batch 2600 Loss 1.0769 Accuracy 0.1159\n",
      "Epoch 28 Batch 2650 Loss 1.0770 Accuracy 0.1159\n",
      "Epoch 28 Batch 2700 Loss 1.0771 Accuracy 0.1159\n",
      "Epoch 28 Batch 2750 Loss 1.0772 Accuracy 0.1159\n",
      "Epoch 28 Batch 2800 Loss 1.0774 Accuracy 0.1159\n",
      "Epoch 28 Batch 2850 Loss 1.0777 Accuracy 0.1159\n",
      "Epoch 28 Batch 2900 Loss 1.0781 Accuracy 0.1159\n",
      "Epoch 28 Batch 2950 Loss 1.0783 Accuracy 0.1159\n",
      "Epoch 28 Batch 3000 Loss 1.0784 Accuracy 0.1159\n",
      "Epoch 28 Loss 1.0792 Accuracy 0.1160\n",
      "Time taken for 1 epoch: 183.71 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.9841 Accuracy 0.1122\n",
      "Epoch 29 Batch 50 Loss 1.0713 Accuracy 0.1157\n",
      "Epoch 29 Batch 100 Loss 1.0868 Accuracy 0.1175\n",
      "Epoch 29 Batch 150 Loss 1.0788 Accuracy 0.1171\n",
      "Epoch 29 Batch 200 Loss 1.0789 Accuracy 0.1172\n",
      "Epoch 29 Batch 250 Loss 1.0734 Accuracy 0.1170\n",
      "Epoch 29 Batch 300 Loss 1.0710 Accuracy 0.1167\n",
      "Epoch 29 Batch 350 Loss 1.0714 Accuracy 0.1166\n",
      "Epoch 29 Batch 400 Loss 1.0706 Accuracy 0.1167\n",
      "Epoch 29 Batch 450 Loss 1.0697 Accuracy 0.1165\n",
      "Epoch 29 Batch 500 Loss 1.0708 Accuracy 0.1165\n",
      "Epoch 29 Batch 550 Loss 1.0714 Accuracy 0.1165\n",
      "Epoch 29 Batch 600 Loss 1.0719 Accuracy 0.1164\n",
      "Epoch 29 Batch 650 Loss 1.0725 Accuracy 0.1166\n",
      "Epoch 29 Batch 700 Loss 1.0734 Accuracy 0.1166\n",
      "Epoch 29 Batch 750 Loss 1.0756 Accuracy 0.1169\n",
      "Epoch 29 Batch 800 Loss 1.0759 Accuracy 0.1168\n",
      "Epoch 29 Batch 850 Loss 1.0768 Accuracy 0.1169\n",
      "Epoch 29 Batch 900 Loss 1.0772 Accuracy 0.1170\n",
      "Epoch 29 Batch 950 Loss 1.0783 Accuracy 0.1170\n",
      "Epoch 29 Batch 1000 Loss 1.0788 Accuracy 0.1170\n",
      "Epoch 29 Batch 1050 Loss 1.0778 Accuracy 0.1169\n",
      "Epoch 29 Batch 1100 Loss 1.0781 Accuracy 0.1168\n",
      "Epoch 29 Batch 1150 Loss 1.0774 Accuracy 0.1168\n",
      "Epoch 29 Batch 1200 Loss 1.0770 Accuracy 0.1167\n",
      "Epoch 29 Batch 1250 Loss 1.0758 Accuracy 0.1167\n",
      "Epoch 29 Batch 1300 Loss 1.0741 Accuracy 0.1166\n",
      "Epoch 29 Batch 1350 Loss 1.0733 Accuracy 0.1166\n",
      "Epoch 29 Batch 1400 Loss 1.0735 Accuracy 0.1166\n",
      "Epoch 29 Batch 1450 Loss 1.0728 Accuracy 0.1166\n",
      "Epoch 29 Batch 1500 Loss 1.0723 Accuracy 0.1166\n",
      "Epoch 29 Batch 1550 Loss 1.0723 Accuracy 0.1165\n",
      "Epoch 29 Batch 1600 Loss 1.0712 Accuracy 0.1164\n",
      "Epoch 29 Batch 1650 Loss 1.0706 Accuracy 0.1163\n",
      "Epoch 29 Batch 1700 Loss 1.0701 Accuracy 0.1163\n",
      "Epoch 29 Batch 1750 Loss 1.0712 Accuracy 0.1163\n",
      "Epoch 29 Batch 1800 Loss 1.0712 Accuracy 0.1162\n",
      "Epoch 29 Batch 1850 Loss 1.0715 Accuracy 0.1162\n",
      "Epoch 29 Batch 1900 Loss 1.0717 Accuracy 0.1161\n",
      "Epoch 29 Batch 1950 Loss 1.0712 Accuracy 0.1160\n",
      "Epoch 29 Batch 2000 Loss 1.0713 Accuracy 0.1159\n",
      "Epoch 29 Batch 2050 Loss 1.0707 Accuracy 0.1159\n",
      "Epoch 29 Batch 2100 Loss 1.0702 Accuracy 0.1158\n",
      "Epoch 29 Batch 2150 Loss 1.0709 Accuracy 0.1158\n",
      "Epoch 29 Batch 2200 Loss 1.0715 Accuracy 0.1158\n",
      "Epoch 29 Batch 2250 Loss 1.0717 Accuracy 0.1158\n",
      "Epoch 29 Batch 2300 Loss 1.0718 Accuracy 0.1158\n",
      "Epoch 29 Batch 2350 Loss 1.0721 Accuracy 0.1158\n",
      "Epoch 29 Batch 2400 Loss 1.0725 Accuracy 0.1158\n",
      "Epoch 29 Batch 2450 Loss 1.0729 Accuracy 0.1158\n",
      "Epoch 29 Batch 2500 Loss 1.0734 Accuracy 0.1159\n",
      "Epoch 29 Batch 2550 Loss 1.0741 Accuracy 0.1159\n",
      "Epoch 29 Batch 2600 Loss 1.0745 Accuracy 0.1159\n",
      "Epoch 29 Batch 2650 Loss 1.0742 Accuracy 0.1159\n",
      "Epoch 29 Batch 2700 Loss 1.0742 Accuracy 0.1159\n",
      "Epoch 29 Batch 2750 Loss 1.0744 Accuracy 0.1159\n",
      "Epoch 29 Batch 2800 Loss 1.0747 Accuracy 0.1160\n",
      "Epoch 29 Batch 2850 Loss 1.0750 Accuracy 0.1160\n",
      "Epoch 29 Batch 2900 Loss 1.0753 Accuracy 0.1160\n",
      "Epoch 29 Batch 2950 Loss 1.0760 Accuracy 0.1160\n",
      "Epoch 29 Batch 3000 Loss 1.0766 Accuracy 0.1161\n",
      "Epoch 29 Loss 1.0771 Accuracy 0.1161\n",
      "Time taken for 1 epoch: 183.83 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.8577 Accuracy 0.1002\n",
      "Epoch 30 Batch 50 Loss 1.0541 Accuracy 0.1149\n",
      "Epoch 30 Batch 100 Loss 1.0723 Accuracy 0.1168\n",
      "Epoch 30 Batch 150 Loss 1.0720 Accuracy 0.1169\n",
      "Epoch 30 Batch 200 Loss 1.0719 Accuracy 0.1168\n",
      "Epoch 30 Batch 250 Loss 1.0689 Accuracy 0.1167\n",
      "Epoch 30 Batch 300 Loss 1.0684 Accuracy 0.1170\n",
      "Epoch 30 Batch 350 Loss 1.0660 Accuracy 0.1169\n",
      "Epoch 30 Batch 400 Loss 1.0668 Accuracy 0.1169\n",
      "Epoch 30 Batch 450 Loss 1.0670 Accuracy 0.1171\n",
      "Epoch 30 Batch 500 Loss 1.0685 Accuracy 0.1171\n",
      "Epoch 30 Batch 550 Loss 1.0703 Accuracy 0.1171\n",
      "Epoch 30 Batch 600 Loss 1.0691 Accuracy 0.1169\n",
      "Epoch 30 Batch 650 Loss 1.0699 Accuracy 0.1169\n",
      "Epoch 30 Batch 700 Loss 1.0710 Accuracy 0.1168\n",
      "Epoch 30 Batch 750 Loss 1.0696 Accuracy 0.1166\n",
      "Epoch 30 Batch 800 Loss 1.0714 Accuracy 0.1167\n",
      "Epoch 30 Batch 850 Loss 1.0725 Accuracy 0.1169\n",
      "Epoch 30 Batch 900 Loss 1.0733 Accuracy 0.1170\n",
      "Epoch 30 Batch 950 Loss 1.0732 Accuracy 0.1169\n",
      "Epoch 30 Batch 1000 Loss 1.0739 Accuracy 0.1171\n",
      "Epoch 30 Batch 1050 Loss 1.0737 Accuracy 0.1171\n",
      "Epoch 30 Batch 1100 Loss 1.0737 Accuracy 0.1171\n",
      "Epoch 30 Batch 1150 Loss 1.0739 Accuracy 0.1171\n",
      "Epoch 30 Batch 1200 Loss 1.0731 Accuracy 0.1171\n",
      "Epoch 30 Batch 1250 Loss 1.0722 Accuracy 0.1170\n",
      "Epoch 30 Batch 1300 Loss 1.0711 Accuracy 0.1169\n",
      "Epoch 30 Batch 1350 Loss 1.0706 Accuracy 0.1168\n",
      "Epoch 30 Batch 1400 Loss 1.0705 Accuracy 0.1168\n",
      "Epoch 30 Batch 1450 Loss 1.0711 Accuracy 0.1169\n",
      "Epoch 30 Batch 1500 Loss 1.0708 Accuracy 0.1169\n",
      "Epoch 30 Batch 1550 Loss 1.0710 Accuracy 0.1169\n",
      "Epoch 30 Batch 1600 Loss 1.0704 Accuracy 0.1168\n",
      "Epoch 30 Batch 1650 Loss 1.0698 Accuracy 0.1167\n",
      "Epoch 30 Batch 1700 Loss 1.0682 Accuracy 0.1166\n",
      "Epoch 30 Batch 1750 Loss 1.0684 Accuracy 0.1165\n",
      "Epoch 30 Batch 1800 Loss 1.0686 Accuracy 0.1166\n",
      "Epoch 30 Batch 1850 Loss 1.0689 Accuracy 0.1165\n",
      "Epoch 30 Batch 1900 Loss 1.0695 Accuracy 0.1165\n",
      "Epoch 30 Batch 1950 Loss 1.0695 Accuracy 0.1164\n",
      "Epoch 30 Batch 2000 Loss 1.0689 Accuracy 0.1163\n",
      "Epoch 30 Batch 2050 Loss 1.0685 Accuracy 0.1162\n",
      "Epoch 30 Batch 2100 Loss 1.0688 Accuracy 0.1161\n",
      "Epoch 30 Batch 2150 Loss 1.0696 Accuracy 0.1162\n",
      "Epoch 30 Batch 2200 Loss 1.0698 Accuracy 0.1161\n",
      "Epoch 30 Batch 2250 Loss 1.0703 Accuracy 0.1162\n",
      "Epoch 30 Batch 2300 Loss 1.0707 Accuracy 0.1162\n",
      "Epoch 30 Batch 2350 Loss 1.0718 Accuracy 0.1163\n",
      "Epoch 30 Batch 2400 Loss 1.0720 Accuracy 0.1163\n",
      "Epoch 30 Batch 2450 Loss 1.0725 Accuracy 0.1163\n",
      "Epoch 30 Batch 2500 Loss 1.0731 Accuracy 0.1163\n",
      "Epoch 30 Batch 2550 Loss 1.0731 Accuracy 0.1163\n",
      "Epoch 30 Batch 2600 Loss 1.0731 Accuracy 0.1163\n",
      "Epoch 30 Batch 2650 Loss 1.0734 Accuracy 0.1163\n",
      "Epoch 30 Batch 2700 Loss 1.0736 Accuracy 0.1164\n",
      "Epoch 30 Batch 2750 Loss 1.0737 Accuracy 0.1164\n",
      "Epoch 30 Batch 2800 Loss 1.0742 Accuracy 0.1164\n",
      "Epoch 30 Batch 2850 Loss 1.0742 Accuracy 0.1164\n",
      "Epoch 30 Batch 2900 Loss 1.0743 Accuracy 0.1164\n",
      "Epoch 30 Batch 2950 Loss 1.0742 Accuracy 0.1164\n",
      "Epoch 30 Batch 3000 Loss 1.0747 Accuracy 0.1164\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
      "Epoch 30 Loss 1.0749 Accuracy 0.1164\n",
      "Time taken for 1 epoch: 184.38 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.8768 Accuracy 0.0950\n",
      "Epoch 31 Batch 50 Loss 1.0642 Accuracy 0.1167\n",
      "Epoch 31 Batch 100 Loss 1.0799 Accuracy 0.1181\n",
      "Epoch 31 Batch 150 Loss 1.0758 Accuracy 0.1183\n",
      "Epoch 31 Batch 200 Loss 1.0661 Accuracy 0.1168\n",
      "Epoch 31 Batch 250 Loss 1.0623 Accuracy 0.1168\n",
      "Epoch 31 Batch 300 Loss 1.0605 Accuracy 0.1171\n",
      "Epoch 31 Batch 350 Loss 1.0598 Accuracy 0.1171\n",
      "Epoch 31 Batch 400 Loss 1.0627 Accuracy 0.1175\n",
      "Epoch 31 Batch 450 Loss 1.0637 Accuracy 0.1174\n",
      "Epoch 31 Batch 500 Loss 1.0654 Accuracy 0.1174\n",
      "Epoch 31 Batch 550 Loss 1.0659 Accuracy 0.1173\n",
      "Epoch 31 Batch 600 Loss 1.0662 Accuracy 0.1174\n",
      "Epoch 31 Batch 650 Loss 1.0651 Accuracy 0.1173\n",
      "Epoch 31 Batch 700 Loss 1.0653 Accuracy 0.1172\n",
      "Epoch 31 Batch 750 Loss 1.0667 Accuracy 0.1172\n",
      "Epoch 31 Batch 800 Loss 1.0671 Accuracy 0.1173\n",
      "Epoch 31 Batch 850 Loss 1.0687 Accuracy 0.1173\n",
      "Epoch 31 Batch 900 Loss 1.0692 Accuracy 0.1174\n",
      "Epoch 31 Batch 950 Loss 1.0688 Accuracy 0.1174\n",
      "Epoch 31 Batch 1000 Loss 1.0703 Accuracy 0.1175\n",
      "Epoch 31 Batch 1050 Loss 1.0718 Accuracy 0.1176\n",
      "Epoch 31 Batch 1100 Loss 1.0712 Accuracy 0.1175\n",
      "Epoch 31 Batch 1150 Loss 1.0707 Accuracy 0.1175\n",
      "Epoch 31 Batch 1200 Loss 1.0717 Accuracy 0.1175\n",
      "Epoch 31 Batch 1250 Loss 1.0698 Accuracy 0.1174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 1300 Loss 1.0684 Accuracy 0.1174\n",
      "Epoch 31 Batch 1350 Loss 1.0675 Accuracy 0.1173\n",
      "Epoch 31 Batch 1400 Loss 1.0672 Accuracy 0.1173\n",
      "Epoch 31 Batch 1450 Loss 1.0678 Accuracy 0.1173\n",
      "Epoch 31 Batch 1500 Loss 1.0682 Accuracy 0.1173\n",
      "Epoch 31 Batch 1550 Loss 1.0685 Accuracy 0.1172\n",
      "Epoch 31 Batch 1600 Loss 1.0684 Accuracy 0.1172\n",
      "Epoch 31 Batch 1650 Loss 1.0682 Accuracy 0.1172\n",
      "Epoch 31 Batch 1700 Loss 1.0685 Accuracy 0.1172\n",
      "Epoch 31 Batch 1750 Loss 1.0690 Accuracy 0.1171\n",
      "Epoch 31 Batch 1800 Loss 1.0685 Accuracy 0.1171\n",
      "Epoch 31 Batch 1850 Loss 1.0683 Accuracy 0.1170\n",
      "Epoch 31 Batch 1900 Loss 1.0680 Accuracy 0.1169\n",
      "Epoch 31 Batch 1950 Loss 1.0684 Accuracy 0.1168\n",
      "Epoch 31 Batch 2000 Loss 1.0672 Accuracy 0.1167\n",
      "Epoch 31 Batch 2050 Loss 1.0665 Accuracy 0.1166\n",
      "Epoch 31 Batch 2100 Loss 1.0668 Accuracy 0.1166\n",
      "Epoch 31 Batch 2150 Loss 1.0664 Accuracy 0.1165\n",
      "Epoch 31 Batch 2200 Loss 1.0670 Accuracy 0.1165\n",
      "Epoch 31 Batch 2250 Loss 1.0679 Accuracy 0.1165\n",
      "Epoch 31 Batch 2300 Loss 1.0684 Accuracy 0.1165\n",
      "Epoch 31 Batch 2350 Loss 1.0690 Accuracy 0.1165\n",
      "Epoch 31 Batch 2400 Loss 1.0694 Accuracy 0.1165\n",
      "Epoch 31 Batch 2450 Loss 1.0702 Accuracy 0.1166\n",
      "Epoch 31 Batch 2500 Loss 1.0706 Accuracy 0.1166\n",
      "Epoch 31 Batch 2550 Loss 1.0711 Accuracy 0.1166\n",
      "Epoch 31 Batch 2600 Loss 1.0710 Accuracy 0.1165\n",
      "Epoch 31 Batch 2650 Loss 1.0715 Accuracy 0.1166\n",
      "Epoch 31 Batch 2700 Loss 1.0715 Accuracy 0.1166\n",
      "Epoch 31 Batch 2750 Loss 1.0717 Accuracy 0.1166\n",
      "Epoch 31 Batch 2800 Loss 1.0718 Accuracy 0.1166\n",
      "Epoch 31 Batch 2850 Loss 1.0723 Accuracy 0.1166\n",
      "Epoch 31 Batch 2900 Loss 1.0721 Accuracy 0.1166\n",
      "Epoch 31 Batch 2950 Loss 1.0720 Accuracy 0.1166\n",
      "Epoch 31 Batch 3000 Loss 1.0724 Accuracy 0.1166\n",
      "Epoch 31 Loss 1.0728 Accuracy 0.1167\n",
      "Time taken for 1 epoch: 179.75 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.9237 Accuracy 0.1070\n",
      "Epoch 32 Batch 50 Loss 1.0547 Accuracy 0.1163\n",
      "Epoch 32 Batch 100 Loss 1.0611 Accuracy 0.1177\n",
      "Epoch 32 Batch 150 Loss 1.0626 Accuracy 0.1173\n",
      "Epoch 32 Batch 200 Loss 1.0633 Accuracy 0.1176\n",
      "Epoch 32 Batch 250 Loss 1.0639 Accuracy 0.1179\n",
      "Epoch 32 Batch 300 Loss 1.0651 Accuracy 0.1182\n",
      "Epoch 32 Batch 350 Loss 1.0604 Accuracy 0.1178\n",
      "Epoch 32 Batch 400 Loss 1.0611 Accuracy 0.1179\n",
      "Epoch 32 Batch 450 Loss 1.0634 Accuracy 0.1178\n",
      "Epoch 32 Batch 500 Loss 1.0654 Accuracy 0.1181\n",
      "Epoch 32 Batch 550 Loss 1.0667 Accuracy 0.1180\n",
      "Epoch 32 Batch 600 Loss 1.0659 Accuracy 0.1178\n",
      "Epoch 32 Batch 650 Loss 1.0662 Accuracy 0.1178\n",
      "Epoch 32 Batch 700 Loss 1.0663 Accuracy 0.1178\n",
      "Epoch 32 Batch 750 Loss 1.0662 Accuracy 0.1177\n",
      "Epoch 32 Batch 800 Loss 1.0679 Accuracy 0.1177\n",
      "Epoch 32 Batch 850 Loss 1.0684 Accuracy 0.1176\n",
      "Epoch 32 Batch 900 Loss 1.0688 Accuracy 0.1177\n",
      "Epoch 32 Batch 950 Loss 1.0690 Accuracy 0.1177\n",
      "Epoch 32 Batch 1000 Loss 1.0698 Accuracy 0.1177\n",
      "Epoch 32 Batch 1050 Loss 1.0689 Accuracy 0.1177\n",
      "Epoch 32 Batch 1100 Loss 1.0687 Accuracy 0.1176\n",
      "Epoch 32 Batch 1150 Loss 1.0692 Accuracy 0.1176\n",
      "Epoch 32 Batch 1200 Loss 1.0701 Accuracy 0.1178\n",
      "Epoch 32 Batch 1250 Loss 1.0678 Accuracy 0.1176\n",
      "Epoch 32 Batch 1300 Loss 1.0675 Accuracy 0.1175\n",
      "Epoch 32 Batch 1350 Loss 1.0675 Accuracy 0.1176\n",
      "Epoch 32 Batch 1400 Loss 1.0668 Accuracy 0.1175\n",
      "Epoch 32 Batch 1450 Loss 1.0670 Accuracy 0.1176\n",
      "Epoch 32 Batch 1500 Loss 1.0660 Accuracy 0.1175\n",
      "Epoch 32 Batch 1550 Loss 1.0659 Accuracy 0.1175\n",
      "Epoch 32 Batch 1600 Loss 1.0656 Accuracy 0.1175\n",
      "Epoch 32 Batch 1650 Loss 1.0652 Accuracy 0.1174\n",
      "Epoch 32 Batch 1700 Loss 1.0652 Accuracy 0.1173\n",
      "Epoch 32 Batch 1750 Loss 1.0660 Accuracy 0.1173\n",
      "Epoch 32 Batch 1800 Loss 1.0656 Accuracy 0.1173\n",
      "Epoch 32 Batch 1850 Loss 1.0655 Accuracy 0.1172\n",
      "Epoch 32 Batch 1900 Loss 1.0656 Accuracy 0.1171\n",
      "Epoch 32 Batch 1950 Loss 1.0653 Accuracy 0.1170\n",
      "Epoch 32 Batch 2000 Loss 1.0647 Accuracy 0.1169\n",
      "Epoch 32 Batch 2050 Loss 1.0649 Accuracy 0.1168\n",
      "Epoch 32 Batch 2100 Loss 1.0651 Accuracy 0.1168\n",
      "Epoch 32 Batch 2150 Loss 1.0646 Accuracy 0.1168\n",
      "Epoch 32 Batch 2200 Loss 1.0652 Accuracy 0.1168\n",
      "Epoch 32 Batch 2250 Loss 1.0655 Accuracy 0.1168\n",
      "Epoch 32 Batch 2300 Loss 1.0658 Accuracy 0.1168\n",
      "Epoch 32 Batch 2350 Loss 1.0667 Accuracy 0.1169\n",
      "Epoch 32 Batch 2400 Loss 1.0676 Accuracy 0.1169\n",
      "Epoch 32 Batch 2450 Loss 1.0675 Accuracy 0.1168\n",
      "Epoch 32 Batch 2500 Loss 1.0679 Accuracy 0.1168\n",
      "Epoch 32 Batch 2550 Loss 1.0683 Accuracy 0.1169\n",
      "Epoch 32 Batch 2600 Loss 1.0687 Accuracy 0.1169\n",
      "Epoch 32 Batch 2650 Loss 1.0690 Accuracy 0.1169\n",
      "Epoch 32 Batch 2700 Loss 1.0692 Accuracy 0.1169\n",
      "Epoch 32 Batch 2750 Loss 1.0698 Accuracy 0.1169\n",
      "Epoch 32 Batch 2800 Loss 1.0699 Accuracy 0.1169\n",
      "Epoch 32 Batch 2850 Loss 1.0701 Accuracy 0.1169\n",
      "Epoch 32 Batch 2900 Loss 1.0701 Accuracy 0.1169\n",
      "Epoch 32 Batch 2950 Loss 1.0708 Accuracy 0.1170\n",
      "Epoch 32 Batch 3000 Loss 1.0708 Accuracy 0.1170\n",
      "Epoch 32 Loss 1.0708 Accuracy 0.1170\n",
      "Time taken for 1 epoch: 188.81 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 1.0021 Accuracy 0.1142\n",
      "Epoch 33 Batch 50 Loss 1.0703 Accuracy 0.1169\n",
      "Epoch 33 Batch 100 Loss 1.0810 Accuracy 0.1187\n",
      "Epoch 33 Batch 150 Loss 1.0662 Accuracy 0.1178\n",
      "Epoch 33 Batch 200 Loss 1.0712 Accuracy 0.1183\n",
      "Epoch 33 Batch 250 Loss 1.0663 Accuracy 0.1181\n",
      "Epoch 33 Batch 300 Loss 1.0661 Accuracy 0.1180\n",
      "Epoch 33 Batch 350 Loss 1.0633 Accuracy 0.1178\n",
      "Epoch 33 Batch 400 Loss 1.0628 Accuracy 0.1180\n",
      "Epoch 33 Batch 450 Loss 1.0606 Accuracy 0.1178\n",
      "Epoch 33 Batch 500 Loss 1.0624 Accuracy 0.1180\n",
      "Epoch 33 Batch 550 Loss 1.0654 Accuracy 0.1181\n",
      "Epoch 33 Batch 600 Loss 1.0655 Accuracy 0.1180\n",
      "Epoch 33 Batch 650 Loss 1.0637 Accuracy 0.1179\n",
      "Epoch 33 Batch 700 Loss 1.0643 Accuracy 0.1178\n",
      "Epoch 33 Batch 750 Loss 1.0649 Accuracy 0.1178\n",
      "Epoch 33 Batch 800 Loss 1.0646 Accuracy 0.1177\n",
      "Epoch 33 Batch 850 Loss 1.0650 Accuracy 0.1177\n",
      "Epoch 33 Batch 900 Loss 1.0659 Accuracy 0.1178\n",
      "Epoch 33 Batch 950 Loss 1.0668 Accuracy 0.1180\n",
      "Epoch 33 Batch 1000 Loss 1.0678 Accuracy 0.1180\n",
      "Epoch 33 Batch 1050 Loss 1.0686 Accuracy 0.1180\n",
      "Epoch 33 Batch 1100 Loss 1.0687 Accuracy 0.1179\n",
      "Epoch 33 Batch 1150 Loss 1.0688 Accuracy 0.1179\n",
      "Epoch 33 Batch 1200 Loss 1.0692 Accuracy 0.1179\n",
      "Epoch 33 Batch 1250 Loss 1.0678 Accuracy 0.1178\n",
      "Epoch 33 Batch 1300 Loss 1.0666 Accuracy 0.1177\n",
      "Epoch 33 Batch 1350 Loss 1.0664 Accuracy 0.1178\n",
      "Epoch 33 Batch 1400 Loss 1.0651 Accuracy 0.1177\n",
      "Epoch 33 Batch 1450 Loss 1.0652 Accuracy 0.1176\n",
      "Epoch 33 Batch 1500 Loss 1.0650 Accuracy 0.1176\n",
      "Epoch 33 Batch 1550 Loss 1.0650 Accuracy 0.1176\n",
      "Epoch 33 Batch 1600 Loss 1.0640 Accuracy 0.1175\n",
      "Epoch 33 Batch 1650 Loss 1.0637 Accuracy 0.1175\n",
      "Epoch 33 Batch 1700 Loss 1.0631 Accuracy 0.1174\n",
      "Epoch 33 Batch 1750 Loss 1.0634 Accuracy 0.1174\n",
      "Epoch 33 Batch 1800 Loss 1.0639 Accuracy 0.1174\n",
      "Epoch 33 Batch 1850 Loss 1.0648 Accuracy 0.1174\n",
      "Epoch 33 Batch 1900 Loss 1.0645 Accuracy 0.1173\n",
      "Epoch 33 Batch 1950 Loss 1.0641 Accuracy 0.1172\n",
      "Epoch 33 Batch 2000 Loss 1.0635 Accuracy 0.1171\n",
      "Epoch 33 Batch 2050 Loss 1.0635 Accuracy 0.1170\n",
      "Epoch 33 Batch 2100 Loss 1.0631 Accuracy 0.1169\n",
      "Epoch 33 Batch 2150 Loss 1.0636 Accuracy 0.1169\n",
      "Epoch 33 Batch 2200 Loss 1.0646 Accuracy 0.1169\n",
      "Epoch 33 Batch 2250 Loss 1.0649 Accuracy 0.1169\n",
      "Epoch 33 Batch 2300 Loss 1.0648 Accuracy 0.1169\n",
      "Epoch 33 Batch 2350 Loss 1.0652 Accuracy 0.1170\n",
      "Epoch 33 Batch 2400 Loss 1.0653 Accuracy 0.1169\n",
      "Epoch 33 Batch 2450 Loss 1.0661 Accuracy 0.1170\n",
      "Epoch 33 Batch 2500 Loss 1.0661 Accuracy 0.1170\n",
      "Epoch 33 Batch 2550 Loss 1.0663 Accuracy 0.1170\n",
      "Epoch 33 Batch 2600 Loss 1.0667 Accuracy 0.1170\n",
      "Epoch 33 Batch 2650 Loss 1.0668 Accuracy 0.1170\n",
      "Epoch 33 Batch 2700 Loss 1.0672 Accuracy 0.1171\n",
      "Epoch 33 Batch 2750 Loss 1.0676 Accuracy 0.1171\n",
      "Epoch 33 Batch 2800 Loss 1.0675 Accuracy 0.1171\n",
      "Epoch 33 Batch 2850 Loss 1.0678 Accuracy 0.1171\n",
      "Epoch 33 Batch 2900 Loss 1.0684 Accuracy 0.1171\n",
      "Epoch 33 Batch 2950 Loss 1.0685 Accuracy 0.1171\n",
      "Epoch 33 Batch 3000 Loss 1.0688 Accuracy 0.1172\n",
      "Epoch 33 Loss 1.0692 Accuracy 0.1172\n",
      "Time taken for 1 epoch: 179.88 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 1.1081 Accuracy 0.1182\n",
      "Epoch 34 Batch 50 Loss 1.0803 Accuracy 0.1184\n",
      "Epoch 34 Batch 100 Loss 1.0772 Accuracy 0.1185\n",
      "Epoch 34 Batch 150 Loss 1.0738 Accuracy 0.1184\n",
      "Epoch 34 Batch 200 Loss 1.0749 Accuracy 0.1184\n",
      "Epoch 34 Batch 250 Loss 1.0699 Accuracy 0.1181\n",
      "Epoch 34 Batch 300 Loss 1.0665 Accuracy 0.1180\n",
      "Epoch 34 Batch 350 Loss 1.0637 Accuracy 0.1176\n",
      "Epoch 34 Batch 400 Loss 1.0644 Accuracy 0.1179\n",
      "Epoch 34 Batch 450 Loss 1.0663 Accuracy 0.1180\n",
      "Epoch 34 Batch 500 Loss 1.0675 Accuracy 0.1180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 550 Loss 1.0688 Accuracy 0.1181\n",
      "Epoch 34 Batch 600 Loss 1.0670 Accuracy 0.1180\n",
      "Epoch 34 Batch 650 Loss 1.0671 Accuracy 0.1180\n",
      "Epoch 34 Batch 700 Loss 1.0677 Accuracy 0.1180\n",
      "Epoch 34 Batch 750 Loss 1.0669 Accuracy 0.1180\n",
      "Epoch 34 Batch 800 Loss 1.0659 Accuracy 0.1179\n",
      "Epoch 34 Batch 850 Loss 1.0662 Accuracy 0.1179\n",
      "Epoch 34 Batch 900 Loss 1.0674 Accuracy 0.1180\n",
      "Epoch 34 Batch 950 Loss 1.0674 Accuracy 0.1180\n",
      "Epoch 34 Batch 1000 Loss 1.0667 Accuracy 0.1179\n",
      "Epoch 34 Batch 1050 Loss 1.0662 Accuracy 0.1179\n",
      "Epoch 34 Batch 1100 Loss 1.0654 Accuracy 0.1179\n",
      "Epoch 34 Batch 1150 Loss 1.0649 Accuracy 0.1178\n",
      "Epoch 34 Batch 1200 Loss 1.0650 Accuracy 0.1178\n",
      "Epoch 34 Batch 1250 Loss 1.0639 Accuracy 0.1177\n",
      "Epoch 34 Batch 1300 Loss 1.0629 Accuracy 0.1177\n",
      "Epoch 34 Batch 1350 Loss 1.0633 Accuracy 0.1178\n",
      "Epoch 34 Batch 1400 Loss 1.0636 Accuracy 0.1178\n",
      "Epoch 34 Batch 1450 Loss 1.0633 Accuracy 0.1178\n",
      "Epoch 34 Batch 1500 Loss 1.0631 Accuracy 0.1177\n",
      "Epoch 34 Batch 1550 Loss 1.0633 Accuracy 0.1177\n",
      "Epoch 34 Batch 1600 Loss 1.0625 Accuracy 0.1176\n",
      "Epoch 34 Batch 1650 Loss 1.0622 Accuracy 0.1175\n",
      "Epoch 34 Batch 1700 Loss 1.0626 Accuracy 0.1175\n",
      "Epoch 34 Batch 1750 Loss 1.0630 Accuracy 0.1176\n",
      "Epoch 34 Batch 1800 Loss 1.0626 Accuracy 0.1175\n",
      "Epoch 34 Batch 1850 Loss 1.0626 Accuracy 0.1174\n",
      "Epoch 34 Batch 1900 Loss 1.0628 Accuracy 0.1173\n",
      "Epoch 34 Batch 1950 Loss 1.0632 Accuracy 0.1173\n",
      "Epoch 34 Batch 2000 Loss 1.0625 Accuracy 0.1172\n",
      "Epoch 34 Batch 2050 Loss 1.0618 Accuracy 0.1171\n",
      "Epoch 34 Batch 2100 Loss 1.0616 Accuracy 0.1171\n",
      "Epoch 34 Batch 2150 Loss 1.0623 Accuracy 0.1171\n",
      "Epoch 34 Batch 2200 Loss 1.0626 Accuracy 0.1171\n",
      "Epoch 34 Batch 2250 Loss 1.0629 Accuracy 0.1171\n",
      "Epoch 34 Batch 2300 Loss 1.0633 Accuracy 0.1171\n",
      "Epoch 34 Batch 2350 Loss 1.0638 Accuracy 0.1172\n",
      "Epoch 34 Batch 2400 Loss 1.0642 Accuracy 0.1172\n",
      "Epoch 34 Batch 2450 Loss 1.0640 Accuracy 0.1171\n",
      "Epoch 34 Batch 2500 Loss 1.0638 Accuracy 0.1171\n",
      "Epoch 34 Batch 2550 Loss 1.0646 Accuracy 0.1171\n",
      "Epoch 34 Batch 2600 Loss 1.0649 Accuracy 0.1171\n",
      "Epoch 34 Batch 2650 Loss 1.0655 Accuracy 0.1171\n",
      "Epoch 34 Batch 2700 Loss 1.0659 Accuracy 0.1172\n",
      "Epoch 34 Batch 2750 Loss 1.0660 Accuracy 0.1172\n",
      "Epoch 34 Batch 2800 Loss 1.0665 Accuracy 0.1172\n",
      "Epoch 34 Batch 2850 Loss 1.0666 Accuracy 0.1172\n",
      "Epoch 34 Batch 2900 Loss 1.0667 Accuracy 0.1172\n",
      "Epoch 34 Batch 2950 Loss 1.0670 Accuracy 0.1173\n",
      "Epoch 34 Batch 3000 Loss 1.0671 Accuracy 0.1173\n",
      "Epoch 34 Loss 1.0676 Accuracy 0.1173\n",
      "Time taken for 1 epoch: 179.53 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 1.0857 Accuracy 0.1190\n",
      "Epoch 35 Batch 50 Loss 1.0554 Accuracy 0.1174\n",
      "Epoch 35 Batch 100 Loss 1.0562 Accuracy 0.1178\n",
      "Epoch 35 Batch 150 Loss 1.0557 Accuracy 0.1183\n",
      "Epoch 35 Batch 200 Loss 1.0565 Accuracy 0.1184\n",
      "Epoch 35 Batch 250 Loss 1.0542 Accuracy 0.1182\n",
      "Epoch 35 Batch 300 Loss 1.0544 Accuracy 0.1185\n",
      "Epoch 35 Batch 350 Loss 1.0576 Accuracy 0.1186\n",
      "Epoch 35 Batch 400 Loss 1.0546 Accuracy 0.1183\n",
      "Epoch 35 Batch 450 Loss 1.0554 Accuracy 0.1184\n",
      "Epoch 35 Batch 500 Loss 1.0552 Accuracy 0.1184\n",
      "Epoch 35 Batch 550 Loss 1.0570 Accuracy 0.1184\n",
      "Epoch 35 Batch 600 Loss 1.0569 Accuracy 0.1182\n",
      "Epoch 35 Batch 650 Loss 1.0573 Accuracy 0.1183\n",
      "Epoch 35 Batch 700 Loss 1.0578 Accuracy 0.1182\n",
      "Epoch 35 Batch 750 Loss 1.0589 Accuracy 0.1183\n",
      "Epoch 35 Batch 800 Loss 1.0596 Accuracy 0.1183\n",
      "Epoch 35 Batch 850 Loss 1.0613 Accuracy 0.1183\n",
      "Epoch 35 Batch 900 Loss 1.0614 Accuracy 0.1184\n",
      "Epoch 35 Batch 950 Loss 1.0629 Accuracy 0.1185\n",
      "Epoch 35 Batch 1000 Loss 1.0626 Accuracy 0.1185\n",
      "Epoch 35 Batch 1050 Loss 1.0641 Accuracy 0.1185\n",
      "Epoch 35 Batch 1100 Loss 1.0643 Accuracy 0.1184\n",
      "Epoch 35 Batch 1150 Loss 1.0649 Accuracy 0.1185\n",
      "Epoch 35 Batch 1200 Loss 1.0646 Accuracy 0.1185\n",
      "Epoch 35 Batch 1250 Loss 1.0628 Accuracy 0.1184\n",
      "Epoch 35 Batch 1300 Loss 1.0612 Accuracy 0.1183\n",
      "Epoch 35 Batch 1350 Loss 1.0613 Accuracy 0.1183\n",
      "Epoch 35 Batch 1400 Loss 1.0611 Accuracy 0.1183\n",
      "Epoch 35 Batch 1450 Loss 1.0614 Accuracy 0.1183\n",
      "Epoch 35 Batch 1500 Loss 1.0613 Accuracy 0.1183\n",
      "Epoch 35 Batch 1550 Loss 1.0618 Accuracy 0.1183\n",
      "Epoch 35 Batch 1600 Loss 1.0611 Accuracy 0.1182\n",
      "Epoch 35 Batch 1650 Loss 1.0605 Accuracy 0.1181\n",
      "Epoch 35 Batch 1700 Loss 1.0608 Accuracy 0.1181\n",
      "Epoch 35 Batch 1750 Loss 1.0605 Accuracy 0.1180\n",
      "Epoch 35 Batch 1800 Loss 1.0601 Accuracy 0.1179\n",
      "Epoch 35 Batch 1850 Loss 1.0604 Accuracy 0.1179\n",
      "Epoch 35 Batch 1900 Loss 1.0604 Accuracy 0.1178\n",
      "Epoch 35 Batch 1950 Loss 1.0601 Accuracy 0.1177\n",
      "Epoch 35 Batch 2000 Loss 1.0597 Accuracy 0.1176\n",
      "Epoch 35 Batch 2050 Loss 1.0593 Accuracy 0.1176\n",
      "Epoch 35 Batch 2100 Loss 1.0592 Accuracy 0.1175\n",
      "Epoch 35 Batch 2150 Loss 1.0591 Accuracy 0.1175\n",
      "Epoch 35 Batch 2200 Loss 1.0596 Accuracy 0.1175\n",
      "Epoch 35 Batch 2250 Loss 1.0602 Accuracy 0.1174\n",
      "Epoch 35 Batch 2300 Loss 1.0600 Accuracy 0.1174\n",
      "Epoch 35 Batch 2350 Loss 1.0610 Accuracy 0.1174\n",
      "Epoch 35 Batch 2400 Loss 1.0615 Accuracy 0.1174\n",
      "Epoch 35 Batch 2450 Loss 1.0619 Accuracy 0.1174\n",
      "Epoch 35 Batch 2500 Loss 1.0625 Accuracy 0.1175\n",
      "Epoch 35 Batch 2550 Loss 1.0629 Accuracy 0.1175\n",
      "Epoch 35 Batch 2600 Loss 1.0638 Accuracy 0.1176\n",
      "Epoch 35 Batch 2650 Loss 1.0638 Accuracy 0.1175\n",
      "Epoch 35 Batch 2700 Loss 1.0633 Accuracy 0.1175\n",
      "Epoch 35 Batch 2750 Loss 1.0636 Accuracy 0.1175\n",
      "Epoch 35 Batch 2800 Loss 1.0640 Accuracy 0.1175\n",
      "Epoch 35 Batch 2850 Loss 1.0642 Accuracy 0.1176\n",
      "Epoch 35 Batch 2900 Loss 1.0644 Accuracy 0.1176\n",
      "Epoch 35 Batch 2950 Loss 1.0645 Accuracy 0.1176\n",
      "Epoch 35 Batch 3000 Loss 1.0654 Accuracy 0.1177\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
      "Epoch 35 Loss 1.0655 Accuracy 0.1177\n",
      "Time taken for 1 epoch: 179.34 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.9861 Accuracy 0.1078\n",
      "Epoch 36 Batch 50 Loss 1.0618 Accuracy 0.1167\n",
      "Epoch 36 Batch 100 Loss 1.0747 Accuracy 0.1183\n",
      "Epoch 36 Batch 150 Loss 1.0716 Accuracy 0.1186\n",
      "Epoch 36 Batch 200 Loss 1.0694 Accuracy 0.1192\n",
      "Epoch 36 Batch 250 Loss 1.0614 Accuracy 0.1189\n",
      "Epoch 36 Batch 300 Loss 1.0619 Accuracy 0.1188\n",
      "Epoch 36 Batch 350 Loss 1.0568 Accuracy 0.1184\n",
      "Epoch 36 Batch 400 Loss 1.0562 Accuracy 0.1183\n",
      "Epoch 36 Batch 450 Loss 1.0563 Accuracy 0.1183\n",
      "Epoch 36 Batch 500 Loss 1.0574 Accuracy 0.1184\n",
      "Epoch 36 Batch 550 Loss 1.0582 Accuracy 0.1185\n",
      "Epoch 36 Batch 600 Loss 1.0583 Accuracy 0.1183\n",
      "Epoch 36 Batch 650 Loss 1.0595 Accuracy 0.1185\n",
      "Epoch 36 Batch 700 Loss 1.0599 Accuracy 0.1186\n",
      "Epoch 36 Batch 750 Loss 1.0604 Accuracy 0.1185\n",
      "Epoch 36 Batch 800 Loss 1.0610 Accuracy 0.1184\n",
      "Epoch 36 Batch 850 Loss 1.0623 Accuracy 0.1185\n",
      "Epoch 36 Batch 900 Loss 1.0618 Accuracy 0.1184\n",
      "Epoch 36 Batch 950 Loss 1.0628 Accuracy 0.1185\n",
      "Epoch 36 Batch 1000 Loss 1.0627 Accuracy 0.1184\n",
      "Epoch 36 Batch 1050 Loss 1.0631 Accuracy 0.1184\n",
      "Epoch 36 Batch 1100 Loss 1.0636 Accuracy 0.1185\n",
      "Epoch 36 Batch 1150 Loss 1.0638 Accuracy 0.1186\n",
      "Epoch 36 Batch 1200 Loss 1.0636 Accuracy 0.1186\n",
      "Epoch 36 Batch 1250 Loss 1.0625 Accuracy 0.1185\n",
      "Epoch 36 Batch 1300 Loss 1.0611 Accuracy 0.1185\n",
      "Epoch 36 Batch 1350 Loss 1.0602 Accuracy 0.1183\n",
      "Epoch 36 Batch 1400 Loss 1.0601 Accuracy 0.1183\n",
      "Epoch 36 Batch 1450 Loss 1.0593 Accuracy 0.1182\n",
      "Epoch 36 Batch 1500 Loss 1.0589 Accuracy 0.1182\n",
      "Epoch 36 Batch 1550 Loss 1.0584 Accuracy 0.1182\n",
      "Epoch 36 Batch 1600 Loss 1.0594 Accuracy 0.1183\n",
      "Epoch 36 Batch 1650 Loss 1.0592 Accuracy 0.1182\n",
      "Epoch 36 Batch 1700 Loss 1.0585 Accuracy 0.1181\n",
      "Epoch 36 Batch 1750 Loss 1.0578 Accuracy 0.1181\n",
      "Epoch 36 Batch 1800 Loss 1.0583 Accuracy 0.1181\n",
      "Epoch 36 Batch 1850 Loss 1.0583 Accuracy 0.1180\n",
      "Epoch 36 Batch 1900 Loss 1.0579 Accuracy 0.1179\n",
      "Epoch 36 Batch 1950 Loss 1.0580 Accuracy 0.1178\n",
      "Epoch 36 Batch 2000 Loss 1.0578 Accuracy 0.1178\n",
      "Epoch 36 Batch 2050 Loss 1.0580 Accuracy 0.1177\n",
      "Epoch 36 Batch 2100 Loss 1.0577 Accuracy 0.1177\n",
      "Epoch 36 Batch 2150 Loss 1.0583 Accuracy 0.1176\n",
      "Epoch 36 Batch 2200 Loss 1.0589 Accuracy 0.1176\n",
      "Epoch 36 Batch 2250 Loss 1.0593 Accuracy 0.1176\n",
      "Epoch 36 Batch 2300 Loss 1.0591 Accuracy 0.1176\n",
      "Epoch 36 Batch 2350 Loss 1.0595 Accuracy 0.1176\n",
      "Epoch 36 Batch 2400 Loss 1.0597 Accuracy 0.1176\n",
      "Epoch 36 Batch 2450 Loss 1.0597 Accuracy 0.1176\n",
      "Epoch 36 Batch 2500 Loss 1.0598 Accuracy 0.1176\n",
      "Epoch 36 Batch 2550 Loss 1.0600 Accuracy 0.1176\n",
      "Epoch 36 Batch 2600 Loss 1.0607 Accuracy 0.1176\n",
      "Epoch 36 Batch 2650 Loss 1.0609 Accuracy 0.1176\n",
      "Epoch 36 Batch 2700 Loss 1.0613 Accuracy 0.1176\n",
      "Epoch 36 Batch 2750 Loss 1.0616 Accuracy 0.1177\n",
      "Epoch 36 Batch 2800 Loss 1.0624 Accuracy 0.1177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 2850 Loss 1.0628 Accuracy 0.1178\n",
      "Epoch 36 Batch 2900 Loss 1.0633 Accuracy 0.1178\n",
      "Epoch 36 Batch 2950 Loss 1.0633 Accuracy 0.1178\n",
      "Epoch 36 Batch 3000 Loss 1.0635 Accuracy 0.1178\n",
      "Epoch 36 Loss 1.0639 Accuracy 0.1178\n",
      "Time taken for 1 epoch: 177.37 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.9889 Accuracy 0.1198\n",
      "Epoch 37 Batch 50 Loss 1.0819 Accuracy 0.1208\n",
      "Epoch 37 Batch 100 Loss 1.0738 Accuracy 0.1199\n",
      "Epoch 37 Batch 150 Loss 1.0604 Accuracy 0.1190\n",
      "Epoch 37 Batch 200 Loss 1.0545 Accuracy 0.1187\n",
      "Epoch 37 Batch 250 Loss 1.0551 Accuracy 0.1185\n",
      "Epoch 37 Batch 300 Loss 1.0533 Accuracy 0.1185\n",
      "Epoch 37 Batch 350 Loss 1.0527 Accuracy 0.1185\n",
      "Epoch 37 Batch 400 Loss 1.0535 Accuracy 0.1186\n",
      "Epoch 37 Batch 450 Loss 1.0528 Accuracy 0.1186\n",
      "Epoch 37 Batch 500 Loss 1.0535 Accuracy 0.1186\n",
      "Epoch 37 Batch 550 Loss 1.0549 Accuracy 0.1188\n",
      "Epoch 37 Batch 600 Loss 1.0558 Accuracy 0.1188\n",
      "Epoch 37 Batch 650 Loss 1.0545 Accuracy 0.1187\n",
      "Epoch 37 Batch 700 Loss 1.0544 Accuracy 0.1186\n",
      "Epoch 37 Batch 750 Loss 1.0561 Accuracy 0.1187\n",
      "Epoch 37 Batch 800 Loss 1.0560 Accuracy 0.1188\n",
      "Epoch 37 Batch 850 Loss 1.0582 Accuracy 0.1189\n",
      "Epoch 37 Batch 900 Loss 1.0593 Accuracy 0.1190\n",
      "Epoch 37 Batch 950 Loss 1.0591 Accuracy 0.1188\n",
      "Epoch 37 Batch 1000 Loss 1.0593 Accuracy 0.1188\n",
      "Epoch 37 Batch 1050 Loss 1.0598 Accuracy 0.1188\n",
      "Epoch 37 Batch 1100 Loss 1.0603 Accuracy 0.1188\n",
      "Epoch 37 Batch 1150 Loss 1.0608 Accuracy 0.1188\n",
      "Epoch 37 Batch 1200 Loss 1.0618 Accuracy 0.1189\n",
      "Epoch 37 Batch 1250 Loss 1.0606 Accuracy 0.1188\n",
      "Epoch 37 Batch 1300 Loss 1.0598 Accuracy 0.1188\n",
      "Epoch 37 Batch 1350 Loss 1.0587 Accuracy 0.1187\n",
      "Epoch 37 Batch 1400 Loss 1.0584 Accuracy 0.1187\n",
      "Epoch 37 Batch 1450 Loss 1.0575 Accuracy 0.1186\n",
      "Epoch 37 Batch 1500 Loss 1.0574 Accuracy 0.1187\n",
      "Epoch 37 Batch 1550 Loss 1.0581 Accuracy 0.1187\n",
      "Epoch 37 Batch 1600 Loss 1.0580 Accuracy 0.1187\n",
      "Epoch 37 Batch 1650 Loss 1.0568 Accuracy 0.1185\n",
      "Epoch 37 Batch 1700 Loss 1.0556 Accuracy 0.1184\n",
      "Epoch 37 Batch 1750 Loss 1.0561 Accuracy 0.1184\n",
      "Epoch 37 Batch 1800 Loss 1.0559 Accuracy 0.1183\n",
      "Epoch 37 Batch 1850 Loss 1.0565 Accuracy 0.1183\n",
      "Epoch 37 Batch 1900 Loss 1.0571 Accuracy 0.1182\n",
      "Epoch 37 Batch 1950 Loss 1.0567 Accuracy 0.1181\n",
      "Epoch 37 Batch 2000 Loss 1.0571 Accuracy 0.1181\n",
      "Epoch 37 Batch 2050 Loss 1.0564 Accuracy 0.1180\n",
      "Epoch 37 Batch 2100 Loss 1.0560 Accuracy 0.1179\n",
      "Epoch 37 Batch 2150 Loss 1.0556 Accuracy 0.1179\n",
      "Epoch 37 Batch 2200 Loss 1.0560 Accuracy 0.1178\n",
      "Epoch 37 Batch 2250 Loss 1.0571 Accuracy 0.1179\n",
      "Epoch 37 Batch 2300 Loss 1.0572 Accuracy 0.1179\n",
      "Epoch 37 Batch 2350 Loss 1.0572 Accuracy 0.1178\n",
      "Epoch 37 Batch 2400 Loss 1.0574 Accuracy 0.1178\n",
      "Epoch 37 Batch 2450 Loss 1.0579 Accuracy 0.1178\n",
      "Epoch 37 Batch 2500 Loss 1.0586 Accuracy 0.1179\n",
      "Epoch 37 Batch 2550 Loss 1.0591 Accuracy 0.1179\n",
      "Epoch 37 Batch 2600 Loss 1.0595 Accuracy 0.1179\n",
      "Epoch 37 Batch 2650 Loss 1.0596 Accuracy 0.1179\n",
      "Epoch 37 Batch 2700 Loss 1.0602 Accuracy 0.1180\n",
      "Epoch 37 Batch 2750 Loss 1.0607 Accuracy 0.1180\n",
      "Epoch 37 Batch 2800 Loss 1.0611 Accuracy 0.1180\n",
      "Epoch 37 Batch 2850 Loss 1.0611 Accuracy 0.1180\n",
      "Epoch 37 Batch 2900 Loss 1.0612 Accuracy 0.1180\n",
      "Epoch 37 Batch 2950 Loss 1.0614 Accuracy 0.1181\n",
      "Epoch 37 Batch 3000 Loss 1.0618 Accuracy 0.1181\n",
      "Epoch 37 Loss 1.0619 Accuracy 0.1181\n",
      "Time taken for 1 epoch: 177.52 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.9911 Accuracy 0.1098\n",
      "Epoch 38 Batch 50 Loss 1.0680 Accuracy 0.1192\n",
      "Epoch 38 Batch 100 Loss 1.0602 Accuracy 0.1185\n",
      "Epoch 38 Batch 150 Loss 1.0559 Accuracy 0.1188\n",
      "Epoch 38 Batch 200 Loss 1.0558 Accuracy 0.1189\n",
      "Epoch 38 Batch 250 Loss 1.0487 Accuracy 0.1184\n",
      "Epoch 38 Batch 300 Loss 1.0484 Accuracy 0.1182\n",
      "Epoch 38 Batch 350 Loss 1.0464 Accuracy 0.1182\n",
      "Epoch 38 Batch 400 Loss 1.0490 Accuracy 0.1184\n",
      "Epoch 38 Batch 450 Loss 1.0488 Accuracy 0.1185\n",
      "Epoch 38 Batch 500 Loss 1.0512 Accuracy 0.1185\n",
      "Epoch 38 Batch 550 Loss 1.0540 Accuracy 0.1185\n",
      "Epoch 38 Batch 600 Loss 1.0552 Accuracy 0.1185\n",
      "Epoch 38 Batch 650 Loss 1.0547 Accuracy 0.1184\n",
      "Epoch 38 Batch 700 Loss 1.0568 Accuracy 0.1186\n",
      "Epoch 38 Batch 750 Loss 1.0582 Accuracy 0.1187\n",
      "Epoch 38 Batch 800 Loss 1.0606 Accuracy 0.1189\n",
      "Epoch 38 Batch 850 Loss 1.0606 Accuracy 0.1188\n",
      "Epoch 38 Batch 900 Loss 1.0591 Accuracy 0.1189\n",
      "Epoch 38 Batch 950 Loss 1.0594 Accuracy 0.1189\n",
      "Epoch 38 Batch 1000 Loss 1.0586 Accuracy 0.1189\n",
      "Epoch 38 Batch 1050 Loss 1.0585 Accuracy 0.1189\n",
      "Epoch 38 Batch 1100 Loss 1.0581 Accuracy 0.1189\n",
      "Epoch 38 Batch 1150 Loss 1.0587 Accuracy 0.1189\n",
      "Epoch 38 Batch 1200 Loss 1.0585 Accuracy 0.1189\n",
      "Epoch 38 Batch 1250 Loss 1.0579 Accuracy 0.1189\n",
      "Epoch 38 Batch 1300 Loss 1.0561 Accuracy 0.1188\n",
      "Epoch 38 Batch 1350 Loss 1.0568 Accuracy 0.1189\n",
      "Epoch 38 Batch 1400 Loss 1.0566 Accuracy 0.1190\n",
      "Epoch 38 Batch 1450 Loss 1.0555 Accuracy 0.1189\n",
      "Epoch 38 Batch 1500 Loss 1.0562 Accuracy 0.1189\n",
      "Epoch 38 Batch 1550 Loss 1.0555 Accuracy 0.1188\n",
      "Epoch 38 Batch 1600 Loss 1.0553 Accuracy 0.1188\n",
      "Epoch 38 Batch 1650 Loss 1.0547 Accuracy 0.1187\n",
      "Epoch 38 Batch 1700 Loss 1.0541 Accuracy 0.1186\n",
      "Epoch 38 Batch 1750 Loss 1.0544 Accuracy 0.1186\n",
      "Epoch 38 Batch 1800 Loss 1.0534 Accuracy 0.1185\n",
      "Epoch 38 Batch 1850 Loss 1.0537 Accuracy 0.1184\n",
      "Epoch 38 Batch 1900 Loss 1.0535 Accuracy 0.1184\n",
      "Epoch 38 Batch 1950 Loss 1.0537 Accuracy 0.1183\n",
      "Epoch 38 Batch 2000 Loss 1.0541 Accuracy 0.1182\n",
      "Epoch 38 Batch 2050 Loss 1.0542 Accuracy 0.1182\n",
      "Epoch 38 Batch 2100 Loss 1.0543 Accuracy 0.1181\n",
      "Epoch 38 Batch 2150 Loss 1.0542 Accuracy 0.1181\n",
      "Epoch 38 Batch 2200 Loss 1.0546 Accuracy 0.1180\n",
      "Epoch 38 Batch 2250 Loss 1.0551 Accuracy 0.1180\n",
      "Epoch 38 Batch 2300 Loss 1.0554 Accuracy 0.1180\n",
      "Epoch 38 Batch 2350 Loss 1.0556 Accuracy 0.1180\n",
      "Epoch 38 Batch 2400 Loss 1.0561 Accuracy 0.1180\n",
      "Epoch 38 Batch 2450 Loss 1.0564 Accuracy 0.1180\n",
      "Epoch 38 Batch 2500 Loss 1.0565 Accuracy 0.1180\n",
      "Epoch 38 Batch 2550 Loss 1.0570 Accuracy 0.1180\n",
      "Epoch 38 Batch 2600 Loss 1.0577 Accuracy 0.1181\n",
      "Epoch 38 Batch 2650 Loss 1.0580 Accuracy 0.1181\n",
      "Epoch 38 Batch 2700 Loss 1.0581 Accuracy 0.1181\n",
      "Epoch 38 Batch 2750 Loss 1.0579 Accuracy 0.1181\n",
      "Epoch 38 Batch 2800 Loss 1.0586 Accuracy 0.1182\n",
      "Epoch 38 Batch 2850 Loss 1.0590 Accuracy 0.1182\n",
      "Epoch 38 Batch 2900 Loss 1.0591 Accuracy 0.1182\n",
      "Epoch 38 Batch 2950 Loss 1.0595 Accuracy 0.1182\n",
      "Epoch 38 Batch 3000 Loss 1.0597 Accuracy 0.1183\n",
      "Epoch 38 Loss 1.0602 Accuracy 0.1183\n",
      "Time taken for 1 epoch: 177.06 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 1.1796 Accuracy 0.1238\n",
      "Epoch 39 Batch 50 Loss 1.0717 Accuracy 0.1197\n",
      "Epoch 39 Batch 100 Loss 1.0628 Accuracy 0.1199\n",
      "Epoch 39 Batch 150 Loss 1.0530 Accuracy 0.1198\n",
      "Epoch 39 Batch 200 Loss 1.0452 Accuracy 0.1190\n",
      "Epoch 39 Batch 250 Loss 1.0457 Accuracy 0.1191\n",
      "Epoch 39 Batch 300 Loss 1.0433 Accuracy 0.1189\n",
      "Epoch 39 Batch 350 Loss 1.0448 Accuracy 0.1188\n",
      "Epoch 39 Batch 400 Loss 1.0503 Accuracy 0.1193\n",
      "Epoch 39 Batch 450 Loss 1.0475 Accuracy 0.1190\n",
      "Epoch 39 Batch 500 Loss 1.0469 Accuracy 0.1188\n",
      "Epoch 39 Batch 550 Loss 1.0484 Accuracy 0.1189\n",
      "Epoch 39 Batch 600 Loss 1.0493 Accuracy 0.1190\n",
      "Epoch 39 Batch 650 Loss 1.0492 Accuracy 0.1191\n",
      "Epoch 39 Batch 700 Loss 1.0502 Accuracy 0.1192\n",
      "Epoch 39 Batch 750 Loss 1.0509 Accuracy 0.1191\n",
      "Epoch 39 Batch 800 Loss 1.0535 Accuracy 0.1192\n",
      "Epoch 39 Batch 850 Loss 1.0545 Accuracy 0.1193\n",
      "Epoch 39 Batch 900 Loss 1.0566 Accuracy 0.1194\n",
      "Epoch 39 Batch 950 Loss 1.0586 Accuracy 0.1193\n",
      "Epoch 39 Batch 1000 Loss 1.0592 Accuracy 0.1194\n",
      "Epoch 39 Batch 1050 Loss 1.0583 Accuracy 0.1193\n",
      "Epoch 39 Batch 1100 Loss 1.0578 Accuracy 0.1192\n",
      "Epoch 39 Batch 1150 Loss 1.0574 Accuracy 0.1192\n",
      "Epoch 39 Batch 1200 Loss 1.0568 Accuracy 0.1192\n",
      "Epoch 39 Batch 1250 Loss 1.0555 Accuracy 0.1191\n",
      "Epoch 39 Batch 1300 Loss 1.0554 Accuracy 0.1192\n",
      "Epoch 39 Batch 1350 Loss 1.0553 Accuracy 0.1192\n",
      "Epoch 39 Batch 1400 Loss 1.0545 Accuracy 0.1191\n",
      "Epoch 39 Batch 1450 Loss 1.0549 Accuracy 0.1192\n",
      "Epoch 39 Batch 1500 Loss 1.0543 Accuracy 0.1191\n",
      "Epoch 39 Batch 1550 Loss 1.0538 Accuracy 0.1190\n",
      "Epoch 39 Batch 1600 Loss 1.0534 Accuracy 0.1190\n",
      "Epoch 39 Batch 1650 Loss 1.0526 Accuracy 0.1189\n",
      "Epoch 39 Batch 1700 Loss 1.0522 Accuracy 0.1188\n",
      "Epoch 39 Batch 1750 Loss 1.0524 Accuracy 0.1188\n",
      "Epoch 39 Batch 1800 Loss 1.0535 Accuracy 0.1188\n",
      "Epoch 39 Batch 1850 Loss 1.0538 Accuracy 0.1188\n",
      "Epoch 39 Batch 1900 Loss 1.0546 Accuracy 0.1187\n",
      "Epoch 39 Batch 1950 Loss 1.0546 Accuracy 0.1186\n",
      "Epoch 39 Batch 2000 Loss 1.0543 Accuracy 0.1185\n",
      "Epoch 39 Batch 2050 Loss 1.0536 Accuracy 0.1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 2100 Loss 1.0532 Accuracy 0.1184\n",
      "Epoch 39 Batch 2150 Loss 1.0537 Accuracy 0.1184\n",
      "Epoch 39 Batch 2200 Loss 1.0544 Accuracy 0.1184\n",
      "Epoch 39 Batch 2250 Loss 1.0542 Accuracy 0.1184\n",
      "Epoch 39 Batch 2300 Loss 1.0542 Accuracy 0.1184\n",
      "Epoch 39 Batch 2350 Loss 1.0548 Accuracy 0.1184\n",
      "Epoch 39 Batch 2400 Loss 1.0553 Accuracy 0.1184\n",
      "Epoch 39 Batch 2450 Loss 1.0557 Accuracy 0.1184\n",
      "Epoch 39 Batch 2500 Loss 1.0564 Accuracy 0.1184\n",
      "Epoch 39 Batch 2550 Loss 1.0564 Accuracy 0.1184\n",
      "Epoch 39 Batch 2600 Loss 1.0566 Accuracy 0.1184\n",
      "Epoch 39 Batch 2650 Loss 1.0570 Accuracy 0.1184\n",
      "Epoch 39 Batch 2700 Loss 1.0571 Accuracy 0.1184\n",
      "Epoch 39 Batch 2750 Loss 1.0573 Accuracy 0.1184\n",
      "Epoch 39 Batch 2800 Loss 1.0578 Accuracy 0.1184\n",
      "Epoch 39 Batch 2850 Loss 1.0584 Accuracy 0.1185\n",
      "Epoch 39 Batch 2900 Loss 1.0584 Accuracy 0.1184\n",
      "Epoch 39 Batch 2950 Loss 1.0589 Accuracy 0.1185\n",
      "Epoch 39 Batch 3000 Loss 1.0589 Accuracy 0.1185\n",
      "Epoch 39 Loss 1.0589 Accuracy 0.1185\n",
      "Time taken for 1 epoch: 177.30 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.9998 Accuracy 0.1154\n",
      "Epoch 40 Batch 50 Loss 1.0655 Accuracy 0.1201\n",
      "Epoch 40 Batch 100 Loss 1.0595 Accuracy 0.1195\n",
      "Epoch 40 Batch 150 Loss 1.0577 Accuracy 0.1192\n",
      "Epoch 40 Batch 200 Loss 1.0579 Accuracy 0.1192\n",
      "Epoch 40 Batch 250 Loss 1.0555 Accuracy 0.1193\n",
      "Epoch 40 Batch 300 Loss 1.0539 Accuracy 0.1192\n",
      "Epoch 40 Batch 350 Loss 1.0521 Accuracy 0.1193\n",
      "Epoch 40 Batch 400 Loss 1.0483 Accuracy 0.1190\n",
      "Epoch 40 Batch 450 Loss 1.0466 Accuracy 0.1188\n",
      "Epoch 40 Batch 500 Loss 1.0466 Accuracy 0.1190\n",
      "Epoch 40 Batch 550 Loss 1.0482 Accuracy 0.1191\n",
      "Epoch 40 Batch 600 Loss 1.0488 Accuracy 0.1192\n",
      "Epoch 40 Batch 650 Loss 1.0500 Accuracy 0.1192\n",
      "Epoch 40 Batch 700 Loss 1.0507 Accuracy 0.1193\n",
      "Epoch 40 Batch 750 Loss 1.0515 Accuracy 0.1193\n",
      "Epoch 40 Batch 800 Loss 1.0523 Accuracy 0.1193\n",
      "Epoch 40 Batch 850 Loss 1.0522 Accuracy 0.1193\n",
      "Epoch 40 Batch 900 Loss 1.0521 Accuracy 0.1193\n",
      "Epoch 40 Batch 950 Loss 1.0540 Accuracy 0.1194\n",
      "Epoch 40 Batch 1000 Loss 1.0542 Accuracy 0.1194\n",
      "Epoch 40 Batch 1050 Loss 1.0545 Accuracy 0.1193\n",
      "Epoch 40 Batch 1100 Loss 1.0559 Accuracy 0.1194\n",
      "Epoch 40 Batch 1150 Loss 1.0560 Accuracy 0.1195\n",
      "Epoch 40 Batch 1200 Loss 1.0549 Accuracy 0.1195\n",
      "Epoch 40 Batch 1250 Loss 1.0537 Accuracy 0.1194\n",
      "Epoch 40 Batch 1300 Loss 1.0525 Accuracy 0.1193\n",
      "Epoch 40 Batch 1350 Loss 1.0517 Accuracy 0.1193\n",
      "Epoch 40 Batch 1400 Loss 1.0512 Accuracy 0.1192\n",
      "Epoch 40 Batch 1450 Loss 1.0520 Accuracy 0.1193\n",
      "Epoch 40 Batch 1500 Loss 1.0514 Accuracy 0.1192\n",
      "Epoch 40 Batch 1550 Loss 1.0521 Accuracy 0.1192\n",
      "Epoch 40 Batch 1600 Loss 1.0515 Accuracy 0.1191\n",
      "Epoch 40 Batch 1650 Loss 1.0502 Accuracy 0.1190\n",
      "Epoch 40 Batch 1700 Loss 1.0501 Accuracy 0.1188\n",
      "Epoch 40 Batch 1750 Loss 1.0511 Accuracy 0.1189\n",
      "Epoch 40 Batch 1800 Loss 1.0513 Accuracy 0.1188\n",
      "Epoch 40 Batch 1850 Loss 1.0512 Accuracy 0.1187\n",
      "Epoch 40 Batch 1900 Loss 1.0515 Accuracy 0.1187\n",
      "Epoch 40 Batch 1950 Loss 1.0518 Accuracy 0.1186\n",
      "Epoch 40 Batch 2000 Loss 1.0511 Accuracy 0.1185\n",
      "Epoch 40 Batch 2050 Loss 1.0509 Accuracy 0.1185\n",
      "Epoch 40 Batch 2100 Loss 1.0506 Accuracy 0.1184\n",
      "Epoch 40 Batch 2150 Loss 1.0507 Accuracy 0.1184\n",
      "Epoch 40 Batch 2200 Loss 1.0512 Accuracy 0.1184\n",
      "Epoch 40 Batch 2250 Loss 1.0517 Accuracy 0.1184\n",
      "Epoch 40 Batch 2300 Loss 1.0518 Accuracy 0.1183\n",
      "Epoch 40 Batch 2350 Loss 1.0522 Accuracy 0.1183\n",
      "Epoch 40 Batch 2400 Loss 1.0527 Accuracy 0.1183\n",
      "Epoch 40 Batch 2450 Loss 1.0532 Accuracy 0.1184\n",
      "Epoch 40 Batch 2500 Loss 1.0536 Accuracy 0.1184\n",
      "Epoch 40 Batch 2550 Loss 1.0540 Accuracy 0.1184\n",
      "Epoch 40 Batch 2600 Loss 1.0545 Accuracy 0.1185\n",
      "Epoch 40 Batch 2650 Loss 1.0548 Accuracy 0.1185\n",
      "Epoch 40 Batch 2700 Loss 1.0554 Accuracy 0.1185\n",
      "Epoch 40 Batch 2750 Loss 1.0559 Accuracy 0.1186\n",
      "Epoch 40 Batch 2800 Loss 1.0562 Accuracy 0.1186\n",
      "Epoch 40 Batch 2850 Loss 1.0567 Accuracy 0.1186\n",
      "Epoch 40 Batch 2900 Loss 1.0569 Accuracy 0.1186\n",
      "Epoch 40 Batch 2950 Loss 1.0568 Accuracy 0.1186\n",
      "Epoch 40 Batch 3000 Loss 1.0572 Accuracy 0.1186\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
      "Epoch 40 Loss 1.0572 Accuracy 0.1186\n",
      "Time taken for 1 epoch: 179.99 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 1.1118 Accuracy 0.1302\n",
      "Epoch 41 Batch 50 Loss 1.0771 Accuracy 0.1192\n",
      "Epoch 41 Batch 100 Loss 1.0637 Accuracy 0.1191\n",
      "Epoch 41 Batch 150 Loss 1.0624 Accuracy 0.1193\n",
      "Epoch 41 Batch 200 Loss 1.0580 Accuracy 0.1195\n",
      "Epoch 41 Batch 250 Loss 1.0541 Accuracy 0.1195\n",
      "Epoch 41 Batch 300 Loss 1.0481 Accuracy 0.1193\n",
      "Epoch 41 Batch 350 Loss 1.0473 Accuracy 0.1192\n",
      "Epoch 41 Batch 400 Loss 1.0503 Accuracy 0.1194\n",
      "Epoch 41 Batch 450 Loss 1.0504 Accuracy 0.1193\n",
      "Epoch 41 Batch 500 Loss 1.0497 Accuracy 0.1193\n",
      "Epoch 41 Batch 550 Loss 1.0497 Accuracy 0.1194\n",
      "Epoch 41 Batch 600 Loss 1.0508 Accuracy 0.1194\n",
      "Epoch 41 Batch 650 Loss 1.0506 Accuracy 0.1195\n",
      "Epoch 41 Batch 700 Loss 1.0513 Accuracy 0.1194\n",
      "Epoch 41 Batch 750 Loss 1.0517 Accuracy 0.1194\n",
      "Epoch 41 Batch 800 Loss 1.0512 Accuracy 0.1193\n",
      "Epoch 41 Batch 850 Loss 1.0512 Accuracy 0.1193\n",
      "Epoch 41 Batch 900 Loss 1.0519 Accuracy 0.1193\n",
      "Epoch 41 Batch 950 Loss 1.0537 Accuracy 0.1194\n",
      "Epoch 41 Batch 1000 Loss 1.0539 Accuracy 0.1195\n",
      "Epoch 41 Batch 1050 Loss 1.0549 Accuracy 0.1196\n",
      "Epoch 41 Batch 1100 Loss 1.0536 Accuracy 0.1196\n",
      "Epoch 41 Batch 1150 Loss 1.0530 Accuracy 0.1196\n",
      "Epoch 41 Batch 1200 Loss 1.0531 Accuracy 0.1196\n",
      "Epoch 41 Batch 1250 Loss 1.0527 Accuracy 0.1196\n",
      "Epoch 41 Batch 1300 Loss 1.0525 Accuracy 0.1196\n",
      "Epoch 41 Batch 1350 Loss 1.0519 Accuracy 0.1196\n",
      "Epoch 41 Batch 1400 Loss 1.0515 Accuracy 0.1195\n",
      "Epoch 41 Batch 1450 Loss 1.0511 Accuracy 0.1195\n",
      "Epoch 41 Batch 1500 Loss 1.0513 Accuracy 0.1195\n",
      "Epoch 41 Batch 1550 Loss 1.0510 Accuracy 0.1194\n",
      "Epoch 41 Batch 1600 Loss 1.0511 Accuracy 0.1194\n",
      "Epoch 41 Batch 1650 Loss 1.0505 Accuracy 0.1193\n",
      "Epoch 41 Batch 1700 Loss 1.0502 Accuracy 0.1192\n",
      "Epoch 41 Batch 1750 Loss 1.0505 Accuracy 0.1193\n",
      "Epoch 41 Batch 1800 Loss 1.0498 Accuracy 0.1191\n",
      "Epoch 41 Batch 1850 Loss 1.0503 Accuracy 0.1191\n",
      "Epoch 41 Batch 1900 Loss 1.0508 Accuracy 0.1191\n",
      "Epoch 41 Batch 1950 Loss 1.0514 Accuracy 0.1190\n",
      "Epoch 41 Batch 2000 Loss 1.0509 Accuracy 0.1189\n",
      "Epoch 41 Batch 2050 Loss 1.0508 Accuracy 0.1188\n",
      "Epoch 41 Batch 2100 Loss 1.0504 Accuracy 0.1188\n",
      "Epoch 41 Batch 2150 Loss 1.0508 Accuracy 0.1188\n",
      "Epoch 41 Batch 2200 Loss 1.0515 Accuracy 0.1188\n",
      "Epoch 41 Batch 2250 Loss 1.0520 Accuracy 0.1188\n",
      "Epoch 41 Batch 2300 Loss 1.0519 Accuracy 0.1187\n",
      "Epoch 41 Batch 2350 Loss 1.0521 Accuracy 0.1187\n",
      "Epoch 41 Batch 2400 Loss 1.0526 Accuracy 0.1188\n",
      "Epoch 41 Batch 2450 Loss 1.0528 Accuracy 0.1188\n",
      "Epoch 41 Batch 2500 Loss 1.0529 Accuracy 0.1187\n",
      "Epoch 41 Batch 2550 Loss 1.0536 Accuracy 0.1187\n",
      "Epoch 41 Batch 2600 Loss 1.0543 Accuracy 0.1188\n",
      "Epoch 41 Batch 2650 Loss 1.0542 Accuracy 0.1188\n",
      "Epoch 41 Batch 2700 Loss 1.0544 Accuracy 0.1188\n",
      "Epoch 41 Batch 2750 Loss 1.0546 Accuracy 0.1189\n",
      "Epoch 41 Batch 2800 Loss 1.0543 Accuracy 0.1189\n",
      "Epoch 41 Batch 2850 Loss 1.0545 Accuracy 0.1189\n",
      "Epoch 41 Batch 2900 Loss 1.0548 Accuracy 0.1189\n",
      "Epoch 41 Batch 2950 Loss 1.0550 Accuracy 0.1189\n",
      "Epoch 41 Batch 3000 Loss 1.0553 Accuracy 0.1189\n",
      "Epoch 41 Loss 1.0556 Accuracy 0.1189\n",
      "Time taken for 1 epoch: 180.30 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 1.2183 Accuracy 0.1254\n",
      "Epoch 42 Batch 50 Loss 1.0869 Accuracy 0.1207\n",
      "Epoch 42 Batch 100 Loss 1.0601 Accuracy 0.1198\n",
      "Epoch 42 Batch 150 Loss 1.0597 Accuracy 0.1201\n",
      "Epoch 42 Batch 200 Loss 1.0498 Accuracy 0.1196\n",
      "Epoch 42 Batch 250 Loss 1.0476 Accuracy 0.1196\n",
      "Epoch 42 Batch 300 Loss 1.0491 Accuracy 0.1200\n",
      "Epoch 42 Batch 350 Loss 1.0473 Accuracy 0.1197\n",
      "Epoch 42 Batch 400 Loss 1.0464 Accuracy 0.1196\n",
      "Epoch 42 Batch 450 Loss 1.0449 Accuracy 0.1194\n",
      "Epoch 42 Batch 500 Loss 1.0467 Accuracy 0.1192\n",
      "Epoch 42 Batch 550 Loss 1.0460 Accuracy 0.1192\n",
      "Epoch 42 Batch 600 Loss 1.0478 Accuracy 0.1193\n",
      "Epoch 42 Batch 650 Loss 1.0482 Accuracy 0.1192\n",
      "Epoch 42 Batch 700 Loss 1.0502 Accuracy 0.1195\n",
      "Epoch 42 Batch 750 Loss 1.0515 Accuracy 0.1195\n",
      "Epoch 42 Batch 800 Loss 1.0525 Accuracy 0.1196\n",
      "Epoch 42 Batch 850 Loss 1.0541 Accuracy 0.1197\n",
      "Epoch 42 Batch 900 Loss 1.0536 Accuracy 0.1198\n",
      "Epoch 42 Batch 950 Loss 1.0535 Accuracy 0.1197\n",
      "Epoch 42 Batch 1000 Loss 1.0533 Accuracy 0.1197\n",
      "Epoch 42 Batch 1050 Loss 1.0531 Accuracy 0.1197\n",
      "Epoch 42 Batch 1100 Loss 1.0540 Accuracy 0.1196\n",
      "Epoch 42 Batch 1150 Loss 1.0543 Accuracy 0.1196\n",
      "Epoch 42 Batch 1200 Loss 1.0534 Accuracy 0.1197\n",
      "Epoch 42 Batch 1250 Loss 1.0523 Accuracy 0.1197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 1300 Loss 1.0513 Accuracy 0.1197\n",
      "Epoch 42 Batch 1350 Loss 1.0497 Accuracy 0.1195\n",
      "Epoch 42 Batch 1400 Loss 1.0488 Accuracy 0.1195\n",
      "Epoch 42 Batch 1450 Loss 1.0492 Accuracy 0.1195\n",
      "Epoch 42 Batch 1500 Loss 1.0496 Accuracy 0.1196\n",
      "Epoch 42 Batch 1550 Loss 1.0488 Accuracy 0.1195\n",
      "Epoch 42 Batch 1600 Loss 1.0481 Accuracy 0.1194\n",
      "Epoch 42 Batch 1650 Loss 1.0478 Accuracy 0.1194\n",
      "Epoch 42 Batch 1700 Loss 1.0477 Accuracy 0.1193\n",
      "Epoch 42 Batch 1750 Loss 1.0481 Accuracy 0.1193\n",
      "Epoch 42 Batch 1800 Loss 1.0472 Accuracy 0.1192\n",
      "Epoch 42 Batch 1850 Loss 1.0472 Accuracy 0.1191\n",
      "Epoch 42 Batch 1900 Loss 1.0474 Accuracy 0.1191\n",
      "Epoch 42 Batch 1950 Loss 1.0479 Accuracy 0.1191\n",
      "Epoch 42 Batch 2000 Loss 1.0476 Accuracy 0.1190\n",
      "Epoch 42 Batch 2050 Loss 1.0479 Accuracy 0.1189\n",
      "Epoch 42 Batch 2100 Loss 1.0476 Accuracy 0.1189\n",
      "Epoch 42 Batch 2150 Loss 1.0485 Accuracy 0.1189\n",
      "Epoch 42 Batch 2200 Loss 1.0494 Accuracy 0.1189\n",
      "Epoch 42 Batch 2250 Loss 1.0492 Accuracy 0.1188\n",
      "Epoch 42 Batch 2300 Loss 1.0494 Accuracy 0.1189\n",
      "Epoch 42 Batch 2350 Loss 1.0502 Accuracy 0.1190\n",
      "Epoch 42 Batch 2400 Loss 1.0505 Accuracy 0.1189\n",
      "Epoch 42 Batch 2450 Loss 1.0512 Accuracy 0.1189\n",
      "Epoch 42 Batch 2500 Loss 1.0512 Accuracy 0.1189\n",
      "Epoch 42 Batch 2550 Loss 1.0513 Accuracy 0.1189\n",
      "Epoch 42 Batch 2600 Loss 1.0520 Accuracy 0.1189\n",
      "Epoch 42 Batch 2650 Loss 1.0521 Accuracy 0.1190\n",
      "Epoch 42 Batch 2700 Loss 1.0518 Accuracy 0.1189\n",
      "Epoch 42 Batch 2750 Loss 1.0521 Accuracy 0.1190\n",
      "Epoch 42 Batch 2800 Loss 1.0526 Accuracy 0.1190\n",
      "Epoch 42 Batch 2850 Loss 1.0533 Accuracy 0.1191\n",
      "Epoch 42 Batch 2900 Loss 1.0534 Accuracy 0.1191\n",
      "Epoch 42 Batch 2950 Loss 1.0534 Accuracy 0.1191\n",
      "Epoch 42 Batch 3000 Loss 1.0537 Accuracy 0.1191\n",
      "Epoch 42 Loss 1.0539 Accuracy 0.1191\n",
      "Time taken for 1 epoch: 179.87 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 1.0578 Accuracy 0.1322\n",
      "Epoch 43 Batch 50 Loss 1.0689 Accuracy 0.1185\n",
      "Epoch 43 Batch 100 Loss 1.0571 Accuracy 0.1201\n",
      "Epoch 43 Batch 150 Loss 1.0479 Accuracy 0.1198\n",
      "Epoch 43 Batch 200 Loss 1.0416 Accuracy 0.1195\n",
      "Epoch 43 Batch 250 Loss 1.0399 Accuracy 0.1196\n",
      "Epoch 43 Batch 300 Loss 1.0389 Accuracy 0.1193\n",
      "Epoch 43 Batch 350 Loss 1.0421 Accuracy 0.1197\n",
      "Epoch 43 Batch 400 Loss 1.0383 Accuracy 0.1192\n",
      "Epoch 43 Batch 450 Loss 1.0390 Accuracy 0.1193\n",
      "Epoch 43 Batch 500 Loss 1.0401 Accuracy 0.1195\n",
      "Epoch 43 Batch 550 Loss 1.0423 Accuracy 0.1195\n",
      "Epoch 43 Batch 600 Loss 1.0428 Accuracy 0.1194\n",
      "Epoch 43 Batch 650 Loss 1.0453 Accuracy 0.1197\n",
      "Epoch 43 Batch 700 Loss 1.0475 Accuracy 0.1198\n",
      "Epoch 43 Batch 750 Loss 1.0486 Accuracy 0.1199\n",
      "Epoch 43 Batch 800 Loss 1.0508 Accuracy 0.1199\n",
      "Epoch 43 Batch 850 Loss 1.0502 Accuracy 0.1199\n",
      "Epoch 43 Batch 900 Loss 1.0510 Accuracy 0.1201\n",
      "Epoch 43 Batch 950 Loss 1.0518 Accuracy 0.1201\n",
      "Epoch 43 Batch 1000 Loss 1.0516 Accuracy 0.1201\n",
      "Epoch 43 Batch 1050 Loss 1.0520 Accuracy 0.1200\n",
      "Epoch 43 Batch 1100 Loss 1.0516 Accuracy 0.1199\n",
      "Epoch 43 Batch 1150 Loss 1.0518 Accuracy 0.1200\n",
      "Epoch 43 Batch 1200 Loss 1.0512 Accuracy 0.1199\n",
      "Epoch 43 Batch 1250 Loss 1.0502 Accuracy 0.1199\n",
      "Epoch 43 Batch 1300 Loss 1.0504 Accuracy 0.1199\n",
      "Epoch 43 Batch 1350 Loss 1.0492 Accuracy 0.1198\n",
      "Epoch 43 Batch 1400 Loss 1.0482 Accuracy 0.1198\n",
      "Epoch 43 Batch 1450 Loss 1.0486 Accuracy 0.1198\n",
      "Epoch 43 Batch 1500 Loss 1.0492 Accuracy 0.1199\n",
      "Epoch 43 Batch 1550 Loss 1.0485 Accuracy 0.1198\n",
      "Epoch 43 Batch 1600 Loss 1.0478 Accuracy 0.1198\n",
      "Epoch 43 Batch 1650 Loss 1.0468 Accuracy 0.1196\n",
      "Epoch 43 Batch 1700 Loss 1.0465 Accuracy 0.1196\n",
      "Epoch 43 Batch 1750 Loss 1.0465 Accuracy 0.1196\n",
      "Epoch 43 Batch 1800 Loss 1.0461 Accuracy 0.1195\n",
      "Epoch 43 Batch 1850 Loss 1.0465 Accuracy 0.1194\n",
      "Epoch 43 Batch 1900 Loss 1.0472 Accuracy 0.1194\n",
      "Epoch 43 Batch 1950 Loss 1.0462 Accuracy 0.1192\n",
      "Epoch 43 Batch 2000 Loss 1.0462 Accuracy 0.1192\n",
      "Epoch 43 Batch 2050 Loss 1.0460 Accuracy 0.1191\n",
      "Epoch 43 Batch 2100 Loss 1.0459 Accuracy 0.1191\n",
      "Epoch 43 Batch 2150 Loss 1.0469 Accuracy 0.1191\n",
      "Epoch 43 Batch 2200 Loss 1.0474 Accuracy 0.1191\n",
      "Epoch 43 Batch 2250 Loss 1.0473 Accuracy 0.1191\n",
      "Epoch 43 Batch 2300 Loss 1.0477 Accuracy 0.1191\n",
      "Epoch 43 Batch 2350 Loss 1.0480 Accuracy 0.1191\n",
      "Epoch 43 Batch 2400 Loss 1.0482 Accuracy 0.1191\n",
      "Epoch 43 Batch 2450 Loss 1.0485 Accuracy 0.1191\n",
      "Epoch 43 Batch 2500 Loss 1.0486 Accuracy 0.1191\n",
      "Epoch 43 Batch 2550 Loss 1.0490 Accuracy 0.1191\n",
      "Epoch 43 Batch 2600 Loss 1.0493 Accuracy 0.1191\n",
      "Epoch 43 Batch 2650 Loss 1.0499 Accuracy 0.1191\n",
      "Epoch 43 Batch 2700 Loss 1.0502 Accuracy 0.1191\n",
      "Epoch 43 Batch 2750 Loss 1.0508 Accuracy 0.1192\n",
      "Epoch 43 Batch 2800 Loss 1.0512 Accuracy 0.1192\n",
      "Epoch 43 Batch 2850 Loss 1.0517 Accuracy 0.1193\n",
      "Epoch 43 Batch 2900 Loss 1.0521 Accuracy 0.1193\n",
      "Epoch 43 Batch 2950 Loss 1.0522 Accuracy 0.1193\n",
      "Epoch 43 Batch 3000 Loss 1.0524 Accuracy 0.1193\n",
      "Epoch 43 Loss 1.0525 Accuracy 0.1193\n",
      "Time taken for 1 epoch: 182.02 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.9817 Accuracy 0.1202\n",
      "Epoch 44 Batch 50 Loss 1.0404 Accuracy 0.1191\n",
      "Epoch 44 Batch 100 Loss 1.0314 Accuracy 0.1187\n",
      "Epoch 44 Batch 150 Loss 1.0417 Accuracy 0.1200\n",
      "Epoch 44 Batch 200 Loss 1.0376 Accuracy 0.1193\n",
      "Epoch 44 Batch 250 Loss 1.0357 Accuracy 0.1195\n",
      "Epoch 44 Batch 300 Loss 1.0397 Accuracy 0.1199\n",
      "Epoch 44 Batch 350 Loss 1.0357 Accuracy 0.1196\n",
      "Epoch 44 Batch 400 Loss 1.0394 Accuracy 0.1197\n",
      "Epoch 44 Batch 450 Loss 1.0391 Accuracy 0.1196\n",
      "Epoch 44 Batch 500 Loss 1.0415 Accuracy 0.1198\n",
      "Epoch 44 Batch 550 Loss 1.0435 Accuracy 0.1200\n",
      "Epoch 44 Batch 600 Loss 1.0448 Accuracy 0.1200\n",
      "Epoch 44 Batch 650 Loss 1.0448 Accuracy 0.1200\n",
      "Epoch 44 Batch 700 Loss 1.0470 Accuracy 0.1200\n",
      "Epoch 44 Batch 750 Loss 1.0473 Accuracy 0.1200\n",
      "Epoch 44 Batch 800 Loss 1.0466 Accuracy 0.1200\n",
      "Epoch 44 Batch 850 Loss 1.0467 Accuracy 0.1199\n",
      "Epoch 44 Batch 900 Loss 1.0476 Accuracy 0.1200\n",
      "Epoch 44 Batch 950 Loss 1.0487 Accuracy 0.1201\n",
      "Epoch 44 Batch 1000 Loss 1.0494 Accuracy 0.1202\n",
      "Epoch 44 Batch 1050 Loss 1.0489 Accuracy 0.1201\n",
      "Epoch 44 Batch 1100 Loss 1.0495 Accuracy 0.1201\n",
      "Epoch 44 Batch 1150 Loss 1.0492 Accuracy 0.1201\n",
      "Epoch 44 Batch 1200 Loss 1.0493 Accuracy 0.1202\n",
      "Epoch 44 Batch 1250 Loss 1.0478 Accuracy 0.1201\n",
      "Epoch 44 Batch 1300 Loss 1.0463 Accuracy 0.1200\n",
      "Epoch 44 Batch 1350 Loss 1.0463 Accuracy 0.1200\n",
      "Epoch 44 Batch 1400 Loss 1.0464 Accuracy 0.1200\n",
      "Epoch 44 Batch 1450 Loss 1.0459 Accuracy 0.1200\n",
      "Epoch 44 Batch 1500 Loss 1.0456 Accuracy 0.1200\n",
      "Epoch 44 Batch 1550 Loss 1.0448 Accuracy 0.1199\n",
      "Epoch 44 Batch 1600 Loss 1.0444 Accuracy 0.1198\n",
      "Epoch 44 Batch 1650 Loss 1.0437 Accuracy 0.1197\n",
      "Epoch 44 Batch 1700 Loss 1.0445 Accuracy 0.1197\n",
      "Epoch 44 Batch 1750 Loss 1.0449 Accuracy 0.1197\n",
      "Epoch 44 Batch 1800 Loss 1.0458 Accuracy 0.1198\n",
      "Epoch 44 Batch 1850 Loss 1.0457 Accuracy 0.1197\n",
      "Epoch 44 Batch 1900 Loss 1.0460 Accuracy 0.1196\n",
      "Epoch 44 Batch 1950 Loss 1.0463 Accuracy 0.1196\n",
      "Epoch 44 Batch 2000 Loss 1.0452 Accuracy 0.1195\n",
      "Epoch 44 Batch 2050 Loss 1.0452 Accuracy 0.1194\n",
      "Epoch 44 Batch 2100 Loss 1.0451 Accuracy 0.1193\n",
      "Epoch 44 Batch 2150 Loss 1.0453 Accuracy 0.1193\n",
      "Epoch 44 Batch 2200 Loss 1.0459 Accuracy 0.1193\n",
      "Epoch 44 Batch 2250 Loss 1.0465 Accuracy 0.1193\n",
      "Epoch 44 Batch 2300 Loss 1.0465 Accuracy 0.1193\n",
      "Epoch 44 Batch 2350 Loss 1.0468 Accuracy 0.1193\n",
      "Epoch 44 Batch 2400 Loss 1.0472 Accuracy 0.1193\n",
      "Epoch 44 Batch 2450 Loss 1.0473 Accuracy 0.1193\n",
      "Epoch 44 Batch 2500 Loss 1.0477 Accuracy 0.1193\n",
      "Epoch 44 Batch 2550 Loss 1.0485 Accuracy 0.1194\n",
      "Epoch 44 Batch 2600 Loss 1.0493 Accuracy 0.1194\n",
      "Epoch 44 Batch 2650 Loss 1.0494 Accuracy 0.1194\n",
      "Epoch 44 Batch 2700 Loss 1.0493 Accuracy 0.1194\n",
      "Epoch 44 Batch 2750 Loss 1.0491 Accuracy 0.1194\n",
      "Epoch 44 Batch 2800 Loss 1.0497 Accuracy 0.1194\n",
      "Epoch 44 Batch 2850 Loss 1.0501 Accuracy 0.1195\n",
      "Epoch 44 Batch 2900 Loss 1.0503 Accuracy 0.1195\n",
      "Epoch 44 Batch 2950 Loss 1.0505 Accuracy 0.1195\n",
      "Epoch 44 Batch 3000 Loss 1.0511 Accuracy 0.1195\n",
      "Epoch 44 Loss 1.0512 Accuracy 0.1195\n",
      "Time taken for 1 epoch: 186.08 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 1.1607 Accuracy 0.1270\n",
      "Epoch 45 Batch 50 Loss 1.0636 Accuracy 0.1209\n",
      "Epoch 45 Batch 100 Loss 1.0624 Accuracy 0.1212\n",
      "Epoch 45 Batch 150 Loss 1.0620 Accuracy 0.1212\n",
      "Epoch 45 Batch 200 Loss 1.0623 Accuracy 0.1216\n",
      "Epoch 45 Batch 250 Loss 1.0541 Accuracy 0.1211\n",
      "Epoch 45 Batch 300 Loss 1.0511 Accuracy 0.1209\n",
      "Epoch 45 Batch 350 Loss 1.0491 Accuracy 0.1210\n",
      "Epoch 45 Batch 400 Loss 1.0459 Accuracy 0.1207\n",
      "Epoch 45 Batch 450 Loss 1.0442 Accuracy 0.1204\n",
      "Epoch 45 Batch 500 Loss 1.0452 Accuracy 0.1205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 550 Loss 1.0452 Accuracy 0.1204\n",
      "Epoch 45 Batch 600 Loss 1.0450 Accuracy 0.1205\n",
      "Epoch 45 Batch 650 Loss 1.0443 Accuracy 0.1204\n",
      "Epoch 45 Batch 700 Loss 1.0456 Accuracy 0.1205\n",
      "Epoch 45 Batch 750 Loss 1.0468 Accuracy 0.1205\n",
      "Epoch 45 Batch 800 Loss 1.0481 Accuracy 0.1204\n",
      "Epoch 45 Batch 850 Loss 1.0489 Accuracy 0.1205\n",
      "Epoch 45 Batch 900 Loss 1.0498 Accuracy 0.1206\n",
      "Epoch 45 Batch 950 Loss 1.0504 Accuracy 0.1206\n",
      "Epoch 45 Batch 1000 Loss 1.0496 Accuracy 0.1205\n",
      "Epoch 45 Batch 1050 Loss 1.0495 Accuracy 0.1205\n",
      "Epoch 45 Batch 1100 Loss 1.0515 Accuracy 0.1207\n",
      "Epoch 45 Batch 1150 Loss 1.0508 Accuracy 0.1206\n",
      "Epoch 45 Batch 1200 Loss 1.0506 Accuracy 0.1207\n",
      "Epoch 45 Batch 1250 Loss 1.0494 Accuracy 0.1206\n",
      "Epoch 45 Batch 1300 Loss 1.0482 Accuracy 0.1205\n",
      "Epoch 45 Batch 1350 Loss 1.0473 Accuracy 0.1204\n",
      "Epoch 45 Batch 1400 Loss 1.0466 Accuracy 0.1204\n",
      "Epoch 45 Batch 1450 Loss 1.0463 Accuracy 0.1203\n",
      "Epoch 45 Batch 1500 Loss 1.0469 Accuracy 0.1204\n",
      "Epoch 45 Batch 1550 Loss 1.0464 Accuracy 0.1203\n",
      "Epoch 45 Batch 1600 Loss 1.0461 Accuracy 0.1203\n",
      "Epoch 45 Batch 1650 Loss 1.0451 Accuracy 0.1202\n",
      "Epoch 45 Batch 1700 Loss 1.0453 Accuracy 0.1201\n",
      "Epoch 45 Batch 1750 Loss 1.0455 Accuracy 0.1201\n",
      "Epoch 45 Batch 1800 Loss 1.0446 Accuracy 0.1200\n",
      "Epoch 45 Batch 1850 Loss 1.0457 Accuracy 0.1200\n",
      "Epoch 45 Batch 1900 Loss 1.0455 Accuracy 0.1199\n",
      "Epoch 45 Batch 1950 Loss 1.0454 Accuracy 0.1198\n",
      "Epoch 45 Batch 2000 Loss 1.0451 Accuracy 0.1197\n",
      "Epoch 45 Batch 2050 Loss 1.0445 Accuracy 0.1196\n",
      "Epoch 45 Batch 2100 Loss 1.0445 Accuracy 0.1196\n",
      "Epoch 45 Batch 2150 Loss 1.0443 Accuracy 0.1195\n",
      "Epoch 45 Batch 2200 Loss 1.0451 Accuracy 0.1195\n",
      "Epoch 45 Batch 2250 Loss 1.0450 Accuracy 0.1194\n",
      "Epoch 45 Batch 2300 Loss 1.0447 Accuracy 0.1194\n",
      "Epoch 45 Batch 2350 Loss 1.0450 Accuracy 0.1194\n",
      "Epoch 45 Batch 2400 Loss 1.0455 Accuracy 0.1194\n",
      "Epoch 45 Batch 2450 Loss 1.0463 Accuracy 0.1194\n",
      "Epoch 45 Batch 2500 Loss 1.0461 Accuracy 0.1194\n",
      "Epoch 45 Batch 2550 Loss 1.0466 Accuracy 0.1194\n",
      "Epoch 45 Batch 2600 Loss 1.0474 Accuracy 0.1195\n",
      "Epoch 45 Batch 2650 Loss 1.0472 Accuracy 0.1195\n",
      "Epoch 45 Batch 2700 Loss 1.0477 Accuracy 0.1195\n",
      "Epoch 45 Batch 2750 Loss 1.0487 Accuracy 0.1196\n",
      "Epoch 45 Batch 2800 Loss 1.0489 Accuracy 0.1196\n",
      "Epoch 45 Batch 2850 Loss 1.0491 Accuracy 0.1196\n",
      "Epoch 45 Batch 2900 Loss 1.0494 Accuracy 0.1197\n",
      "Epoch 45 Batch 2950 Loss 1.0495 Accuracy 0.1197\n",
      "Epoch 45 Batch 3000 Loss 1.0499 Accuracy 0.1197\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
      "Epoch 45 Loss 1.0499 Accuracy 0.1197\n",
      "Time taken for 1 epoch: 185.10 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 1.0636 Accuracy 0.1246\n",
      "Epoch 46 Batch 50 Loss 1.0722 Accuracy 0.1218\n",
      "Epoch 46 Batch 100 Loss 1.0697 Accuracy 0.1217\n",
      "Epoch 46 Batch 150 Loss 1.0582 Accuracy 0.1211\n",
      "Epoch 46 Batch 200 Loss 1.0567 Accuracy 0.1212\n",
      "Epoch 46 Batch 250 Loss 1.0502 Accuracy 0.1211\n",
      "Epoch 46 Batch 300 Loss 1.0463 Accuracy 0.1210\n",
      "Epoch 46 Batch 350 Loss 1.0484 Accuracy 0.1211\n",
      "Epoch 46 Batch 400 Loss 1.0456 Accuracy 0.1207\n",
      "Epoch 46 Batch 450 Loss 1.0473 Accuracy 0.1210\n",
      "Epoch 46 Batch 500 Loss 1.0488 Accuracy 0.1210\n",
      "Epoch 46 Batch 550 Loss 1.0494 Accuracy 0.1210\n",
      "Epoch 46 Batch 600 Loss 1.0472 Accuracy 0.1209\n",
      "Epoch 46 Batch 650 Loss 1.0471 Accuracy 0.1209\n",
      "Epoch 46 Batch 700 Loss 1.0461 Accuracy 0.1207\n",
      "Epoch 46 Batch 750 Loss 1.0458 Accuracy 0.1208\n",
      "Epoch 46 Batch 800 Loss 1.0475 Accuracy 0.1208\n",
      "Epoch 46 Batch 850 Loss 1.0485 Accuracy 0.1209\n",
      "Epoch 46 Batch 900 Loss 1.0468 Accuracy 0.1208\n",
      "Epoch 46 Batch 950 Loss 1.0474 Accuracy 0.1209\n",
      "Epoch 46 Batch 1000 Loss 1.0484 Accuracy 0.1208\n",
      "Epoch 46 Batch 1050 Loss 1.0473 Accuracy 0.1208\n",
      "Epoch 46 Batch 1100 Loss 1.0473 Accuracy 0.1207\n",
      "Epoch 46 Batch 1150 Loss 1.0472 Accuracy 0.1208\n",
      "Epoch 46 Batch 1200 Loss 1.0466 Accuracy 0.1208\n",
      "Epoch 46 Batch 1250 Loss 1.0459 Accuracy 0.1208\n",
      "Epoch 46 Batch 1300 Loss 1.0454 Accuracy 0.1207\n",
      "Epoch 46 Batch 1350 Loss 1.0450 Accuracy 0.1206\n",
      "Epoch 46 Batch 1400 Loss 1.0448 Accuracy 0.1206\n",
      "Epoch 46 Batch 1450 Loss 1.0448 Accuracy 0.1206\n",
      "Epoch 46 Batch 1500 Loss 1.0444 Accuracy 0.1206\n",
      "Epoch 46 Batch 1550 Loss 1.0445 Accuracy 0.1206\n",
      "Epoch 46 Batch 1600 Loss 1.0440 Accuracy 0.1205\n",
      "Epoch 46 Batch 1650 Loss 1.0438 Accuracy 0.1204\n",
      "Epoch 46 Batch 1700 Loss 1.0432 Accuracy 0.1203\n",
      "Epoch 46 Batch 1750 Loss 1.0434 Accuracy 0.1203\n",
      "Epoch 46 Batch 1800 Loss 1.0427 Accuracy 0.1201\n",
      "Epoch 46 Batch 1850 Loss 1.0421 Accuracy 0.1200\n",
      "Epoch 46 Batch 1900 Loss 1.0428 Accuracy 0.1200\n",
      "Epoch 46 Batch 1950 Loss 1.0426 Accuracy 0.1199\n",
      "Epoch 46 Batch 2000 Loss 1.0421 Accuracy 0.1198\n",
      "Epoch 46 Batch 2050 Loss 1.0420 Accuracy 0.1198\n",
      "Epoch 46 Batch 2100 Loss 1.0420 Accuracy 0.1197\n",
      "Epoch 46 Batch 2150 Loss 1.0420 Accuracy 0.1196\n",
      "Epoch 46 Batch 2200 Loss 1.0425 Accuracy 0.1197\n",
      "Epoch 46 Batch 2250 Loss 1.0427 Accuracy 0.1197\n",
      "Epoch 46 Batch 2300 Loss 1.0430 Accuracy 0.1197\n",
      "Epoch 46 Batch 2350 Loss 1.0438 Accuracy 0.1197\n",
      "Epoch 46 Batch 2400 Loss 1.0441 Accuracy 0.1197\n",
      "Epoch 46 Batch 2450 Loss 1.0450 Accuracy 0.1198\n",
      "Epoch 46 Batch 2500 Loss 1.0454 Accuracy 0.1197\n",
      "Epoch 46 Batch 2550 Loss 1.0460 Accuracy 0.1197\n",
      "Epoch 46 Batch 2600 Loss 1.0462 Accuracy 0.1197\n",
      "Epoch 46 Batch 2650 Loss 1.0463 Accuracy 0.1198\n",
      "Epoch 46 Batch 2700 Loss 1.0462 Accuracy 0.1198\n",
      "Epoch 46 Batch 2750 Loss 1.0463 Accuracy 0.1198\n",
      "Epoch 46 Batch 2800 Loss 1.0468 Accuracy 0.1198\n",
      "Epoch 46 Batch 2850 Loss 1.0471 Accuracy 0.1198\n",
      "Epoch 46 Batch 2900 Loss 1.0472 Accuracy 0.1198\n",
      "Epoch 46 Batch 2950 Loss 1.0476 Accuracy 0.1199\n",
      "Epoch 46 Batch 3000 Loss 1.0481 Accuracy 0.1199\n",
      "Epoch 46 Loss 1.0483 Accuracy 0.1199\n",
      "Time taken for 1 epoch: 190.73 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 1.0199 Accuracy 0.1194\n",
      "Epoch 47 Batch 50 Loss 1.0507 Accuracy 0.1203\n",
      "Epoch 47 Batch 100 Loss 1.0519 Accuracy 0.1217\n",
      "Epoch 47 Batch 150 Loss 1.0513 Accuracy 0.1217\n",
      "Epoch 47 Batch 200 Loss 1.0511 Accuracy 0.1218\n",
      "Epoch 47 Batch 250 Loss 1.0465 Accuracy 0.1215\n",
      "Epoch 47 Batch 300 Loss 1.0417 Accuracy 0.1210\n",
      "Epoch 47 Batch 350 Loss 1.0434 Accuracy 0.1209\n",
      "Epoch 47 Batch 400 Loss 1.0429 Accuracy 0.1210\n",
      "Epoch 47 Batch 450 Loss 1.0410 Accuracy 0.1208\n",
      "Epoch 47 Batch 500 Loss 1.0402 Accuracy 0.1208\n",
      "Epoch 47 Batch 550 Loss 1.0399 Accuracy 0.1209\n",
      "Epoch 47 Batch 600 Loss 1.0408 Accuracy 0.1209\n",
      "Epoch 47 Batch 650 Loss 1.0404 Accuracy 0.1208\n",
      "Epoch 47 Batch 700 Loss 1.0403 Accuracy 0.1207\n",
      "Epoch 47 Batch 750 Loss 1.0403 Accuracy 0.1206\n",
      "Epoch 47 Batch 800 Loss 1.0408 Accuracy 0.1206\n",
      "Epoch 47 Batch 850 Loss 1.0416 Accuracy 0.1206\n",
      "Epoch 47 Batch 900 Loss 1.0441 Accuracy 0.1208\n",
      "Epoch 47 Batch 950 Loss 1.0462 Accuracy 0.1209\n",
      "Epoch 47 Batch 1000 Loss 1.0460 Accuracy 0.1209\n",
      "Epoch 47 Batch 1050 Loss 1.0461 Accuracy 0.1209\n",
      "Epoch 47 Batch 1100 Loss 1.0474 Accuracy 0.1210\n",
      "Epoch 47 Batch 1150 Loss 1.0462 Accuracy 0.1209\n",
      "Epoch 47 Batch 1200 Loss 1.0452 Accuracy 0.1209\n",
      "Epoch 47 Batch 1250 Loss 1.0452 Accuracy 0.1208\n",
      "Epoch 47 Batch 1300 Loss 1.0450 Accuracy 0.1208\n",
      "Epoch 47 Batch 1350 Loss 1.0435 Accuracy 0.1207\n",
      "Epoch 47 Batch 1400 Loss 1.0428 Accuracy 0.1206\n",
      "Epoch 47 Batch 1450 Loss 1.0427 Accuracy 0.1207\n",
      "Epoch 47 Batch 1500 Loss 1.0427 Accuracy 0.1207\n",
      "Epoch 47 Batch 1550 Loss 1.0431 Accuracy 0.1207\n",
      "Epoch 47 Batch 1600 Loss 1.0429 Accuracy 0.1206\n",
      "Epoch 47 Batch 1650 Loss 1.0424 Accuracy 0.1205\n",
      "Epoch 47 Batch 1700 Loss 1.0413 Accuracy 0.1204\n",
      "Epoch 47 Batch 1750 Loss 1.0413 Accuracy 0.1203\n",
      "Epoch 47 Batch 1800 Loss 1.0416 Accuracy 0.1203\n",
      "Epoch 47 Batch 1850 Loss 1.0419 Accuracy 0.1203\n",
      "Epoch 47 Batch 1900 Loss 1.0421 Accuracy 0.1202\n",
      "Epoch 47 Batch 1950 Loss 1.0422 Accuracy 0.1202\n",
      "Epoch 47 Batch 2000 Loss 1.0418 Accuracy 0.1201\n",
      "Epoch 47 Batch 2050 Loss 1.0415 Accuracy 0.1201\n",
      "Epoch 47 Batch 2100 Loss 1.0416 Accuracy 0.1200\n",
      "Epoch 47 Batch 2150 Loss 1.0422 Accuracy 0.1200\n",
      "Epoch 47 Batch 2200 Loss 1.0428 Accuracy 0.1200\n",
      "Epoch 47 Batch 2250 Loss 1.0428 Accuracy 0.1200\n",
      "Epoch 47 Batch 2300 Loss 1.0431 Accuracy 0.1200\n",
      "Epoch 47 Batch 2350 Loss 1.0438 Accuracy 0.1201\n",
      "Epoch 47 Batch 2400 Loss 1.0445 Accuracy 0.1201\n",
      "Epoch 47 Batch 2450 Loss 1.0447 Accuracy 0.1201\n",
      "Epoch 47 Batch 2500 Loss 1.0453 Accuracy 0.1201\n",
      "Epoch 47 Batch 2550 Loss 1.0453 Accuracy 0.1201\n",
      "Epoch 47 Batch 2600 Loss 1.0454 Accuracy 0.1201\n",
      "Epoch 47 Batch 2650 Loss 1.0455 Accuracy 0.1201\n",
      "Epoch 47 Batch 2700 Loss 1.0454 Accuracy 0.1201\n",
      "Epoch 47 Batch 2750 Loss 1.0452 Accuracy 0.1201\n",
      "Epoch 47 Batch 2800 Loss 1.0455 Accuracy 0.1201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 2850 Loss 1.0454 Accuracy 0.1201\n",
      "Epoch 47 Batch 2900 Loss 1.0462 Accuracy 0.1201\n",
      "Epoch 47 Batch 2950 Loss 1.0467 Accuracy 0.1201\n",
      "Epoch 47 Batch 3000 Loss 1.0472 Accuracy 0.1202\n",
      "Epoch 47 Loss 1.0473 Accuracy 0.1202\n",
      "Time taken for 1 epoch: 190.99 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 1.0935 Accuracy 0.1214\n",
      "Epoch 48 Batch 50 Loss 1.0614 Accuracy 0.1205\n",
      "Epoch 48 Batch 100 Loss 1.0576 Accuracy 0.1209\n",
      "Epoch 48 Batch 150 Loss 1.0490 Accuracy 0.1210\n",
      "Epoch 48 Batch 200 Loss 1.0436 Accuracy 0.1207\n",
      "Epoch 48 Batch 250 Loss 1.0385 Accuracy 0.1203\n",
      "Epoch 48 Batch 300 Loss 1.0364 Accuracy 0.1205\n",
      "Epoch 48 Batch 350 Loss 1.0331 Accuracy 0.1203\n",
      "Epoch 48 Batch 400 Loss 1.0359 Accuracy 0.1204\n",
      "Epoch 48 Batch 450 Loss 1.0354 Accuracy 0.1205\n",
      "Epoch 48 Batch 500 Loss 1.0370 Accuracy 0.1207\n",
      "Epoch 48 Batch 550 Loss 1.0387 Accuracy 0.1209\n",
      "Epoch 48 Batch 600 Loss 1.0384 Accuracy 0.1209\n",
      "Epoch 48 Batch 650 Loss 1.0383 Accuracy 0.1208\n",
      "Epoch 48 Batch 700 Loss 1.0400 Accuracy 0.1209\n",
      "Epoch 48 Batch 750 Loss 1.0424 Accuracy 0.1210\n",
      "Epoch 48 Batch 800 Loss 1.0442 Accuracy 0.1211\n",
      "Epoch 48 Batch 850 Loss 1.0446 Accuracy 0.1211\n",
      "Epoch 48 Batch 900 Loss 1.0453 Accuracy 0.1213\n",
      "Epoch 48 Batch 950 Loss 1.0463 Accuracy 0.1213\n",
      "Epoch 48 Batch 1000 Loss 1.0462 Accuracy 0.1213\n",
      "Epoch 48 Batch 1050 Loss 1.0465 Accuracy 0.1212\n",
      "Epoch 48 Batch 1100 Loss 1.0476 Accuracy 0.1212\n",
      "Epoch 48 Batch 1150 Loss 1.0465 Accuracy 0.1211\n",
      "Epoch 48 Batch 1200 Loss 1.0464 Accuracy 0.1211\n",
      "Epoch 48 Batch 1250 Loss 1.0449 Accuracy 0.1210\n",
      "Epoch 48 Batch 1300 Loss 1.0430 Accuracy 0.1208\n",
      "Epoch 48 Batch 1350 Loss 1.0427 Accuracy 0.1208\n",
      "Epoch 48 Batch 1400 Loss 1.0413 Accuracy 0.1207\n",
      "Epoch 48 Batch 1450 Loss 1.0409 Accuracy 0.1207\n",
      "Epoch 48 Batch 1500 Loss 1.0413 Accuracy 0.1207\n",
      "Epoch 48 Batch 1550 Loss 1.0413 Accuracy 0.1207\n",
      "Epoch 48 Batch 1600 Loss 1.0414 Accuracy 0.1207\n",
      "Epoch 48 Batch 1650 Loss 1.0411 Accuracy 0.1207\n",
      "Epoch 48 Batch 1700 Loss 1.0404 Accuracy 0.1206\n",
      "Epoch 48 Batch 1750 Loss 1.0404 Accuracy 0.1205\n",
      "Epoch 48 Batch 1800 Loss 1.0401 Accuracy 0.1205\n",
      "Epoch 48 Batch 1850 Loss 1.0408 Accuracy 0.1205\n",
      "Epoch 48 Batch 1900 Loss 1.0410 Accuracy 0.1204\n",
      "Epoch 48 Batch 1950 Loss 1.0406 Accuracy 0.1202\n",
      "Epoch 48 Batch 2000 Loss 1.0399 Accuracy 0.1201\n",
      "Epoch 48 Batch 2050 Loss 1.0400 Accuracy 0.1201\n",
      "Epoch 48 Batch 2100 Loss 1.0399 Accuracy 0.1200\n",
      "Epoch 48 Batch 2150 Loss 1.0404 Accuracy 0.1200\n",
      "Epoch 48 Batch 2200 Loss 1.0406 Accuracy 0.1200\n",
      "Epoch 48 Batch 2250 Loss 1.0409 Accuracy 0.1200\n",
      "Epoch 48 Batch 2300 Loss 1.0411 Accuracy 0.1200\n",
      "Epoch 48 Batch 2350 Loss 1.0413 Accuracy 0.1200\n",
      "Epoch 48 Batch 2400 Loss 1.0422 Accuracy 0.1201\n",
      "Epoch 48 Batch 2450 Loss 1.0425 Accuracy 0.1201\n",
      "Epoch 48 Batch 2500 Loss 1.0425 Accuracy 0.1201\n",
      "Epoch 48 Batch 2550 Loss 1.0427 Accuracy 0.1201\n",
      "Epoch 48 Batch 2600 Loss 1.0432 Accuracy 0.1201\n",
      "Epoch 48 Batch 2650 Loss 1.0431 Accuracy 0.1201\n",
      "Epoch 48 Batch 2700 Loss 1.0432 Accuracy 0.1201\n",
      "Epoch 48 Batch 2750 Loss 1.0434 Accuracy 0.1201\n",
      "Epoch 48 Batch 2800 Loss 1.0435 Accuracy 0.1201\n",
      "Epoch 48 Batch 2850 Loss 1.0438 Accuracy 0.1201\n",
      "Epoch 48 Batch 2900 Loss 1.0442 Accuracy 0.1201\n",
      "Epoch 48 Batch 2950 Loss 1.0451 Accuracy 0.1202\n",
      "Epoch 48 Batch 3000 Loss 1.0454 Accuracy 0.1203\n",
      "Epoch 48 Loss 1.0461 Accuracy 0.1203\n",
      "Time taken for 1 epoch: 186.69 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 1.1387 Accuracy 0.1234\n",
      "Epoch 49 Batch 50 Loss 1.0601 Accuracy 0.1212\n",
      "Epoch 49 Batch 100 Loss 1.0520 Accuracy 0.1212\n",
      "Epoch 49 Batch 150 Loss 1.0460 Accuracy 0.1206\n",
      "Epoch 49 Batch 200 Loss 1.0431 Accuracy 0.1208\n",
      "Epoch 49 Batch 250 Loss 1.0384 Accuracy 0.1207\n",
      "Epoch 49 Batch 300 Loss 1.0351 Accuracy 0.1206\n",
      "Epoch 49 Batch 350 Loss 1.0321 Accuracy 0.1204\n",
      "Epoch 49 Batch 400 Loss 1.0322 Accuracy 0.1204\n",
      "Epoch 49 Batch 450 Loss 1.0346 Accuracy 0.1208\n",
      "Epoch 49 Batch 500 Loss 1.0332 Accuracy 0.1206\n",
      "Epoch 49 Batch 550 Loss 1.0329 Accuracy 0.1205\n",
      "Epoch 49 Batch 600 Loss 1.0331 Accuracy 0.1203\n",
      "Epoch 49 Batch 650 Loss 1.0330 Accuracy 0.1203\n",
      "Epoch 49 Batch 700 Loss 1.0343 Accuracy 0.1205\n",
      "Epoch 49 Batch 750 Loss 1.0363 Accuracy 0.1206\n",
      "Epoch 49 Batch 800 Loss 1.0379 Accuracy 0.1206\n",
      "Epoch 49 Batch 850 Loss 1.0399 Accuracy 0.1207\n",
      "Epoch 49 Batch 900 Loss 1.0410 Accuracy 0.1209\n",
      "Epoch 49 Batch 950 Loss 1.0427 Accuracy 0.1210\n",
      "Epoch 49 Batch 1000 Loss 1.0437 Accuracy 0.1211\n",
      "Epoch 49 Batch 1050 Loss 1.0437 Accuracy 0.1211\n",
      "Epoch 49 Batch 1100 Loss 1.0449 Accuracy 0.1212\n",
      "Epoch 49 Batch 1150 Loss 1.0450 Accuracy 0.1213\n",
      "Epoch 49 Batch 1200 Loss 1.0446 Accuracy 0.1212\n",
      "Epoch 49 Batch 1250 Loss 1.0426 Accuracy 0.1211\n",
      "Epoch 49 Batch 1300 Loss 1.0416 Accuracy 0.1211\n",
      "Epoch 49 Batch 1350 Loss 1.0405 Accuracy 0.1211\n",
      "Epoch 49 Batch 1400 Loss 1.0414 Accuracy 0.1211\n",
      "Epoch 49 Batch 1450 Loss 1.0409 Accuracy 0.1211\n",
      "Epoch 49 Batch 1500 Loss 1.0411 Accuracy 0.1211\n",
      "Epoch 49 Batch 1550 Loss 1.0407 Accuracy 0.1210\n",
      "Epoch 49 Batch 1600 Loss 1.0406 Accuracy 0.1210\n",
      "Epoch 49 Batch 1650 Loss 1.0409 Accuracy 0.1210\n",
      "Epoch 49 Batch 1700 Loss 1.0407 Accuracy 0.1209\n",
      "Epoch 49 Batch 1750 Loss 1.0401 Accuracy 0.1208\n",
      "Epoch 49 Batch 1800 Loss 1.0402 Accuracy 0.1207\n",
      "Epoch 49 Batch 1850 Loss 1.0408 Accuracy 0.1207\n",
      "Epoch 49 Batch 1900 Loss 1.0408 Accuracy 0.1206\n",
      "Epoch 49 Batch 1950 Loss 1.0401 Accuracy 0.1205\n",
      "Epoch 49 Batch 2000 Loss 1.0394 Accuracy 0.1204\n",
      "Epoch 49 Batch 2050 Loss 1.0389 Accuracy 0.1204\n",
      "Epoch 49 Batch 2100 Loss 1.0392 Accuracy 0.1203\n",
      "Epoch 49 Batch 2150 Loss 1.0398 Accuracy 0.1203\n",
      "Epoch 49 Batch 2200 Loss 1.0399 Accuracy 0.1203\n",
      "Epoch 49 Batch 2250 Loss 1.0403 Accuracy 0.1203\n",
      "Epoch 49 Batch 2300 Loss 1.0400 Accuracy 0.1202\n",
      "Epoch 49 Batch 2350 Loss 1.0405 Accuracy 0.1202\n",
      "Epoch 49 Batch 2400 Loss 1.0405 Accuracy 0.1202\n",
      "Epoch 49 Batch 2450 Loss 1.0412 Accuracy 0.1202\n",
      "Epoch 49 Batch 2500 Loss 1.0416 Accuracy 0.1202\n",
      "Epoch 49 Batch 2550 Loss 1.0418 Accuracy 0.1202\n",
      "Epoch 49 Batch 2600 Loss 1.0422 Accuracy 0.1202\n",
      "Epoch 49 Batch 2650 Loss 1.0423 Accuracy 0.1203\n",
      "Epoch 49 Batch 2700 Loss 1.0423 Accuracy 0.1203\n",
      "Epoch 49 Batch 2750 Loss 1.0425 Accuracy 0.1203\n",
      "Epoch 49 Batch 2800 Loss 1.0433 Accuracy 0.1203\n",
      "Epoch 49 Batch 2850 Loss 1.0431 Accuracy 0.1203\n",
      "Epoch 49 Batch 2900 Loss 1.0437 Accuracy 0.1203\n",
      "Epoch 49 Batch 2950 Loss 1.0442 Accuracy 0.1204\n",
      "Epoch 49 Batch 3000 Loss 1.0445 Accuracy 0.1204\n",
      "Epoch 49 Loss 1.0449 Accuracy 0.1205\n",
      "Time taken for 1 epoch: 188.11 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 1.1191 Accuracy 0.1282\n",
      "Epoch 50 Batch 50 Loss 1.0637 Accuracy 0.1223\n",
      "Epoch 50 Batch 100 Loss 1.0505 Accuracy 0.1216\n",
      "Epoch 50 Batch 150 Loss 1.0402 Accuracy 0.1215\n",
      "Epoch 50 Batch 200 Loss 1.0374 Accuracy 0.1214\n",
      "Epoch 50 Batch 250 Loss 1.0346 Accuracy 0.1212\n",
      "Epoch 50 Batch 300 Loss 1.0372 Accuracy 0.1215\n",
      "Epoch 50 Batch 350 Loss 1.0386 Accuracy 0.1217\n",
      "Epoch 50 Batch 400 Loss 1.0384 Accuracy 0.1215\n",
      "Epoch 50 Batch 450 Loss 1.0388 Accuracy 0.1215\n",
      "Epoch 50 Batch 500 Loss 1.0392 Accuracy 0.1216\n",
      "Epoch 50 Batch 550 Loss 1.0410 Accuracy 0.1217\n",
      "Epoch 50 Batch 600 Loss 1.0407 Accuracy 0.1216\n",
      "Epoch 50 Batch 650 Loss 1.0411 Accuracy 0.1215\n",
      "Epoch 50 Batch 700 Loss 1.0399 Accuracy 0.1213\n",
      "Epoch 50 Batch 750 Loss 1.0402 Accuracy 0.1213\n",
      "Epoch 50 Batch 800 Loss 1.0395 Accuracy 0.1212\n",
      "Epoch 50 Batch 850 Loss 1.0404 Accuracy 0.1212\n",
      "Epoch 50 Batch 900 Loss 1.0410 Accuracy 0.1213\n",
      "Epoch 50 Batch 950 Loss 1.0410 Accuracy 0.1214\n",
      "Epoch 50 Batch 1000 Loss 1.0415 Accuracy 0.1215\n",
      "Epoch 50 Batch 1050 Loss 1.0423 Accuracy 0.1215\n",
      "Epoch 50 Batch 1100 Loss 1.0431 Accuracy 0.1215\n",
      "Epoch 50 Batch 1150 Loss 1.0426 Accuracy 0.1215\n",
      "Epoch 50 Batch 1200 Loss 1.0420 Accuracy 0.1215\n",
      "Epoch 50 Batch 1250 Loss 1.0416 Accuracy 0.1215\n",
      "Epoch 50 Batch 1300 Loss 1.0405 Accuracy 0.1214\n",
      "Epoch 50 Batch 1350 Loss 1.0398 Accuracy 0.1214\n",
      "Epoch 50 Batch 1400 Loss 1.0390 Accuracy 0.1213\n",
      "Epoch 50 Batch 1450 Loss 1.0386 Accuracy 0.1212\n",
      "Epoch 50 Batch 1500 Loss 1.0383 Accuracy 0.1212\n",
      "Epoch 50 Batch 1550 Loss 1.0388 Accuracy 0.1213\n",
      "Epoch 50 Batch 1600 Loss 1.0380 Accuracy 0.1212\n",
      "Epoch 50 Batch 1650 Loss 1.0378 Accuracy 0.1211\n",
      "Epoch 50 Batch 1700 Loss 1.0373 Accuracy 0.1210\n",
      "Epoch 50 Batch 1750 Loss 1.0375 Accuracy 0.1210\n",
      "Epoch 50 Batch 1800 Loss 1.0371 Accuracy 0.1209\n",
      "Epoch 50 Batch 1850 Loss 1.0372 Accuracy 0.1209\n",
      "Epoch 50 Batch 1900 Loss 1.0378 Accuracy 0.1208\n",
      "Epoch 50 Batch 1950 Loss 1.0384 Accuracy 0.1207\n",
      "Epoch 50 Batch 2000 Loss 1.0379 Accuracy 0.1206\n",
      "Epoch 50 Batch 2050 Loss 1.0378 Accuracy 0.1205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 2100 Loss 1.0376 Accuracy 0.1205\n",
      "Epoch 50 Batch 2150 Loss 1.0378 Accuracy 0.1204\n",
      "Epoch 50 Batch 2200 Loss 1.0382 Accuracy 0.1204\n",
      "Epoch 50 Batch 2250 Loss 1.0389 Accuracy 0.1204\n",
      "Epoch 50 Batch 2300 Loss 1.0390 Accuracy 0.1204\n",
      "Epoch 50 Batch 2350 Loss 1.0392 Accuracy 0.1204\n",
      "Epoch 50 Batch 2400 Loss 1.0397 Accuracy 0.1204\n",
      "Epoch 50 Batch 2450 Loss 1.0401 Accuracy 0.1204\n",
      "Epoch 50 Batch 2500 Loss 1.0405 Accuracy 0.1205\n",
      "Epoch 50 Batch 2550 Loss 1.0408 Accuracy 0.1205\n",
      "Epoch 50 Batch 2600 Loss 1.0410 Accuracy 0.1204\n",
      "Epoch 50 Batch 2650 Loss 1.0413 Accuracy 0.1205\n",
      "Epoch 50 Batch 2700 Loss 1.0415 Accuracy 0.1205\n",
      "Epoch 50 Batch 2750 Loss 1.0418 Accuracy 0.1205\n",
      "Epoch 50 Batch 2800 Loss 1.0420 Accuracy 0.1206\n",
      "Epoch 50 Batch 2850 Loss 1.0420 Accuracy 0.1205\n",
      "Epoch 50 Batch 2900 Loss 1.0423 Accuracy 0.1206\n",
      "Epoch 50 Batch 2950 Loss 1.0429 Accuracy 0.1207\n",
      "Epoch 50 Batch 3000 Loss 1.0435 Accuracy 0.1207\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
      "Epoch 50 Loss 1.0435 Accuracy 0.1207\n",
      "Time taken for 1 epoch: 186.83 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1DUXog6WqV-"
   },
   "source": [
    "## Evaluate and predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_NjsS3zuAbRn"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "    encoder_input = sentence\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # concatenated the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J3Jdtk2P-RT"
   },
   "source": [
    "Let's test our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "6IeMSGEgRTvC",
    "outputId": "5b98cd62-4a42-4b5c-8ddb-222cab99a639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Where have you been?\n",
      "Output: i was going to get you out of here .\n"
     ]
    }
   ],
   "source": [
    "output = predict('Where have you been?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ivVgU6ydRV8R",
    "outputId": "121ae5c7-7447-4d06-e2dd-563c8eae76ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: It's a trap\n",
      "Output: i know , but i m not going to let you know what i m talking about .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"It's a trap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "s5zG7i8KAtRU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tell me a story\n",
      "Output: i m not going to let you out .\n",
      "\n",
      "Input: i m not going to let you out .\n",
      "Output: i m not going to let you go .\n",
      "\n",
      "Input: i m not going to let you go .\n",
      "Output: i m not going to let you go .\n",
      "\n",
      "Input: i m not going to let you go .\n",
      "Output: i m not going to let you go .\n",
      "\n",
      "Input: i m not going to let you go .\n",
      "Output: i m not going to let you go .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feed the model with its previous output\n",
    "sentence = 'tell me a story'\n",
    "for _ in range(5):\n",
    "  sentence = predict(sentence)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_chatbot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
